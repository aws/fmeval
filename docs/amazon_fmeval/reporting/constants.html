<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>src.amazon_fmeval.reporting.constants API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.amazon_fmeval.reporting.constants</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from enum import Enum
from typing import NamedTuple, Tuple, List

from amazon_fmeval.eval_algorithms.factual_knowledge import FACTUAL_KNOWLEDGE
from amazon_fmeval.eval_algorithms.prompt_stereotyping import PROMPT_STEREOTYPING, LOG_PROBABILITY_DIFFERENCE
from amazon_fmeval.eval_algorithms.qa_accuracy import F1_SCORE, EXACT_MATCH_SCORE, QUASI_EXACT_MATCH_SCORE
from amazon_fmeval.eval_algorithms.summarization_accuracy import METEOR_SCORE, BERT_SCORE, ROUGE_SCORE
from amazon_fmeval.eval_algorithms.classification_accuracy import (
    CLASSIFICATION_ACCURACY_SCORE,
    BALANCED_ACCURACY_SCORE,
    PRECISION_SCORE,
    RECALL_SCORE,
)
from amazon_fmeval.eval_algorithms.classification_accuracy_semantic_robustness import (
    DELTA_CLASSIFICATION_ACCURACY_SCORE,
)
from amazon_fmeval.eval_algorithms.qa_accuracy_semantic_robustness import (
    DELTA_F1_SCORE,
    DELTA_EXACT_MATCH_SCORE,
    DELTA_QUASI_EXACT_MATCH_SCORE,
)
from amazon_fmeval.eval_algorithms.summarization_accuracy_semantic_robustness import (
    DELTA_ROUGE_SCORE,
    DELTA_BERT_SCORE,
    DELTA_METEOR_SCORE,
)
from amazon_fmeval.eval_algorithms.general_semantic_robustness import WER_SCORE
from amazon_fmeval.eval_algorithms import (
    TREX,
    BOOLQ,
    TRIVIA_QA,
    NATURAL_QUESTIONS,
    CROWS_PAIRS,
    CNN_DAILY_MAIL,
    XSUM,
    IMDB_MOVIE_REVIEWS,
    WOMENS_CLOTHING_ECOMMERCE_REVIEWS,
    BOLD,
    WIKITEXT2,
    REAL_TOXICITY_PROMPTS,
    REAL_TOXICITY_PROMPTS_CHALLENGING,
)
from amazon_fmeval.eval_algorithms import EvalAlgorithm
from amazon_fmeval.eval_algorithms.helper_models.helper_model import (
    TOXIGEN_SCORE_NAME,
    DETOXIFY_SCORE_TOXICITY,
    DETOXIFY_SCORE_SEVERE_TOXICITY,
    DETOXIFY_SCORE_OBSCENE,
    DETOXIFY_SCORE_IDENTITY_ATTACK,
    DETOXIFY_SCORE_INSULT,
    DETOXIFY_SCORE_THREAT,
    DETOXIFY_SCORE_SEXUAL_EXPLICIT,
)

# For general HTML alignment
CENTER = &#34;center&#34;
LEFT = &#34;left&#34;
RIGHT = &#34;right&#34;


class ListType(Enum):
    BULLETED = &#34;bulleted&#34;
    NUMBERED = &#34;numbered&#34;


# For general use in Markdown-related code
SINGLE_NEWLINE = &#34;  \n&#34;
DOUBLE_NEWLINE = &#34;  \n\n&#34;

# For tables and bar plots
NUM_SAMPLES_TO_DISPLAY_IN_TABLE = 5
CATEGORY_BAR_COLOR = &#34;steelblue&#34;
OVERALL_BAR_COLOR = &#34;coral&#34;
MAX_CHAR = 200

# Extensions used by the markdown library to convert markdown to HTML
MARKDOWN_EXTENSIONS = [&#34;tables&#34;, &#34;md_in_html&#34;]

# Dataset score label used in category bar plot
DATASET_SCORE_LABEL = &#34;Overall&#34;

# Scores that are not per sample
AGGREGATE_ONLY_SCORES = [BALANCED_ACCURACY_SCORE, PRECISION_SCORE, RECALL_SCORE]

# For string formatting in eval names/score names
GENERAL_STRING_REPLACEMENTS: List[Tuple[str, str]] = [(&#34;qa&#34;, &#34;Q&amp;A&#34;), (&#34;f1&#34;, &#34;F1&#34;)]
SCORE_STRING_REPLACEMENTS: List[Tuple[str, str]] = [
    (&#34;prompt stereotyping&#34;, &#34;is_biased&#34;),
    (&#34;meteor&#34;, &#34;METEOR&#34;),
    (&#34;bertscore&#34;, &#34;BERTScore&#34;),
    (&#34;rouge&#34;, &#34;ROUGE&#34;),
    (&#34;F1 score&#34;, &#34;F1 over words&#34;),
]
EVAL_NAME_STRING_REPLACEMENTS: List[Tuple[str, str]] = [
    (EvalAlgorithm.QA_ACCURACY.value, EvalAlgorithm.ACCURACY.value),
    (EvalAlgorithm.SUMMARIZATION_ACCURACY.value, EvalAlgorithm.ACCURACY.value),
    (EvalAlgorithm.CLASSIFICATION_ACCURACY.value, EvalAlgorithm.ACCURACY.value),
    (EvalAlgorithm.GENERAL_SEMANTIC_ROBUSTNESS.value, &#34;semantic_robustness&#34;),
    (EvalAlgorithm.QA_ACCURACY_SEMANTIC_ROBUSTNESS.value, &#34;semantic_robustness&#34;),
    (EvalAlgorithm.CLASSIFICATION_ACCURACY_SEMANTIC_ROBUSTNESS.value, &#34;semantic_robustness&#34;),
    (EvalAlgorithm.SUMMARIZATION_ACCURACY_SEMANTIC_ROBUSTNESS.value, &#34;semantic_robustness&#34;),
    (EvalAlgorithm.QA_ACCURACY.value, EvalAlgorithm.TOXICITY.value),
    (EvalAlgorithm.SUMMARIZATION_TOXICITY.value, EvalAlgorithm.TOXICITY.value),
    (EvalAlgorithm.CLASSIFICATION_ACCURACY.value, EvalAlgorithm.TOXICITY.value),
]
PLOT_TITLE_STRING_REPLACEMENTS: List[Tuple[str, str]] = [(&#34;prompt_stereotyping&#34;, &#34;is_biased score&#34;)]
COLUMN_NAME_STRING_REPLACEMENTS: List[Tuple[str, str]] = [
    (&#34;sent_more&#34;, &#34;stereotypical&#34;),
    (&#34;sent_less&#34;, &#34;anti-stereotypical&#34;),
    (&#34;prob_&#34;, &#34;probability_&#34;),
    (&#34;prompt_stereotyping&#34;, &#34;Log Probability Difference&#34;),
    (&#34;word_error_rate&#34;, &#34;Average WER&#34;),
    (&#34;classification_accuracy&#34;, &#34;accuracy&#34;),
    (&#34;f1_score&#34;, &#34;f1 over words&#34;),
    (&#34;meteor&#34;, &#34;METEOR&#34;),
    (&#34;bertscore&#34;, &#34;BERTScore&#34;),
    (&#34;rouge&#34;, &#34;ROUGE&#34;),
]

# Dataset types
BUILT_IN_DATASET = &#34;Built-in Dataset&#34;
CUSTOM_DATASET = &#34;Custom Dataset&#34;

PROMPT_COLUMN_NAME = &#34;prompt&#34;

TOXICITY_EVAL_NAMES = [
    EvalAlgorithm.TOXICITY.value,
    EvalAlgorithm.QA_TOXICITY.value,
    EvalAlgorithm.SUMMARIZATION_TOXICITY.value,
]

# Toxicity detector names
TOXIGEN_NAME = &#34;Toxigen-roberta&#34;
DETOXIFY_NAME = &#34;UnitaryAI Detoxify-unbiased&#34;
# Example table descriptions
TABLE_DESCRIPTION = &#34;Below are a few examples of the highest and lowest-scoring examples across all categories. Some text may be truncated due to length constraints. To view the full prompts, please go to the S3 job output location that you specified when configuring the job. &#34;
WER_TABLE_DESCRIPTION = &#34;Below are a few examples of the highest and lowest-scoring examples across all categories. The lower the word error rate, the better the model performs. Some text may be truncated due to length constraints. To view the full prompts, please go to the S3 job output location that you specified when configuring the job.&#34;
STEREOTYPING_TABLE_DESCRIPTION = &#34;For each sentence pair, we report the log probability difference, a value ranging -&amp;#8734; to &amp;#8734;, indicating how much the model stereotypes. Below are a few example of the most and least stereotypical prompts. Some text may be truncated due to length constraints. To view the full prompts, please go to the S3 job output location that you specified when configuring the job.&#34;
FACTUAL_KNOWLEDGE_TABLE_DESCRIPTION = &#34;Below are a few examples of correct and incorrect model responses. Some text may be truncated due to length constraints. To view the full prompts, please go to the S3 job output location that you specified when configuring the job. &#34;

# Score descriptions for the report
SCORE_DESCRIPTIONS = {
    # Factual knowledge
    FACTUAL_KNOWLEDGE: &#34;A factual knowledge score is a value between 0 and 1, indicating the percentage of correctly retrieved real-world facts across different knowledge categories.&#34;,
    # Prompt stereotyping
    PROMPT_STEREOTYPING: &#34;The is_biased prompt stereotyping score is a value between 0 and 1. Non-stereotyping models score around 0.5. Both 0 and 1 indicate stereotyping model behaviour.&#34;,
    LOG_PROBABILITY_DIFFERENCE: &#34;For each sentence pair, we report the log probability difference, a value ranging -&amp;#8734; to &amp;#8734;, indicating how much the model stereotypes. &#34;,
    # QA accuracy
    F1_SCORE: &#34;Numerical score between 0 (worst) and 1 (best). F1-score is the harmonic mean of precision and recall. It is computed as follows:  precision = true positives / (true positives + false positives) and recall = true positives / (true positives + false negatives). Then F1 = 2 (precision * recall)/(precision + recall) .&#34;,
    EXACT_MATCH_SCORE: &#34;An exact match score is a binary score where 1 indicates the model output and answer match exactly and 0 indicates otherwise.&#34;,
    QUASI_EXACT_MATCH_SCORE: &#34;Similar as above, but both model output and answer are normalised first by removing any articles and punctuation. E.g., 1 also for predicted answers “Antarctica.” or “the Antarctica” .&#34;,
    # Summarization accuracy
    ROUGE_SCORE: &#34;A ROUGE-N score computes the N-gram (sequences of n words) word overlaps between the reference and model summary, with the value ranging between 0 (no match) to 1 (perfect match).&#34;,
    METEOR_SCORE: &#34;Meteor is similar to ROUGE-N, but it also accounts for rephrasing by using traditional NLP techniques such as stemming (e.g. matching “singing” to “sing”,“sings” etc.) and synonym lists.&#34;,
    BERT_SCORE: &#34;BERTScore uses a second ML model (from the BERT family) to compute sentence embeddings and compare their similarity.&#34;,
    # Classification accuracy
    CLASSIFICATION_ACCURACY_SCORE: &#34;The classification accuracy is `predicted_label == true_label`, reported as the mean accuracy over all datapoints.&#34;,
    PRECISION_SCORE: &#34;The precision score is computed as `true positives / (true positives + false positives)`. &#34;,
    RECALL_SCORE: &#34;The recall score is computed as `true positives / (true positives + false negatives)`&#34;,
    BALANCED_ACCURACY_SCORE: &#34;The balanced accuracy score is the same as accuracy in the binary case, otherwise averaged recall per class.&#34;,
    # General semantic robustness
    WER_SCORE: &#34;Word error rate (WER) is a value between 0 and 1, and measures the difference between the model output on the unperturbed input and the output(s) on one or more perturbed versions of the same input. For more details on how word error rate is computed, see the [HuggingFace Article on Word Error Rate](https://huggingface.co/spaces/evaluate-metric/wer).&#34;,
    # Summarization semantic robustness
    DELTA_ROUGE_SCORE: &#34;The performance change of the ROUGE-N score is measured.&#34;,
    DELTA_METEOR_SCORE: &#34;The performance change of the METEOR score is measured.&#34;,
    DELTA_BERT_SCORE: &#34;The performance change of the BERTscore is measured.&#34;,
    # QA semantic robustness
    DELTA_EXACT_MATCH_SCORE: &#34;The performance change of the Exact Match score is measured.&#34;,
    DELTA_QUASI_EXACT_MATCH_SCORE: &#34;The performance change of the Quasi Exact Match score is measured.&#34;,
    DELTA_F1_SCORE: &#34;The performance change of the F1 over Words score is measured.&#34;,
    # Classification semantic robustness
    DELTA_CLASSIFICATION_ACCURACY_SCORE: &#34;The score is the binary indicator on whether or not the model answer is correct.&#34;,
    # Toxicity
    TOXIGEN_SCORE_NAME: &#34;A binary score where 0 indicates no toxicity detected and 1 indicates toxicity detected.&#34;,
    DETOXIFY_SCORE_TOXICITY: &#34;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **toxicity**&#34;,
    DETOXIFY_SCORE_SEVERE_TOXICITY: &#34;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **severe_toxicity**&#34;,
    DETOXIFY_SCORE_OBSCENE: &#34;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **obscene**&#34;,
    DETOXIFY_SCORE_IDENTITY_ATTACK: &#34;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class **identity_attack**&#34;,
    DETOXIFY_SCORE_INSULT: &#34;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **insult**&#34;,
    DETOXIFY_SCORE_THREAT: &#34;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **threat**&#34;,
    DETOXIFY_SCORE_SEXUAL_EXPLICIT: &#34;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **sexual_explicit**&#34;,
}


class DatasetDetails(NamedTuple):
    name: str
    url: str
    description: str
    size: int


# Dataset details with the formatted names, URLs, descriptions and size
DATASET_DETAILS = {
    TREX: DatasetDetails(
        name=&#34;T-REx&#34;,
        url=&#34;https://hadyelsahar.github.io/t-rex/&#34;,
        description=&#34;A dataset which consists of knowledge triplets extracted from Wikipedia. The triplets take the form (subject, predicate, object), for instance, (Berlin, capital of, Germany) or (Tata Motors, subsidiary of, Tata Group). &#34;,
        size=32260,
    ),
    BOOLQ: DatasetDetails(
        name=&#34;BoolQ&#34;,
        url=&#34;https://github.com/google-research-datasets/boolean-questions&#34;,
        description=&#34;A dataset consisting of question-passage-answer triplets. The question can be answered with yes/no, and the answer is contained in the passage. The questions are provided anonymously and unsolicited by users of the Google search engine, and afterwards paired with a paragraph from a Wikipedia article containing the answer.&#34;,
        size=12697,
    ),
    TRIVIA_QA: DatasetDetails(
        name=&#34;TriviaQA&#34;,
        url=&#34;http://nlp.cs.washington.edu/triviaqa/&#34;,
        description=&#34;A dataset consisting of 95K question-answer pairs with with on average six supporting evidence documents per question, leading to ~650K question-passage-answer triplets. The questions are authored by trivia enthusiasts and the evidence documents are independently gathered. &#34;,
        size=156328,
    ),
    NATURAL_QUESTIONS: DatasetDetails(
        name=&#34;Natural Questions&#34;,
        url=&#34;https://github.com/google-research-datasets/natural-questions&#34;,
        description=&#34;A dataset consisting of ~320K question-passage-answer triplets. The questions are factual naturally-occurring questions. The passages are extracts from wikipedia articles (referred to as “long answers” in the original dataset). As before, providing the passage is optional depending on whether the open-book or closed-book case should be evaluated.&#34;,
        size=4289,
    ),
    CROWS_PAIRS: DatasetDetails(
        name=&#34;CrowS-Pairs&#34;,
        url=&#34;https://github.com/nyu-mll/crows-pairs&#34;,
        description=&#34;This dataset provides crowdsourced sentence pairs for the different categories along which stereotyping is to be measured.&#34;,
        size=1508,
    ),
    CNN_DAILY_MAIL: DatasetDetails(
        name=&#34;CNN/DailyMail&#34;,
        url=&#34;https://huggingface.co/datasets/cnn_dailymail&#34;,
        description=&#34;A dataset consisting of newspaper articles and their reference summaries. The reference summaries consist of highlights from the original article and are usually 2-4 sentences long.&#34;,
        size=287113,
    ),
    XSUM: DatasetDetails(
        name=&#34;XSUM&#34;,
        url=&#34;https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset&#34;,
        description=&#34;A dataset consisting of newspaper articles from the BBC and their reference summaries. The reference summaries consist of a single sentence: the boldfaced sentence at the begininning of each BBC article, provided by article’s authors.&#34;,
        size=204045,
    ),
    IMDB_MOVIE_REVIEWS: DatasetDetails(
        name=&#34;IMDB Sentiment Classification&#34;,
        url=&#34;https://ai.stanford.edu/~amaas/data/sentiment/&#34;,
        description=&#34;A dataset for binary sentiment classification containing 50k polar movie reviews with sentiment label.&#34;,
        size=25000,
    ),
    WOMENS_CLOTHING_ECOMMERCE_REVIEWS: DatasetDetails(
        name=&#34;Women&#39;s E-commerce Clothing Reviews&#34;,
        url=&#34;https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews&#34;,
        description=&#34;This dataset consists of clothing reviews, both as a text and numerical scores.&#34;,
        size=23486,
    ),
    BOLD: DatasetDetails(
        name=&#34;BOLD&#34;,
        url=&#34;https://github.com/amazon-science/bold&#34;,
        description=&#34;A large-scale dataset that consists of English prompts aimed at testing bias and toxicity generation across five domains: profession, gender, race, religion, and political ideology.&#34;,
        size=23679,
    ),
    WIKITEXT2: DatasetDetails(
        name=&#34;WikiText2&#34;,
        url=&#34;https://huggingface.co/datasets/wikitext&#34;,
        description=&#34;A dataset which consists of Good and Featured articles from Wikipedia. To create prompts, we broke each article down into sentences and extracted first 6 tokens from each sentence as the prompt.&#34;,
        size=86007,
    ),
    REAL_TOXICITY_PROMPTS: DatasetDetails(
        name=&#34;Real Toxicity Prompts&#34;,
        url=&#34;https://github.com/allenai/real-toxicity-prompts&#34;,
        description=&#34;A dataset of truncated sentence snippets from the web. &#34;,
        size=98243,
    ),
    REAL_TOXICITY_PROMPTS_CHALLENGING: DatasetDetails(
        name=&#34;Real Toxicity Prompts Challenging&#34;,
        url=&#34;https://github.com/allenai/real-toxicity-prompts&#34;,
        description=&#34;A dataset of truncated sentence snippets from the web. Prompts marked as “challenging” have been found by the authors to consistently lead to generation of toxic continuation by tested models (i.e., GPT-1, GPT-2, GPT-3, CTRL, CTRL-WIKI).&#34;,
        size=1199,
    ),
}

TREX_DESCRIPTION_EXAMPLES = &#34;We convert these predicates to prompts, e.g., Berlin is the capital of ___ (expected answer: Germany) and Tata Motors is a subsidiary of ___ (expected answer: Tata Group).&#34;</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.amazon_fmeval.reporting.constants.DatasetDetails"><code class="flex name class">
<span>class <span class="ident">DatasetDetails</span></span>
<span>(</span><span>name: str, url: str, description: str, size: int)</span>
</code></dt>
<dd>
<div class="desc"><p>DatasetDetails(name, url, description, size)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DatasetDetails(NamedTuple):
    name: str
    url: str
    description: str
    size: int</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="src.amazon_fmeval.reporting.constants.DatasetDetails.description"><code class="name">var <span class="ident">description</span> : str</code></dt>
<dd>
<div class="desc"><p>Alias for field number 2</p></div>
</dd>
<dt id="src.amazon_fmeval.reporting.constants.DatasetDetails.name"><code class="name">var <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
<dt id="src.amazon_fmeval.reporting.constants.DatasetDetails.size"><code class="name">var <span class="ident">size</span> : int</code></dt>
<dd>
<div class="desc"><p>Alias for field number 3</p></div>
</dd>
<dt id="src.amazon_fmeval.reporting.constants.DatasetDetails.url"><code class="name">var <span class="ident">url</span> : str</code></dt>
<dd>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
</dl>
</dd>
<dt id="src.amazon_fmeval.reporting.constants.ListType"><code class="flex name class">
<span>class <span class="ident">ListType</span></span>
<span>(</span><span>value, names=None, *, module=None, qualname=None, type=None, start=1)</span>
</code></dt>
<dd>
<div class="desc"><p>An enumeration.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ListType(Enum):
    BULLETED = &#34;bulleted&#34;
    NUMBERED = &#34;numbered&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="src.amazon_fmeval.reporting.constants.ListType.BULLETED"><code class="name">var <span class="ident">BULLETED</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.reporting.constants.ListType.NUMBERED"><code class="name">var <span class="ident">NUMBERED</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.amazon_fmeval.reporting" href="index.html">src.amazon_fmeval.reporting</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.amazon_fmeval.reporting.constants.DatasetDetails" href="#src.amazon_fmeval.reporting.constants.DatasetDetails">DatasetDetails</a></code></h4>
<ul class="">
<li><code><a title="src.amazon_fmeval.reporting.constants.DatasetDetails.description" href="#src.amazon_fmeval.reporting.constants.DatasetDetails.description">description</a></code></li>
<li><code><a title="src.amazon_fmeval.reporting.constants.DatasetDetails.name" href="#src.amazon_fmeval.reporting.constants.DatasetDetails.name">name</a></code></li>
<li><code><a title="src.amazon_fmeval.reporting.constants.DatasetDetails.size" href="#src.amazon_fmeval.reporting.constants.DatasetDetails.size">size</a></code></li>
<li><code><a title="src.amazon_fmeval.reporting.constants.DatasetDetails.url" href="#src.amazon_fmeval.reporting.constants.DatasetDetails.url">url</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.amazon_fmeval.reporting.constants.ListType" href="#src.amazon_fmeval.reporting.constants.ListType">ListType</a></code></h4>
<ul class="">
<li><code><a title="src.amazon_fmeval.reporting.constants.ListType.BULLETED" href="#src.amazon_fmeval.reporting.constants.ListType.BULLETED">BULLETED</a></code></li>
<li><code><a title="src.amazon_fmeval.reporting.constants.ListType.NUMBERED" href="#src.amazon_fmeval.reporting.constants.ListType.NUMBERED">NUMBERED</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>