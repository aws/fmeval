<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>src.amazon_fmeval.eval_algorithms.classification_accuracy API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.amazon_fmeval.eval_algorithms.classification_accuracy</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import logging
import warnings
from dataclasses import dataclass
from typing import Callable, Dict, List, Optional

import pandas as pd
from ray.data import Dataset
from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score

import amazon_fmeval.util as util
from amazon_fmeval.constants import (
    MODEL_OUTPUT_COLUMN_NAME,
    MODEL_INPUT_COLUMN_NAME,
    TARGET_OUTPUT_COLUMN_NAME,
    MEAN,
    CATEGORY_COLUMN_NAME,
)
from amazon_fmeval.data_loaders.util import get_dataset
from amazon_fmeval.data_loaders.data_config import DataConfig
from amazon_fmeval.eval_algorithms.eval_algorithm import (
    EvalAlgorithmInterface,
    EvalAlgorithmConfig,
)
from amazon_fmeval.eval_algorithms import (
    EvalAlgorithm,
    EvalOutput,
    EvalScore,
    EVAL_DATASETS,
    DATASET_CONFIGS,
    CategoryScore,
    get_default_prompt_template,
)
from amazon_fmeval.eval_algorithms.util import (
    generate_prompt_column_for_dataset,
    generate_model_predict_response_for_dataset,
    validate_dataset,
    category_wise_aggregation,
    save_dataset,
    generate_output_dataset_path,
)
from amazon_fmeval.exceptions import EvalAlgorithmClientError
from amazon_fmeval.model_runners.model_runner import ModelRunner
from amazon_fmeval.perf_util import timed_block

CLASSIFICATION_ACCURACY_SCORE = &#34;classification_accuracy_score&#34;
BALANCED_ACCURACY_SCORE = &#34;balanced_accuracy_score&#34;
PRECISION_SCORE = &#34;precision_score&#34;
RECALL_SCORE = &#34;recall_score&#34;

UNKNOWN_LABEL = &#34;unknown&#34;

PROMPT_COLUMN_NAME = &#34;prompt&#34;

CLASSIFIED_MODEL_OUTPUT_COLUMN_NAME = &#34;classified_model_output&#34;


CLASSIFICATION_ACCURACY_SCORES_TO_FUNCS: Dict[str, Callable[..., float]] = {
    BALANCED_ACCURACY_SCORE: balanced_accuracy_score,
    PRECISION_SCORE: precision_score,
    RECALL_SCORE: recall_score,
}
logger = logging.getLogger(__name__)


def convert_model_output_to_label(model_output: str, valid_labels: List[str]) -&gt; str:
    &#34;&#34;&#34;Convert model output to string class label. The model is expected to return a label directly (if it has a
    classification head), or a string containing a label (if it has a language modelling head). In the latter case we
    strip any additional text (e.g. &#34;The answer is 2.&#34; --&gt; &#34;2&#34;). If no valid labels is contained in the
    `model_output` an &#34;unknown&#34; label is returned. Users can define other `converter_fn`s, e.g. to translate a text
    label to string (&#34;NEGATIVE&#34; --&gt; &#34;0&#34;).

    :param model_output: Value returned by the model.
    :param valid_labels: Valid labels.
    :return: `model_output` transformed into a label
    &#34;&#34;&#34;
    # normalise to lowercase &amp; strip
    valid_labels = [label.lower().strip() for label in valid_labels]

    response_words = model_output.split(&#34; &#34;)
    predicted_labels = [word.lower().strip() for word in response_words if word.lower().strip() in valid_labels]
    # if there is more than one label in the model output we pick the first
    string_label = predicted_labels[0] if predicted_labels else UNKNOWN_LABEL

    return string_label


@dataclass(frozen=True)
class ClassificationAccuracyConfig(EvalAlgorithmConfig):
    &#34;&#34;&#34;
    Configuration for the Classification Accuracy Evaluation

    :param multiclass_average_strategy: `average` to be passed to sklearn&#39;s precision and recall scores.
        This determines how scores are aggregated in the multiclass classification setting
        (see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html).
        Options are {&#39;micro&#39;, &#39;macro&#39;, &#39;samples&#39;, &#39;weighted&#39;, &#39;binary&#39;} or None, default=&#39;micro&#39;.
    :param converter_fn: Function to process model output to labels, defaults to simple integer conversion.
    &#34;&#34;&#34;

    valid_labels: List[str]
    converter_fn: Callable[[str, List[str]], str] = convert_model_output_to_label
    multiclass_average_strategy: Optional[str] = &#34;micro&#34;

    def __post_init__(self):
        for i, label in enumerate(self.valid_labels):
            if not isinstance(label, str):
                warnings.warn(&#34;Valid labels should be strings, casting.&#34;)
                self.valid_labels[i] = str(label)


class ClassificationAccuracy(EvalAlgorithmInterface):
    eval_name = EvalAlgorithm.CLASSIFICATION_ACCURACY.value

    def __init__(self, eval_algorithm_config: ClassificationAccuracyConfig):
        &#34;&#34;&#34;Default constructor

        :param eval_algorithm_config: Classification Accuracy eval algorithm config.
        &#34;&#34;&#34;
        super().__init__(eval_algorithm_config)
        self._eval_algorithm_config = eval_algorithm_config

    def evaluate(
        self,
        model: Optional[ModelRunner] = None,
        dataset_config: Optional[DataConfig] = None,
        prompt_template: Optional[str] = None,
        save: bool = False,
        num_records=100,
    ) -&gt; List[EvalOutput]:
        &#34;&#34;&#34;
        Classification Accuracy evaluate.

        :param model: An instance of ModelRunner which is the model under evaluation
        :param dataset_config: The config to load the dataset to use for evaluation. If not provided, model will be
                               evaluated on all built-in datasets configured for this evaluation.
        :param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults
            will be used.
        :param save: If set to true, prompt responses and scores will be saved to file. The output is written to
                     EvalAlgorithmInterface.EVAL_RESULTS_PATH
        :param num_records: The number of records to be sampled randomly from the input dataset to perform the
                            evaluation
        :returns: List of EvalOutput objects. Current implementation returns only one score.
        &#34;&#34;&#34;
        if dataset_config:
            dataset_configs = [dataset_config]
        else:
            dataset_configs = [DATASET_CONFIGS[dataset_name] for dataset_name in EVAL_DATASETS[self.eval_name]]
        eval_outputs: List[EvalOutput] = []
        for dataset_config in dataset_configs:
            dataset = get_dataset(dataset_config, num_records)
            validate_dataset(dataset, [TARGET_OUTPUT_COLUMN_NAME, MODEL_INPUT_COLUMN_NAME])
            dataset_prompt_template = None
            if MODEL_OUTPUT_COLUMN_NAME not in dataset.columns():
                util.require(model, &#34;No ModelRunner provided. ModelRunner is required for inference on model_inputs&#34;)
                dataset_prompt_template = (
                    get_default_prompt_template(dataset_config.dataset_name) if not prompt_template else prompt_template
                )
                dataset = generate_prompt_column_for_dataset(
                    prompt_template=dataset_prompt_template,
                    data=dataset,
                    model_input_column_name=MODEL_INPUT_COLUMN_NAME,
                    prompt_column_name=PROMPT_COLUMN_NAME,
                )
                assert model  # to satisfy mypy
                dataset = generate_model_predict_response_for_dataset(
                    model=model,
                    data=dataset,
                    model_input_column_name=PROMPT_COLUMN_NAME,
                    model_output_column_name=MODEL_OUTPUT_COLUMN_NAME,
                )

            with timed_block(f&#34;Computing score and aggregation on dataset {dataset_config.dataset_name}&#34;, logger):

                def _generate_classified_model_output_column(df: pd.DataFrame) -&gt; pd.Series:  # pragma: no cover
                    return pd.Series(
                        data=[
                            self._eval_algorithm_config.converter_fn(
                                row[MODEL_OUTPUT_COLUMN_NAME], self._eval_algorithm_config.valid_labels
                            )
                            for index, row in df.iterrows()
                        ]
                    )

                dataset = dataset.add_column(
                    CLASSIFIED_MODEL_OUTPUT_COLUMN_NAME, _generate_classified_model_output_column
                )
                dataset = dataset.materialize()

                def _generate_classification_accuracy_column(df: pd.DataFrame) -&gt; pd.Series:  # pragma: no cover
                    return pd.Series(
                        data=[
                            int(row[CLASSIFIED_MODEL_OUTPUT_COLUMN_NAME] == str(row[TARGET_OUTPUT_COLUMN_NAME]))
                            for index, row in df.iterrows()
                        ]
                    )

                dataset = dataset.add_column(CLASSIFICATION_ACCURACY_SCORE, _generate_classification_accuracy_column)
                dataset = dataset.materialize()

                df = dataset.to_pandas()
                dataset_scores = [
                    EvalScore(name=CLASSIFICATION_ACCURACY_SCORE, value=dataset.mean(CLASSIFICATION_ACCURACY_SCORE))
                ]

                for eval_score, eval_fn in CLASSIFICATION_ACCURACY_SCORES_TO_FUNCS.items():
                    dataset_scores.append(
                        EvalScore(
                            name=eval_score,
                            value=self._get_score(
                                # TODO dataloader should ensure target output is string
                                y_true=df[TARGET_OUTPUT_COLUMN_NAME],
                                y_pred=df[CLASSIFIED_MODEL_OUTPUT_COLUMN_NAME],
                                eval_fn=eval_fn,
                            ),
                        )
                    )

                category_scores: Optional[Dict[str, CategoryScore]] = None
                if CATEGORY_COLUMN_NAME in dataset.columns():
                    category_scores = {
                        name: CategoryScore(name=name, scores=[]) for name in dataset.unique(CATEGORY_COLUMN_NAME)
                    }
                    category_aggregate: Dataset = category_wise_aggregation(
                        dataset, CLASSIFICATION_ACCURACY_SCORE, MEAN
                    )
                    for row in category_aggregate.iter_rows():
                        category_scores[row[CATEGORY_COLUMN_NAME]].scores.append(
                            EvalScore(
                                name=CLASSIFICATION_ACCURACY_SCORE, value=row[f&#34;mean({CLASSIFICATION_ACCURACY_SCORE})&#34;]
                            )
                        )
                        categorical_y_true = df.loc[
                            df[CATEGORY_COLUMN_NAME] == row[CATEGORY_COLUMN_NAME], TARGET_OUTPUT_COLUMN_NAME
                        ]
                        categorical_y_pred = df.loc[
                            df[CATEGORY_COLUMN_NAME] == row[CATEGORY_COLUMN_NAME], CLASSIFIED_MODEL_OUTPUT_COLUMN_NAME
                        ]
                        for eval_score, eval_fn in CLASSIFICATION_ACCURACY_SCORES_TO_FUNCS.items():
                            category_scores[row[CATEGORY_COLUMN_NAME]].scores.append(
                                EvalScore(
                                    name=eval_score,
                                    value=self._get_score(
                                        y_true=categorical_y_true, y_pred=categorical_y_pred, eval_fn=eval_fn
                                    ),
                                )
                            )

                eval_outputs.append(
                    EvalOutput(
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                        prompt_template=dataset_prompt_template,
                        dataset_scores=dataset_scores,
                        category_scores=list(category_scores.values()) if category_scores else None,
                        output_path=generate_output_dataset_path(
                            path_to_parent_dir=self._eval_results_path,
                            eval_name=self.eval_name,
                            dataset_name=dataset_config.dataset_name,
                        ),
                    )
                )
            if save:
                save_dataset(
                    dataset=dataset,
                    score_names=[CLASSIFICATION_ACCURACY_SCORE],
                    path=generate_output_dataset_path(
                        path_to_parent_dir=self._eval_results_path,
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                    ),
                )

        return eval_outputs

    def _get_score(self, y_true, y_pred, eval_fn: Callable[..., float]) -&gt; float:
        &#34;&#34;&#34;
        Method to generate accuracy score
        :param y_true: Ground truth (correct) target values.
        :param y_pred: Estimated targets as returned by a classifier.
        :param eval_fn: Score evaluate function.
        :returns: Computed score
        &#34;&#34;&#34;
        if eval_fn == recall_score or eval_fn == precision_score:
            return eval_fn(y_true, y_pred, average=self._eval_algorithm_config.multiclass_average_strategy)
        return eval_fn(y_true, y_pred)

    def evaluate_sample(self, target_output: str, model_output: str) -&gt; List[EvalScore]:  # type: ignore[override]
        &#34;&#34;&#34;
        Evaluate a single Classification record.

        :param model_output: An instance of ModelOutput which contains the responses from the model needed for this
                             evaluation.
        :param target_output: The expected responses from the model.
        :returns: A List of EvalScores computed for prompts and responses.
        &#34;&#34;&#34;
        if target_output is None:
            raise EvalAlgorithmClientError(
                &#34;Missing required input: target_output, for Classification Accuracy evaluate_sample&#34;
            )
        if model_output is None:
            raise EvalAlgorithmClientError(
                &#34;Missing required input: model_output, for Classification Accuracy evaluate_sample&#34;
            )
        return [
            EvalScore(
                name=CLASSIFICATION_ACCURACY_SCORE,
                value=int(
                    self._eval_algorithm_config.converter_fn(model_output, self._eval_algorithm_config.valid_labels)
                    == str(target_output)
                ),
            )
        ]</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.classification_accuracy.convert_model_output_to_label"><code class="name flex">
<span>def <span class="ident">convert_model_output_to_label</span></span>(<span>model_output: str, valid_labels: List[str]) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Convert model output to string class label. The model is expected to return a label directly (if it has a
classification head), or a string containing a label (if it has a language modelling head). In the latter case we
strip any additional text (e.g. "The answer is 2." &ndash;&gt; "2"). If no valid labels is contained in the
<code>model_output</code> an "unknown" label is returned. Users can define other <code>converter_fn</code>s, e.g. to translate a text
label to string ("NEGATIVE" &ndash;&gt; "0").</p>
<p>:param model_output: Value returned by the model.
:param valid_labels: Valid labels.
:return: <code>model_output</code> transformed into a label</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convert_model_output_to_label(model_output: str, valid_labels: List[str]) -&gt; str:
    &#34;&#34;&#34;Convert model output to string class label. The model is expected to return a label directly (if it has a
    classification head), or a string containing a label (if it has a language modelling head). In the latter case we
    strip any additional text (e.g. &#34;The answer is 2.&#34; --&gt; &#34;2&#34;). If no valid labels is contained in the
    `model_output` an &#34;unknown&#34; label is returned. Users can define other `converter_fn`s, e.g. to translate a text
    label to string (&#34;NEGATIVE&#34; --&gt; &#34;0&#34;).

    :param model_output: Value returned by the model.
    :param valid_labels: Valid labels.
    :return: `model_output` transformed into a label
    &#34;&#34;&#34;
    # normalise to lowercase &amp; strip
    valid_labels = [label.lower().strip() for label in valid_labels]

    response_words = model_output.split(&#34; &#34;)
    predicted_labels = [word.lower().strip() for word in response_words if word.lower().strip() in valid_labels]
    # if there is more than one label in the model output we pick the first
    string_label = predicted_labels[0] if predicted_labels else UNKNOWN_LABEL

    return string_label</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy"><code class="flex name class">
<span>class <span class="ident">ClassificationAccuracy</span></span>
<span>(</span><span>eval_algorithm_config: <a title="src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig" href="#src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig">ClassificationAccuracyConfig</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Interface class for eval algorithms.</p>
<p>Default constructor</p>
<p>:param eval_algorithm_config: Classification Accuracy eval algorithm config.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ClassificationAccuracy(EvalAlgorithmInterface):
    eval_name = EvalAlgorithm.CLASSIFICATION_ACCURACY.value

    def __init__(self, eval_algorithm_config: ClassificationAccuracyConfig):
        &#34;&#34;&#34;Default constructor

        :param eval_algorithm_config: Classification Accuracy eval algorithm config.
        &#34;&#34;&#34;
        super().__init__(eval_algorithm_config)
        self._eval_algorithm_config = eval_algorithm_config

    def evaluate(
        self,
        model: Optional[ModelRunner] = None,
        dataset_config: Optional[DataConfig] = None,
        prompt_template: Optional[str] = None,
        save: bool = False,
        num_records=100,
    ) -&gt; List[EvalOutput]:
        &#34;&#34;&#34;
        Classification Accuracy evaluate.

        :param model: An instance of ModelRunner which is the model under evaluation
        :param dataset_config: The config to load the dataset to use for evaluation. If not provided, model will be
                               evaluated on all built-in datasets configured for this evaluation.
        :param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults
            will be used.
        :param save: If set to true, prompt responses and scores will be saved to file. The output is written to
                     EvalAlgorithmInterface.EVAL_RESULTS_PATH
        :param num_records: The number of records to be sampled randomly from the input dataset to perform the
                            evaluation
        :returns: List of EvalOutput objects. Current implementation returns only one score.
        &#34;&#34;&#34;
        if dataset_config:
            dataset_configs = [dataset_config]
        else:
            dataset_configs = [DATASET_CONFIGS[dataset_name] for dataset_name in EVAL_DATASETS[self.eval_name]]
        eval_outputs: List[EvalOutput] = []
        for dataset_config in dataset_configs:
            dataset = get_dataset(dataset_config, num_records)
            validate_dataset(dataset, [TARGET_OUTPUT_COLUMN_NAME, MODEL_INPUT_COLUMN_NAME])
            dataset_prompt_template = None
            if MODEL_OUTPUT_COLUMN_NAME not in dataset.columns():
                util.require(model, &#34;No ModelRunner provided. ModelRunner is required for inference on model_inputs&#34;)
                dataset_prompt_template = (
                    get_default_prompt_template(dataset_config.dataset_name) if not prompt_template else prompt_template
                )
                dataset = generate_prompt_column_for_dataset(
                    prompt_template=dataset_prompt_template,
                    data=dataset,
                    model_input_column_name=MODEL_INPUT_COLUMN_NAME,
                    prompt_column_name=PROMPT_COLUMN_NAME,
                )
                assert model  # to satisfy mypy
                dataset = generate_model_predict_response_for_dataset(
                    model=model,
                    data=dataset,
                    model_input_column_name=PROMPT_COLUMN_NAME,
                    model_output_column_name=MODEL_OUTPUT_COLUMN_NAME,
                )

            with timed_block(f&#34;Computing score and aggregation on dataset {dataset_config.dataset_name}&#34;, logger):

                def _generate_classified_model_output_column(df: pd.DataFrame) -&gt; pd.Series:  # pragma: no cover
                    return pd.Series(
                        data=[
                            self._eval_algorithm_config.converter_fn(
                                row[MODEL_OUTPUT_COLUMN_NAME], self._eval_algorithm_config.valid_labels
                            )
                            for index, row in df.iterrows()
                        ]
                    )

                dataset = dataset.add_column(
                    CLASSIFIED_MODEL_OUTPUT_COLUMN_NAME, _generate_classified_model_output_column
                )
                dataset = dataset.materialize()

                def _generate_classification_accuracy_column(df: pd.DataFrame) -&gt; pd.Series:  # pragma: no cover
                    return pd.Series(
                        data=[
                            int(row[CLASSIFIED_MODEL_OUTPUT_COLUMN_NAME] == str(row[TARGET_OUTPUT_COLUMN_NAME]))
                            for index, row in df.iterrows()
                        ]
                    )

                dataset = dataset.add_column(CLASSIFICATION_ACCURACY_SCORE, _generate_classification_accuracy_column)
                dataset = dataset.materialize()

                df = dataset.to_pandas()
                dataset_scores = [
                    EvalScore(name=CLASSIFICATION_ACCURACY_SCORE, value=dataset.mean(CLASSIFICATION_ACCURACY_SCORE))
                ]

                for eval_score, eval_fn in CLASSIFICATION_ACCURACY_SCORES_TO_FUNCS.items():
                    dataset_scores.append(
                        EvalScore(
                            name=eval_score,
                            value=self._get_score(
                                # TODO dataloader should ensure target output is string
                                y_true=df[TARGET_OUTPUT_COLUMN_NAME],
                                y_pred=df[CLASSIFIED_MODEL_OUTPUT_COLUMN_NAME],
                                eval_fn=eval_fn,
                            ),
                        )
                    )

                category_scores: Optional[Dict[str, CategoryScore]] = None
                if CATEGORY_COLUMN_NAME in dataset.columns():
                    category_scores = {
                        name: CategoryScore(name=name, scores=[]) for name in dataset.unique(CATEGORY_COLUMN_NAME)
                    }
                    category_aggregate: Dataset = category_wise_aggregation(
                        dataset, CLASSIFICATION_ACCURACY_SCORE, MEAN
                    )
                    for row in category_aggregate.iter_rows():
                        category_scores[row[CATEGORY_COLUMN_NAME]].scores.append(
                            EvalScore(
                                name=CLASSIFICATION_ACCURACY_SCORE, value=row[f&#34;mean({CLASSIFICATION_ACCURACY_SCORE})&#34;]
                            )
                        )
                        categorical_y_true = df.loc[
                            df[CATEGORY_COLUMN_NAME] == row[CATEGORY_COLUMN_NAME], TARGET_OUTPUT_COLUMN_NAME
                        ]
                        categorical_y_pred = df.loc[
                            df[CATEGORY_COLUMN_NAME] == row[CATEGORY_COLUMN_NAME], CLASSIFIED_MODEL_OUTPUT_COLUMN_NAME
                        ]
                        for eval_score, eval_fn in CLASSIFICATION_ACCURACY_SCORES_TO_FUNCS.items():
                            category_scores[row[CATEGORY_COLUMN_NAME]].scores.append(
                                EvalScore(
                                    name=eval_score,
                                    value=self._get_score(
                                        y_true=categorical_y_true, y_pred=categorical_y_pred, eval_fn=eval_fn
                                    ),
                                )
                            )

                eval_outputs.append(
                    EvalOutput(
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                        prompt_template=dataset_prompt_template,
                        dataset_scores=dataset_scores,
                        category_scores=list(category_scores.values()) if category_scores else None,
                        output_path=generate_output_dataset_path(
                            path_to_parent_dir=self._eval_results_path,
                            eval_name=self.eval_name,
                            dataset_name=dataset_config.dataset_name,
                        ),
                    )
                )
            if save:
                save_dataset(
                    dataset=dataset,
                    score_names=[CLASSIFICATION_ACCURACY_SCORE],
                    path=generate_output_dataset_path(
                        path_to_parent_dir=self._eval_results_path,
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                    ),
                )

        return eval_outputs

    def _get_score(self, y_true, y_pred, eval_fn: Callable[..., float]) -&gt; float:
        &#34;&#34;&#34;
        Method to generate accuracy score
        :param y_true: Ground truth (correct) target values.
        :param y_pred: Estimated targets as returned by a classifier.
        :param eval_fn: Score evaluate function.
        :returns: Computed score
        &#34;&#34;&#34;
        if eval_fn == recall_score or eval_fn == precision_score:
            return eval_fn(y_true, y_pred, average=self._eval_algorithm_config.multiclass_average_strategy)
        return eval_fn(y_true, y_pred)

    def evaluate_sample(self, target_output: str, model_output: str) -&gt; List[EvalScore]:  # type: ignore[override]
        &#34;&#34;&#34;
        Evaluate a single Classification record.

        :param model_output: An instance of ModelOutput which contains the responses from the model needed for this
                             evaluation.
        :param target_output: The expected responses from the model.
        :returns: A List of EvalScores computed for prompts and responses.
        &#34;&#34;&#34;
        if target_output is None:
            raise EvalAlgorithmClientError(
                &#34;Missing required input: target_output, for Classification Accuracy evaluate_sample&#34;
            )
        if model_output is None:
            raise EvalAlgorithmClientError(
                &#34;Missing required input: model_output, for Classification Accuracy evaluate_sample&#34;
            )
        return [
            EvalScore(
                name=CLASSIFICATION_ACCURACY_SCORE,
                value=int(
                    self._eval_algorithm_config.converter_fn(model_output, self._eval_algorithm_config.valid_labels)
                    == str(target_output)
                ),
            )
        ]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>amazon_fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface</li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy.eval_name"><code class="name">var <span class="ident">eval_name</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, model: Optional[amazon_fmeval.model_runners.model_runner.ModelRunner] = None, dataset_config: Optional[amazon_fmeval.data_loaders.data_config.DataConfig] = None, prompt_template: Optional[str] = None, save: bool = False, num_records=100) ‑> List[amazon_fmeval.eval_algorithms.EvalOutput]</span>
</code></dt>
<dd>
<div class="desc"><p>Classification Accuracy evaluate.</p>
<p>:param model: An instance of ModelRunner which is the model under evaluation
:param dataset_config: The config to load the dataset to use for evaluation. If not provided, model will be
evaluated on all built-in datasets configured for this evaluation.
:param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults
will be used.
:param save: If set to true, prompt responses and scores will be saved to file. The output is written to
EvalAlgorithmInterface.EVAL_RESULTS_PATH
:param num_records: The number of records to be sampled randomly from the input dataset to perform the
evaluation
:returns: List of EvalOutput objects. Current implementation returns only one score.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(
    self,
    model: Optional[ModelRunner] = None,
    dataset_config: Optional[DataConfig] = None,
    prompt_template: Optional[str] = None,
    save: bool = False,
    num_records=100,
) -&gt; List[EvalOutput]:
    &#34;&#34;&#34;
    Classification Accuracy evaluate.

    :param model: An instance of ModelRunner which is the model under evaluation
    :param dataset_config: The config to load the dataset to use for evaluation. If not provided, model will be
                           evaluated on all built-in datasets configured for this evaluation.
    :param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults
        will be used.
    :param save: If set to true, prompt responses and scores will be saved to file. The output is written to
                 EvalAlgorithmInterface.EVAL_RESULTS_PATH
    :param num_records: The number of records to be sampled randomly from the input dataset to perform the
                        evaluation
    :returns: List of EvalOutput objects. Current implementation returns only one score.
    &#34;&#34;&#34;
    if dataset_config:
        dataset_configs = [dataset_config]
    else:
        dataset_configs = [DATASET_CONFIGS[dataset_name] for dataset_name in EVAL_DATASETS[self.eval_name]]
    eval_outputs: List[EvalOutput] = []
    for dataset_config in dataset_configs:
        dataset = get_dataset(dataset_config, num_records)
        validate_dataset(dataset, [TARGET_OUTPUT_COLUMN_NAME, MODEL_INPUT_COLUMN_NAME])
        dataset_prompt_template = None
        if MODEL_OUTPUT_COLUMN_NAME not in dataset.columns():
            util.require(model, &#34;No ModelRunner provided. ModelRunner is required for inference on model_inputs&#34;)
            dataset_prompt_template = (
                get_default_prompt_template(dataset_config.dataset_name) if not prompt_template else prompt_template
            )
            dataset = generate_prompt_column_for_dataset(
                prompt_template=dataset_prompt_template,
                data=dataset,
                model_input_column_name=MODEL_INPUT_COLUMN_NAME,
                prompt_column_name=PROMPT_COLUMN_NAME,
            )
            assert model  # to satisfy mypy
            dataset = generate_model_predict_response_for_dataset(
                model=model,
                data=dataset,
                model_input_column_name=PROMPT_COLUMN_NAME,
                model_output_column_name=MODEL_OUTPUT_COLUMN_NAME,
            )

        with timed_block(f&#34;Computing score and aggregation on dataset {dataset_config.dataset_name}&#34;, logger):

            def _generate_classified_model_output_column(df: pd.DataFrame) -&gt; pd.Series:  # pragma: no cover
                return pd.Series(
                    data=[
                        self._eval_algorithm_config.converter_fn(
                            row[MODEL_OUTPUT_COLUMN_NAME], self._eval_algorithm_config.valid_labels
                        )
                        for index, row in df.iterrows()
                    ]
                )

            dataset = dataset.add_column(
                CLASSIFIED_MODEL_OUTPUT_COLUMN_NAME, _generate_classified_model_output_column
            )
            dataset = dataset.materialize()

            def _generate_classification_accuracy_column(df: pd.DataFrame) -&gt; pd.Series:  # pragma: no cover
                return pd.Series(
                    data=[
                        int(row[CLASSIFIED_MODEL_OUTPUT_COLUMN_NAME] == str(row[TARGET_OUTPUT_COLUMN_NAME]))
                        for index, row in df.iterrows()
                    ]
                )

            dataset = dataset.add_column(CLASSIFICATION_ACCURACY_SCORE, _generate_classification_accuracy_column)
            dataset = dataset.materialize()

            df = dataset.to_pandas()
            dataset_scores = [
                EvalScore(name=CLASSIFICATION_ACCURACY_SCORE, value=dataset.mean(CLASSIFICATION_ACCURACY_SCORE))
            ]

            for eval_score, eval_fn in CLASSIFICATION_ACCURACY_SCORES_TO_FUNCS.items():
                dataset_scores.append(
                    EvalScore(
                        name=eval_score,
                        value=self._get_score(
                            # TODO dataloader should ensure target output is string
                            y_true=df[TARGET_OUTPUT_COLUMN_NAME],
                            y_pred=df[CLASSIFIED_MODEL_OUTPUT_COLUMN_NAME],
                            eval_fn=eval_fn,
                        ),
                    )
                )

            category_scores: Optional[Dict[str, CategoryScore]] = None
            if CATEGORY_COLUMN_NAME in dataset.columns():
                category_scores = {
                    name: CategoryScore(name=name, scores=[]) for name in dataset.unique(CATEGORY_COLUMN_NAME)
                }
                category_aggregate: Dataset = category_wise_aggregation(
                    dataset, CLASSIFICATION_ACCURACY_SCORE, MEAN
                )
                for row in category_aggregate.iter_rows():
                    category_scores[row[CATEGORY_COLUMN_NAME]].scores.append(
                        EvalScore(
                            name=CLASSIFICATION_ACCURACY_SCORE, value=row[f&#34;mean({CLASSIFICATION_ACCURACY_SCORE})&#34;]
                        )
                    )
                    categorical_y_true = df.loc[
                        df[CATEGORY_COLUMN_NAME] == row[CATEGORY_COLUMN_NAME], TARGET_OUTPUT_COLUMN_NAME
                    ]
                    categorical_y_pred = df.loc[
                        df[CATEGORY_COLUMN_NAME] == row[CATEGORY_COLUMN_NAME], CLASSIFIED_MODEL_OUTPUT_COLUMN_NAME
                    ]
                    for eval_score, eval_fn in CLASSIFICATION_ACCURACY_SCORES_TO_FUNCS.items():
                        category_scores[row[CATEGORY_COLUMN_NAME]].scores.append(
                            EvalScore(
                                name=eval_score,
                                value=self._get_score(
                                    y_true=categorical_y_true, y_pred=categorical_y_pred, eval_fn=eval_fn
                                ),
                            )
                        )

            eval_outputs.append(
                EvalOutput(
                    eval_name=self.eval_name,
                    dataset_name=dataset_config.dataset_name,
                    prompt_template=dataset_prompt_template,
                    dataset_scores=dataset_scores,
                    category_scores=list(category_scores.values()) if category_scores else None,
                    output_path=generate_output_dataset_path(
                        path_to_parent_dir=self._eval_results_path,
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                    ),
                )
            )
        if save:
            save_dataset(
                dataset=dataset,
                score_names=[CLASSIFICATION_ACCURACY_SCORE],
                path=generate_output_dataset_path(
                    path_to_parent_dir=self._eval_results_path,
                    eval_name=self.eval_name,
                    dataset_name=dataset_config.dataset_name,
                ),
            )

    return eval_outputs</code></pre>
</details>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy.evaluate_sample"><code class="name flex">
<span>def <span class="ident">evaluate_sample</span></span>(<span>self, target_output: str, model_output: str) ‑> List[amazon_fmeval.eval_algorithms.EvalScore]</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate a single Classification record.</p>
<p>:param model_output: An instance of ModelOutput which contains the responses from the model needed for this
evaluation.
:param target_output: The expected responses from the model.
:returns: A List of EvalScores computed for prompts and responses.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_sample(self, target_output: str, model_output: str) -&gt; List[EvalScore]:  # type: ignore[override]
    &#34;&#34;&#34;
    Evaluate a single Classification record.

    :param model_output: An instance of ModelOutput which contains the responses from the model needed for this
                         evaluation.
    :param target_output: The expected responses from the model.
    :returns: A List of EvalScores computed for prompts and responses.
    &#34;&#34;&#34;
    if target_output is None:
        raise EvalAlgorithmClientError(
            &#34;Missing required input: target_output, for Classification Accuracy evaluate_sample&#34;
        )
    if model_output is None:
        raise EvalAlgorithmClientError(
            &#34;Missing required input: model_output, for Classification Accuracy evaluate_sample&#34;
        )
    return [
        EvalScore(
            name=CLASSIFICATION_ACCURACY_SCORE,
            value=int(
                self._eval_algorithm_config.converter_fn(model_output, self._eval_algorithm_config.valid_labels)
                == str(target_output)
            ),
        )
    ]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig"><code class="flex name class">
<span>class <span class="ident">ClassificationAccuracyConfig</span></span>
<span>(</span><span>valid_labels: List[str], converter_fn: Callable[[str, List[str]], str] = &lt;function convert_model_output_to_label&gt;, multiclass_average_strategy: Optional[str] = 'micro')</span>
</code></dt>
<dd>
<div class="desc"><p>Configuration for the Classification Accuracy Evaluation</p>
<p>:param multiclass_average_strategy: <code>average</code> to be passed to sklearn's precision and recall scores.
This determines how scores are aggregated in the multiclass classification setting
(see <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html</a>).
Options are {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, default='micro'.
:param converter_fn: Function to process model output to labels, defaults to simple integer conversion.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass(frozen=True)
class ClassificationAccuracyConfig(EvalAlgorithmConfig):
    &#34;&#34;&#34;
    Configuration for the Classification Accuracy Evaluation

    :param multiclass_average_strategy: `average` to be passed to sklearn&#39;s precision and recall scores.
        This determines how scores are aggregated in the multiclass classification setting
        (see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html).
        Options are {&#39;micro&#39;, &#39;macro&#39;, &#39;samples&#39;, &#39;weighted&#39;, &#39;binary&#39;} or None, default=&#39;micro&#39;.
    :param converter_fn: Function to process model output to labels, defaults to simple integer conversion.
    &#34;&#34;&#34;

    valid_labels: List[str]
    converter_fn: Callable[[str, List[str]], str] = convert_model_output_to_label
    multiclass_average_strategy: Optional[str] = &#34;micro&#34;

    def __post_init__(self):
        for i, label in enumerate(self.valid_labels):
            if not isinstance(label, str):
                warnings.warn(&#34;Valid labels should be strings, casting.&#34;)
                self.valid_labels[i] = str(label)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>amazon_fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmConfig</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig.multiclass_average_strategy"><code class="name">var <span class="ident">multiclass_average_strategy</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig.valid_labels"><code class="name">var <span class="ident">valid_labels</span> : List[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig.converter_fn"><code class="name flex">
<span>def <span class="ident">converter_fn</span></span>(<span>model_output: str, valid_labels: List[str]) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Convert model output to string class label. The model is expected to return a label directly (if it has a
classification head), or a string containing a label (if it has a language modelling head). In the latter case we
strip any additional text (e.g. "The answer is 2." &ndash;&gt; "2"). If no valid labels is contained in the
<code>model_output</code> an "unknown" label is returned. Users can define other <code>converter_fn</code>s, e.g. to translate a text
label to string ("NEGATIVE" &ndash;&gt; "0").</p>
<p>:param model_output: Value returned by the model.
:param valid_labels: Valid labels.
:return: <code>model_output</code> transformed into a label</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convert_model_output_to_label(model_output: str, valid_labels: List[str]) -&gt; str:
    &#34;&#34;&#34;Convert model output to string class label. The model is expected to return a label directly (if it has a
    classification head), or a string containing a label (if it has a language modelling head). In the latter case we
    strip any additional text (e.g. &#34;The answer is 2.&#34; --&gt; &#34;2&#34;). If no valid labels is contained in the
    `model_output` an &#34;unknown&#34; label is returned. Users can define other `converter_fn`s, e.g. to translate a text
    label to string (&#34;NEGATIVE&#34; --&gt; &#34;0&#34;).

    :param model_output: Value returned by the model.
    :param valid_labels: Valid labels.
    :return: `model_output` transformed into a label
    &#34;&#34;&#34;
    # normalise to lowercase &amp; strip
    valid_labels = [label.lower().strip() for label in valid_labels]

    response_words = model_output.split(&#34; &#34;)
    predicted_labels = [word.lower().strip() for word in response_words if word.lower().strip() in valid_labels]
    # if there is more than one label in the model output we pick the first
    string_label = predicted_labels[0] if predicted_labels else UNKNOWN_LABEL

    return string_label</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.amazon_fmeval.eval_algorithms" href="index.html">src.amazon_fmeval.eval_algorithms</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.amazon_fmeval.eval_algorithms.classification_accuracy.convert_model_output_to_label" href="#src.amazon_fmeval.eval_algorithms.classification_accuracy.convert_model_output_to_label">convert_model_output_to_label</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy" href="#src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy">ClassificationAccuracy</a></code></h4>
<ul class="">
<li><code><a title="src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy.eval_name" href="#src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy.eval_name">eval_name</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy.evaluate" href="#src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy.evaluate">evaluate</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy.evaluate_sample" href="#src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy.evaluate_sample">evaluate_sample</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig" href="#src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig">ClassificationAccuracyConfig</a></code></h4>
<ul class="">
<li><code><a title="src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig.converter_fn" href="#src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig.converter_fn">converter_fn</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig.multiclass_average_strategy" href="#src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig.multiclass_average_strategy">multiclass_average_strategy</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig.valid_labels" href="#src.amazon_fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig.valid_labels">valid_labels</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>