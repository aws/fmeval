<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>src.amazon_fmeval.eval_algorithms.util API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.amazon_fmeval.eval_algorithms.util</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import json
import logging
import os
import pandas as pd
import ray.data
import multiprocessing as mp


import amazon_fmeval.util as util

from ray.data import Dataset
from collections import OrderedDict
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, Union
from amazon_fmeval.constants import (
    CATEGORY_COLUMN_NAME,
    EVAL_OUTPUT_RECORDS_BATCH_SIZE,
    MEAN,
    PARALLELIZATION_FACTOR,
    NUM_ROWS_DETERMINISTIC,
)
from amazon_fmeval.eval_algorithms import EvalScore, CategoryScore
from amazon_fmeval.exceptions import EvalAlgorithmInternalError
from amazon_fmeval.model_runners.composers.composers import PromptComposer
from amazon_fmeval.model_runners.model_runner import ModelRunner
from amazon_fmeval.perf_util import timed_block


logger = logging.getLogger(__name__)


def get_num_actors():
    try:
        num_actors = (
            int(os.environ[PARALLELIZATION_FACTOR]) if PARALLELIZATION_FACTOR in os.environ else (mp.cpu_count() - 1)
        )
    except ValueError:
        num_actors = mp.cpu_count() - 1
    return num_actors


def generate_model_predict_response_for_dataset(
    model: ModelRunner,
    data: Dataset,
    model_input_column_name: str,
    model_output_column_name: Optional[str] = None,
    model_log_probability_column_name: Optional[str] = None,
) -&gt; Dataset:
    &#34;&#34;&#34;
    Runs the model on the given data. Output will be written to the
    `model_output_column_name` column, and log_probability will be
    written to the `model_log_probability_column_name` column.

    :param model: ModelRunner to get predictions from.
    :param data: The dataset containing model inputs to feed to `model`.
    :param model_input_column_name: The name of the column containing the model input.
    :param model_output_column_name: The name of the column to write the model output to.
    :param model_log_probability_column_name: The name of the column to write the model log probability to.
    :return: The dataset with a model output column and model log probability column added.
        Note that both columns are optional, i.e. it is possible that a model output
        column is added, but a log probability column is not added (and vice versa).
    &#34;&#34;&#34;
    with timed_block(f&#34;Performing inference on dataset on {model}&#34;, logger):

        class ModelRunnerWrapper:  # pragma: no cover
            &#34;&#34;&#34;
            This class represents the Ray Actor that gets model predictions
            by feeding model inputs from the dataset to the model runner.

            We use Ray Actors instead of Tasks because the Actor approach minimizes
            the number of times that the ModelRunner `model` gets deserialized.
            With Tasks, Ray will serialize and deserialize `model` for every single
            prediction. With Actors, `model` gets deserialized once per Actor when
            the Actor gets initialized.
            &#34;&#34;&#34;

            def __init__(self):
                self.model_runner = model
                logger.setLevel(logging.DEBUG)

            def __call__(self, row: Dict[str, Any]) -&gt; Dict[str, Any]:
                predict_output = self.model_runner.predict(row[model_input_column_name])
                if model_output_column_name:
                    row[model_output_column_name] = predict_output[0]
                if model_log_probability_column_name:
                    row[model_log_probability_column_name] = predict_output[1]
                return row

        data = data.map(
            ModelRunnerWrapper, compute=ray.data.ActorPoolStrategy(size=get_num_actors())  # type: ignore[arg-type]
        ).materialize()
    return data


def generate_prompt_column_for_dataset(
    prompt_template: str, data: Dataset, model_input_column_name: str, prompt_column_name: str
) -&gt; Dataset:
    &#34;&#34;&#34;
    Generates prompts column for a given input dataset and prompt_template
    :param prompt_template: Prompt template
    :param data: the dataset where each instance is a row in the dataset.
    :param model_input_column_name: the name of the column containing the model input.
    :param prompt_column_name: Output column name to which composed prompts are added
    :return: the dataset with the composed prompts added.
    &#34;&#34;&#34;
    with timed_block(f&#34;Generating prompt column&#34;, logger):
        prompt_composer = PromptComposer(prompt_template)

        def _generate_prompt_column(df: pd.DataFrame) -&gt; pd.Series:  # pragma: no cover
            &#34;&#34;&#34;
            Map function generating the prompt column values given a batch of records in pandas format.
            &#34;&#34;&#34;
            return pd.Series(
                data=[prompt_composer.compose(row[model_input_column_name]) for index, row in df.iterrows()]
            )

        data = data.add_column(prompt_column_name, _generate_prompt_column).materialize()
    return data


def validate_dataset(dataset: Dataset, column_names: List[str]):
    &#34;&#34;&#34;
    Util function to validate that dataset contains the required column names.

    :param dataset: Input ray dataset
    :param column_names: names of the columns that must be present in the dataset
    :raises: EvalAlgorithmClientError for an invalid dataset
    &#34;&#34;&#34;
    for column_name in column_names:
        util.require(
            column_name in dataset.columns(),
            f&#34;Missing required column: {column_name}, for evaluate() method&#34;,
        )


def aggregate_evaluation_scores(
    dataset: Dataset, score_column_names: List[str], agg_method: str
) -&gt; Tuple[List[EvalScore], Optional[List[CategoryScore]]]:
    &#34;&#34;&#34;
    Factual knowledge eval algo aggregation method.

    :param dataset: ray dataset with eval scores
    :param score_column_names: a list of column names which contain the scores

    :return a tuple containing dataset and category level scores
    &#34;&#34;&#34;
    dataset_scores = [
        EvalScore(name=score_column_name, value=dataset_aggregation(dataset, score_column_name, agg_method))
        for score_column_name in score_column_names
    ]
    category_scores: Optional[Dict[str, CategoryScore]] = None
    if CATEGORY_COLUMN_NAME in dataset.columns():
        category_scores = {name: CategoryScore(name=name, scores=[]) for name in dataset.unique(CATEGORY_COLUMN_NAME)}
        for score_column_name in score_column_names:
            category_aggregate: Dataset = category_wise_aggregation(dataset, score_column_name, agg_method)
            for row in category_aggregate.iter_rows():
                category_scores[row[CATEGORY_COLUMN_NAME]].scores.append(
                    EvalScore(name=score_column_name, value=row[f&#34;mean({score_column_name})&#34;])
                )

    return dataset_scores, list(category_scores.values()) if category_scores else None


def dataset_aggregation(dataset: Dataset, score_column_name: str, agg_method: str) -&gt; float:
    if agg_method == MEAN:
        aggregate = dataset.mean(score_column_name)
        assert isinstance(aggregate, float)
        return aggregate
    else:
        raise EvalAlgorithmInternalError(f&#34;Aggregation method {agg_method} is not supported&#34;)


def category_wise_aggregation(dataset: Dataset, score_column_name: str, agg_method: str) -&gt; Dataset:
    category_aggregate: Dataset = dataset.groupby(CATEGORY_COLUMN_NAME)  # type: ignore
    if agg_method == MEAN:
        category_aggregate = category_aggregate.mean(score_column_name)
    else:
        raise EvalAlgorithmInternalError(f&#34;Aggregation method {agg_method} is not supported&#34;)
    return category_aggregate


@dataclass
class EvalOutputRecord:
    &#34;&#34;&#34;
    The schema used to define the records that get written
    to a JSON Lines file when `save_dataset` is called.

    :param model_input: the model input
    :param model_output: the model output
    :param model_log_probability: the model log probability
    :param target_output: the target output
    :param category: the category
    :param sent_more_input: the &#34;sent more&#34; input (used by Prompt stereotyping)
    :param sent_less_input: the &#34;sent less&#34; input (used by Prompt stereotyping)
    :param sent_more_input_prob: the &#34;sent more&#34; input probability (used by Prompt stereotyping)
    :param sent_less_input_prob: the &#34;sent less&#34; input probability (used by Prompt stereotyping)
    :param sent_more_output: the &#34;sent more&#34; output (used by Prompt stereotyping)
    :param sent_less_output: the &#34;sent less&#34; output (used by Prompt stereotyping)

    IMPORTANT:
        The attributes of this class MUST match the values of the
        column name constants in COLUMN_NAMES in src/constants.py.

        Reason:
        The `from_row` method validates the column names included
        in its `row` input, making sure that these column names
        match the attribute names of this class (this validation
        only occurs for column names that don&#39;t correspond to score
        names).

        Since the `row` input comes from a Ray Dataset produced by
        the `evaluate` method of an `EvalAlgorithmInterface`, the column
        names in the row must come from COLUMN_NAMES in src/constants.py.

        Thus, the attribute names of this class must match the constants
        in COLUMN_NAMES in order for the validation to make sense.
    &#34;&#34;&#34;

    scores: List[EvalScore]
    model_input: Optional[str] = None
    model_output: Optional[str] = None
    model_log_probability: Optional[float] = None
    target_output: Optional[str] = None
    category: Optional[str] = None
    sent_more_input: Optional[str] = None
    sent_less_input: Optional[str] = None
    sent_more_input_prob: Optional[str] = None
    sent_less_input_prob: Optional[str] = None
    sent_more_output: Optional[str] = None
    sent_less_output: Optional[str] = None
    prompt: Optional[str] = None
    sent_more_prompt: Optional[str] = None
    sent_less_prompt: Optional[str] = None

    def __str__(self):
        return json.dumps(self._to_dict())

    def _to_dict(self):
        &#34;&#34;&#34;
        Returns a dictionary representation of this instance,
        to be used when writing this object to JSON Lines.

        Note that attributes with value None are not included
        in the JSON representation. Additionally, we want the
        key &#34;scores&#34; to appear last in the JSON representation,
        but this attribute appears first in EvalOutputRecord&#39;s
        class definition, hence the sorting code below.
        &#34;&#34;&#34;
        attributes = list(self.__dict__.keys())
        attributes.sort(key=lambda x: x == &#34;scores&#34;)
        json_obj = OrderedDict(  # regular Dicts don&#39;t guarantee key order
            (attr, self.__dict__[attr]) for attr in attributes if self.__dict__[attr] is not None
        )
        json_obj[&#34;scores&#34;] = [eval_score.__dict__ for eval_score in json_obj[&#34;scores&#34;]]
        return json_obj

    @staticmethod
    def from_row(row: Dict[str, Union[str, float]], score_names: List[str]) -&gt; &#34;EvalOutputRecord&#34;:
        &#34;&#34;&#34;
        Returns an instance of EvalOutputRecord, created from a Ray Dataset row (represented as a dict).

        Example input:
            row = {
                &#34;model_input&#34;: &#34;input&#34;,
                &#34;model_output&#34;: &#34;output&#34;,
                &#34;rouge&#34;: 0.42,
                &#34;bert&#34;: 0.162
            }

        Corresponding output:
            EvalOutputRecord(
                model_input=&#34;input&#34;,
                model_output=&#34;output&#34;,
                scores=[
                    EvalScore(name=&#34;rouge&#34;, value=0.42),
                    EvalScore(name=&#34;bert&#34;, value=0.162)
                ]
            )

        :param row: a Ray Dataset row represented as a dict
        :param score_names: column names included in the Ray Dataset that `row`
            is a sample of that correspond to evaluation algorithm scores
        :returns: an instance of EvalOutputRecord corresponding to `row`
        &#34;&#34;&#34;
        eval_output_record_attribute_names = set(EvalOutputRecord.__annotations__.keys())
        non_score_columns = {}
        scores = []
        for column_name, value in row.items():
            if column_name not in score_names:  # pragma: no branch
                if column_name in eval_output_record_attribute_names:  # pragma: no branch
                    non_score_columns[column_name] = value
            else:
                assert isinstance(value, float) or isinstance(value, int)  # to satisfy Mypy
                scores.append(EvalScore(name=column_name, value=value))

        return EvalOutputRecord(
            scores=scores,
            **non_score_columns,  # type: ignore
        )


def save_dataset(dataset: Dataset, score_names: List[str], path: str) -&gt; None:  # pragma: no cover
    &#34;&#34;&#34;
    Writes the dataset to a JSON Lines file, where each JSON Lines object
    follows the schema defined by `EvalOutputRecord`.

    :param dataset: a Ray Dataset that is produced during the execution of
        an EvalAlgorithmInterface&#39;s `evaluate` method. This dataset is expected
        to include columns for every score computed by the evaluation algorithm.
    :param score_names: the names of the score columns in `dataset`
    :param path: a local file path to write the dataset to. The file name specified
        by this argument may not end in the extension `.jsonl`. In this case,
        we append the extension ourselves.


        Example Dataset:
         ________________________________________
        | &#34;model_input&#34; | &#34;rouge&#34; | &#34;bert_score&#34;|
        ----------------------------------------
        |   &#34;hello&#34;    |   0.5   |     0.42    |
        ---------------------------------------
        |   &#34;world&#34;   |  0.314  |    0.271    |
        ---------------------------------------

        Corresponding Json Lines file contents:
        {&#34;model_input&#34; : &#34;hello&#34;, &#34;scores&#34; : [{&#34;name&#34;: &#34;rouge&#34;, &#34;value&#34;: 0.5}, {&#34;name&#34;: &#34;bert_score&#34;, &#34;value&#34;: 0.42}]}
        {&#34;model_input&#34; : &#34;world&#34;, &#34;scores&#34; : [{&#34;name&#34;: &#34;rouge&#34;, &#34;value&#34;: 0.314}, {&#34;name&#34;: &#34;bert_score&#34;, &#34;value&#34;: 0.271}]}
    &#34;&#34;&#34;
    with timed_block(f&#34;Saving dataset to file&#34;, logger):
        # We need the outer dict that wraps the EvalOutputRecord because map() requires
        # whatever is returned from the lambda function to be a dict
        dataset = dataset.map(lambda row: {&#34;record&#34;: EvalOutputRecord.from_row(row, score_names)})
        # Without this line, dataset.iter_rows() below is not guaranteed to return the rows
        # in the same order that they appear in `dataset`.
        dataset.materialize()

        path_to_parent_dir = os.path.dirname(path)
        file_name = os.path.basename(path)
        file_name_without_extension = os.path.splitext(file_name)[0]
        full_path = f&#34;{path_to_parent_dir}/{file_name_without_extension}.jsonl&#34;
        with open(full_path, &#34;w&#34;) as fh:
            records = []
            for dataset_row in dataset.iter_rows():
                record = dataset_row[&#34;record&#34;]
                records.append(str(record))
                if len(records) == EVAL_OUTPUT_RECORDS_BATCH_SIZE:
                    fh.write(&#34;\n&#34;.join(records) + &#34;\n&#34;)
                    records = []
            if records:  # pragma: no branch
                fh.write(&#34;\n&#34;.join(records))  # handle the last batch


def generate_output_dataset_path(path_to_parent_dir: str, eval_name: str, dataset_name) -&gt; str:
    &#34;&#34;&#34;
    Returns the path to be used by an EvalAlgorithmInterface when calling `save_dataset`.

    :param path_to_parent_dir: The path to the parent directory of the file to be saved.
    :param eval_name: The evaluation name provided by the EvalAlgorithmInterface.
    :param dataset_name: The name of the dataset.
    :returns: A path that is unique to an evaluation/dataset pair for a given job.
    &#34;&#34;&#34;
    return os.path.join(path_to_parent_dir, f&#34;{eval_name}_{dataset_name}.jsonl&#34;)


def generate_mean_delta_score(original_score: EvalScore, perturbed_input_scores: List[EvalScore]) -&gt; float:
    &#34;&#34;&#34;
    Util method to generate mean of difference between original and perturbed input scores
    :param original_score: Original score
    :param perturbed_input_scores: List of scores for model inference outputs on perturbed inputs
    :returns: mean of delta between the scores
    &#34;&#34;&#34;
    return sum([original_score.value - reference_score.value for reference_score in perturbed_input_scores]) / len(
        perturbed_input_scores
    )


def verify_model_determinism(model: ModelRunner, dataset: Dataset, prompt_column_name: str) -&gt; bool:
    &#34;&#34;&#34;
    Check model is not deterministic for first NUM_ROWS_DETERMINISTIC rows
    :param model: An instance of ModelRunner which is the model under evaluation
    :param dataset: a Ray Dataset that expected to include columns for prompts
    :param prompt_column_name: Prompt column name
    :return True if model is deterministic, False otherwise
    &#34;&#34;&#34;
    for row in dataset.limit(NUM_ROWS_DETERMINISTIC).iter_rows():
        original_prompt = row[prompt_column_name]
        original_model_output = model.predict(original_prompt)[0]
        if model.predict(original_prompt)[0] != original_model_output:
            return False
    return True</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.util.aggregate_evaluation_scores"><code class="name flex">
<span>def <span class="ident">aggregate_evaluation_scores</span></span>(<span>dataset: ray.data.dataset.Dataset, score_column_names: List[str], agg_method: str) ‑> Tuple[List[amazon_fmeval.eval_algorithms.EvalScore], Optional[List[amazon_fmeval.eval_algorithms.CategoryScore]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Factual knowledge eval algo aggregation method.</p>
<p>:param dataset: ray dataset with eval scores
:param score_column_names: a list of column names which contain the scores</p>
<p>:return a tuple containing dataset and category level scores</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def aggregate_evaluation_scores(
    dataset: Dataset, score_column_names: List[str], agg_method: str
) -&gt; Tuple[List[EvalScore], Optional[List[CategoryScore]]]:
    &#34;&#34;&#34;
    Factual knowledge eval algo aggregation method.

    :param dataset: ray dataset with eval scores
    :param score_column_names: a list of column names which contain the scores

    :return a tuple containing dataset and category level scores
    &#34;&#34;&#34;
    dataset_scores = [
        EvalScore(name=score_column_name, value=dataset_aggregation(dataset, score_column_name, agg_method))
        for score_column_name in score_column_names
    ]
    category_scores: Optional[Dict[str, CategoryScore]] = None
    if CATEGORY_COLUMN_NAME in dataset.columns():
        category_scores = {name: CategoryScore(name=name, scores=[]) for name in dataset.unique(CATEGORY_COLUMN_NAME)}
        for score_column_name in score_column_names:
            category_aggregate: Dataset = category_wise_aggregation(dataset, score_column_name, agg_method)
            for row in category_aggregate.iter_rows():
                category_scores[row[CATEGORY_COLUMN_NAME]].scores.append(
                    EvalScore(name=score_column_name, value=row[f&#34;mean({score_column_name})&#34;])
                )

    return dataset_scores, list(category_scores.values()) if category_scores else None</code></pre>
</details>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.category_wise_aggregation"><code class="name flex">
<span>def <span class="ident">category_wise_aggregation</span></span>(<span>dataset: ray.data.dataset.Dataset, score_column_name: str, agg_method: str) ‑> ray.data.dataset.Dataset</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def category_wise_aggregation(dataset: Dataset, score_column_name: str, agg_method: str) -&gt; Dataset:
    category_aggregate: Dataset = dataset.groupby(CATEGORY_COLUMN_NAME)  # type: ignore
    if agg_method == MEAN:
        category_aggregate = category_aggregate.mean(score_column_name)
    else:
        raise EvalAlgorithmInternalError(f&#34;Aggregation method {agg_method} is not supported&#34;)
    return category_aggregate</code></pre>
</details>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.dataset_aggregation"><code class="name flex">
<span>def <span class="ident">dataset_aggregation</span></span>(<span>dataset: ray.data.dataset.Dataset, score_column_name: str, agg_method: str) ‑> float</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dataset_aggregation(dataset: Dataset, score_column_name: str, agg_method: str) -&gt; float:
    if agg_method == MEAN:
        aggregate = dataset.mean(score_column_name)
        assert isinstance(aggregate, float)
        return aggregate
    else:
        raise EvalAlgorithmInternalError(f&#34;Aggregation method {agg_method} is not supported&#34;)</code></pre>
</details>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.generate_mean_delta_score"><code class="name flex">
<span>def <span class="ident">generate_mean_delta_score</span></span>(<span>original_score: amazon_fmeval.eval_algorithms.EvalScore, perturbed_input_scores: List[amazon_fmeval.eval_algorithms.EvalScore]) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Util method to generate mean of difference between original and perturbed input scores
:param original_score: Original score
:param perturbed_input_scores: List of scores for model inference outputs on perturbed inputs
:returns: mean of delta between the scores</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_mean_delta_score(original_score: EvalScore, perturbed_input_scores: List[EvalScore]) -&gt; float:
    &#34;&#34;&#34;
    Util method to generate mean of difference between original and perturbed input scores
    :param original_score: Original score
    :param perturbed_input_scores: List of scores for model inference outputs on perturbed inputs
    :returns: mean of delta between the scores
    &#34;&#34;&#34;
    return sum([original_score.value - reference_score.value for reference_score in perturbed_input_scores]) / len(
        perturbed_input_scores
    )</code></pre>
</details>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.generate_model_predict_response_for_dataset"><code class="name flex">
<span>def <span class="ident">generate_model_predict_response_for_dataset</span></span>(<span>model: amazon_fmeval.model_runners.model_runner.ModelRunner, data: ray.data.dataset.Dataset, model_input_column_name: str, model_output_column_name: Optional[str] = None, model_log_probability_column_name: Optional[str] = None) ‑> ray.data.dataset.Dataset</span>
</code></dt>
<dd>
<div class="desc"><p>Runs the model on the given data. Output will be written to the
<code>model_output_column_name</code> column, and log_probability will be
written to the <code>model_log_probability_column_name</code> column.</p>
<p>:param model: ModelRunner to get predictions from.
:param data: The dataset containing model inputs to feed to <code>model</code>.
:param model_input_column_name: The name of the column containing the model input.
:param model_output_column_name: The name of the column to write the model output to.
:param model_log_probability_column_name: The name of the column to write the model log probability to.
:return: The dataset with a model output column and model log probability column added.
Note that both columns are optional, i.e. it is possible that a model output
column is added, but a log probability column is not added (and vice versa).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_model_predict_response_for_dataset(
    model: ModelRunner,
    data: Dataset,
    model_input_column_name: str,
    model_output_column_name: Optional[str] = None,
    model_log_probability_column_name: Optional[str] = None,
) -&gt; Dataset:
    &#34;&#34;&#34;
    Runs the model on the given data. Output will be written to the
    `model_output_column_name` column, and log_probability will be
    written to the `model_log_probability_column_name` column.

    :param model: ModelRunner to get predictions from.
    :param data: The dataset containing model inputs to feed to `model`.
    :param model_input_column_name: The name of the column containing the model input.
    :param model_output_column_name: The name of the column to write the model output to.
    :param model_log_probability_column_name: The name of the column to write the model log probability to.
    :return: The dataset with a model output column and model log probability column added.
        Note that both columns are optional, i.e. it is possible that a model output
        column is added, but a log probability column is not added (and vice versa).
    &#34;&#34;&#34;
    with timed_block(f&#34;Performing inference on dataset on {model}&#34;, logger):

        class ModelRunnerWrapper:  # pragma: no cover
            &#34;&#34;&#34;
            This class represents the Ray Actor that gets model predictions
            by feeding model inputs from the dataset to the model runner.

            We use Ray Actors instead of Tasks because the Actor approach minimizes
            the number of times that the ModelRunner `model` gets deserialized.
            With Tasks, Ray will serialize and deserialize `model` for every single
            prediction. With Actors, `model` gets deserialized once per Actor when
            the Actor gets initialized.
            &#34;&#34;&#34;

            def __init__(self):
                self.model_runner = model
                logger.setLevel(logging.DEBUG)

            def __call__(self, row: Dict[str, Any]) -&gt; Dict[str, Any]:
                predict_output = self.model_runner.predict(row[model_input_column_name])
                if model_output_column_name:
                    row[model_output_column_name] = predict_output[0]
                if model_log_probability_column_name:
                    row[model_log_probability_column_name] = predict_output[1]
                return row

        data = data.map(
            ModelRunnerWrapper, compute=ray.data.ActorPoolStrategy(size=get_num_actors())  # type: ignore[arg-type]
        ).materialize()
    return data</code></pre>
</details>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.generate_output_dataset_path"><code class="name flex">
<span>def <span class="ident">generate_output_dataset_path</span></span>(<span>path_to_parent_dir: str, eval_name: str, dataset_name) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the path to be used by an EvalAlgorithmInterface when calling <code><a title="src.amazon_fmeval.eval_algorithms.util.save_dataset" href="#src.amazon_fmeval.eval_algorithms.util.save_dataset">save_dataset()</a></code>.</p>
<p>:param path_to_parent_dir: The path to the parent directory of the file to be saved.
:param eval_name: The evaluation name provided by the EvalAlgorithmInterface.
:param dataset_name: The name of the dataset.
:returns: A path that is unique to an evaluation/dataset pair for a given job.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_output_dataset_path(path_to_parent_dir: str, eval_name: str, dataset_name) -&gt; str:
    &#34;&#34;&#34;
    Returns the path to be used by an EvalAlgorithmInterface when calling `save_dataset`.

    :param path_to_parent_dir: The path to the parent directory of the file to be saved.
    :param eval_name: The evaluation name provided by the EvalAlgorithmInterface.
    :param dataset_name: The name of the dataset.
    :returns: A path that is unique to an evaluation/dataset pair for a given job.
    &#34;&#34;&#34;
    return os.path.join(path_to_parent_dir, f&#34;{eval_name}_{dataset_name}.jsonl&#34;)</code></pre>
</details>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.generate_prompt_column_for_dataset"><code class="name flex">
<span>def <span class="ident">generate_prompt_column_for_dataset</span></span>(<span>prompt_template: str, data: ray.data.dataset.Dataset, model_input_column_name: str, prompt_column_name: str) ‑> ray.data.dataset.Dataset</span>
</code></dt>
<dd>
<div class="desc"><p>Generates prompts column for a given input dataset and prompt_template
:param prompt_template: Prompt template
:param data: the dataset where each instance is a row in the dataset.
:param model_input_column_name: the name of the column containing the model input.
:param prompt_column_name: Output column name to which composed prompts are added
:return: the dataset with the composed prompts added.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_prompt_column_for_dataset(
    prompt_template: str, data: Dataset, model_input_column_name: str, prompt_column_name: str
) -&gt; Dataset:
    &#34;&#34;&#34;
    Generates prompts column for a given input dataset and prompt_template
    :param prompt_template: Prompt template
    :param data: the dataset where each instance is a row in the dataset.
    :param model_input_column_name: the name of the column containing the model input.
    :param prompt_column_name: Output column name to which composed prompts are added
    :return: the dataset with the composed prompts added.
    &#34;&#34;&#34;
    with timed_block(f&#34;Generating prompt column&#34;, logger):
        prompt_composer = PromptComposer(prompt_template)

        def _generate_prompt_column(df: pd.DataFrame) -&gt; pd.Series:  # pragma: no cover
            &#34;&#34;&#34;
            Map function generating the prompt column values given a batch of records in pandas format.
            &#34;&#34;&#34;
            return pd.Series(
                data=[prompt_composer.compose(row[model_input_column_name]) for index, row in df.iterrows()]
            )

        data = data.add_column(prompt_column_name, _generate_prompt_column).materialize()
    return data</code></pre>
</details>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.get_num_actors"><code class="name flex">
<span>def <span class="ident">get_num_actors</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_num_actors():
    try:
        num_actors = (
            int(os.environ[PARALLELIZATION_FACTOR]) if PARALLELIZATION_FACTOR in os.environ else (mp.cpu_count() - 1)
        )
    except ValueError:
        num_actors = mp.cpu_count() - 1
    return num_actors</code></pre>
</details>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.save_dataset"><code class="name flex">
<span>def <span class="ident">save_dataset</span></span>(<span>dataset: ray.data.dataset.Dataset, score_names: List[str], path: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Writes the dataset to a JSON Lines file, where each JSON Lines object
follows the schema defined by <code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord">EvalOutputRecord</a></code>.</p>
<p>:param dataset: a Ray Dataset that is produced during the execution of
an EvalAlgorithmInterface's <code>evaluate</code> method. This dataset is expected
to include columns for every score computed by the evaluation algorithm.
:param score_names: the names of the score columns in <code>dataset</code>
:param path: a local file path to write the dataset to. The file name specified
by this argument may not end in the extension <code>.jsonl</code>. In this case,
we append the extension ourselves.</p>
<pre><code>Example Dataset:
 ________________________________________
| "model_input" | "rouge" | "bert_score"|
----------------------------------------
|   "hello"    |   0.5   |     0.42    |
---------------------------------------
|   "world"   |  0.314  |    0.271    |
---------------------------------------

Corresponding Json Lines file contents:
{"model_input" : "hello", "scores" : [{"name": "rouge", "value": 0.5}, {"name": "bert_score", "value": 0.42}]}
{"model_input" : "world", "scores" : [{"name": "rouge", "value": 0.314}, {"name": "bert_score", "value": 0.271}]}
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_dataset(dataset: Dataset, score_names: List[str], path: str) -&gt; None:  # pragma: no cover
    &#34;&#34;&#34;
    Writes the dataset to a JSON Lines file, where each JSON Lines object
    follows the schema defined by `EvalOutputRecord`.

    :param dataset: a Ray Dataset that is produced during the execution of
        an EvalAlgorithmInterface&#39;s `evaluate` method. This dataset is expected
        to include columns for every score computed by the evaluation algorithm.
    :param score_names: the names of the score columns in `dataset`
    :param path: a local file path to write the dataset to. The file name specified
        by this argument may not end in the extension `.jsonl`. In this case,
        we append the extension ourselves.


        Example Dataset:
         ________________________________________
        | &#34;model_input&#34; | &#34;rouge&#34; | &#34;bert_score&#34;|
        ----------------------------------------
        |   &#34;hello&#34;    |   0.5   |     0.42    |
        ---------------------------------------
        |   &#34;world&#34;   |  0.314  |    0.271    |
        ---------------------------------------

        Corresponding Json Lines file contents:
        {&#34;model_input&#34; : &#34;hello&#34;, &#34;scores&#34; : [{&#34;name&#34;: &#34;rouge&#34;, &#34;value&#34;: 0.5}, {&#34;name&#34;: &#34;bert_score&#34;, &#34;value&#34;: 0.42}]}
        {&#34;model_input&#34; : &#34;world&#34;, &#34;scores&#34; : [{&#34;name&#34;: &#34;rouge&#34;, &#34;value&#34;: 0.314}, {&#34;name&#34;: &#34;bert_score&#34;, &#34;value&#34;: 0.271}]}
    &#34;&#34;&#34;
    with timed_block(f&#34;Saving dataset to file&#34;, logger):
        # We need the outer dict that wraps the EvalOutputRecord because map() requires
        # whatever is returned from the lambda function to be a dict
        dataset = dataset.map(lambda row: {&#34;record&#34;: EvalOutputRecord.from_row(row, score_names)})
        # Without this line, dataset.iter_rows() below is not guaranteed to return the rows
        # in the same order that they appear in `dataset`.
        dataset.materialize()

        path_to_parent_dir = os.path.dirname(path)
        file_name = os.path.basename(path)
        file_name_without_extension = os.path.splitext(file_name)[0]
        full_path = f&#34;{path_to_parent_dir}/{file_name_without_extension}.jsonl&#34;
        with open(full_path, &#34;w&#34;) as fh:
            records = []
            for dataset_row in dataset.iter_rows():
                record = dataset_row[&#34;record&#34;]
                records.append(str(record))
                if len(records) == EVAL_OUTPUT_RECORDS_BATCH_SIZE:
                    fh.write(&#34;\n&#34;.join(records) + &#34;\n&#34;)
                    records = []
            if records:  # pragma: no branch
                fh.write(&#34;\n&#34;.join(records))  # handle the last batch</code></pre>
</details>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.validate_dataset"><code class="name flex">
<span>def <span class="ident">validate_dataset</span></span>(<span>dataset: ray.data.dataset.Dataset, column_names: List[str])</span>
</code></dt>
<dd>
<div class="desc"><p>Util function to validate that dataset contains the required column names.</p>
<p>:param dataset: Input ray dataset
:param column_names: names of the columns that must be present in the dataset
:raises: EvalAlgorithmClientError for an invalid dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_dataset(dataset: Dataset, column_names: List[str]):
    &#34;&#34;&#34;
    Util function to validate that dataset contains the required column names.

    :param dataset: Input ray dataset
    :param column_names: names of the columns that must be present in the dataset
    :raises: EvalAlgorithmClientError for an invalid dataset
    &#34;&#34;&#34;
    for column_name in column_names:
        util.require(
            column_name in dataset.columns(),
            f&#34;Missing required column: {column_name}, for evaluate() method&#34;,
        )</code></pre>
</details>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.verify_model_determinism"><code class="name flex">
<span>def <span class="ident">verify_model_determinism</span></span>(<span>model: amazon_fmeval.model_runners.model_runner.ModelRunner, dataset: ray.data.dataset.Dataset, prompt_column_name: str) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Check model is not deterministic for first NUM_ROWS_DETERMINISTIC rows
:param model: An instance of ModelRunner which is the model under evaluation
:param dataset: a Ray Dataset that expected to include columns for prompts
:param prompt_column_name: Prompt column name
:return True if model is deterministic, False otherwise</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def verify_model_determinism(model: ModelRunner, dataset: Dataset, prompt_column_name: str) -&gt; bool:
    &#34;&#34;&#34;
    Check model is not deterministic for first NUM_ROWS_DETERMINISTIC rows
    :param model: An instance of ModelRunner which is the model under evaluation
    :param dataset: a Ray Dataset that expected to include columns for prompts
    :param prompt_column_name: Prompt column name
    :return True if model is deterministic, False otherwise
    &#34;&#34;&#34;
    for row in dataset.limit(NUM_ROWS_DETERMINISTIC).iter_rows():
        original_prompt = row[prompt_column_name]
        original_model_output = model.predict(original_prompt)[0]
        if model.predict(original_prompt)[0] != original_model_output:
            return False
    return True</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord"><code class="flex name class">
<span>class <span class="ident">EvalOutputRecord</span></span>
<span>(</span><span>scores: List[amazon_fmeval.eval_algorithms.EvalScore], model_input: Optional[str] = None, model_output: Optional[str] = None, model_log_probability: Optional[float] = None, target_output: Optional[str] = None, category: Optional[str] = None, sent_more_input: Optional[str] = None, sent_less_input: Optional[str] = None, sent_more_input_prob: Optional[str] = None, sent_less_input_prob: Optional[str] = None, sent_more_output: Optional[str] = None, sent_less_output: Optional[str] = None, prompt: Optional[str] = None, sent_more_prompt: Optional[str] = None, sent_less_prompt: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>The schema used to define the records that get written
to a JSON Lines file when <code><a title="src.amazon_fmeval.eval_algorithms.util.save_dataset" href="#src.amazon_fmeval.eval_algorithms.util.save_dataset">save_dataset()</a></code> is called.</p>
<p>:param model_input: the model input
:param model_output: the model output
:param model_log_probability: the model log probability
:param target_output: the target output
:param category: the category
:param sent_more_input: the "sent more" input (used by Prompt stereotyping)
:param sent_less_input: the "sent less" input (used by Prompt stereotyping)
:param sent_more_input_prob: the "sent more" input probability (used by Prompt stereotyping)
:param sent_less_input_prob: the "sent less" input probability (used by Prompt stereotyping)
:param sent_more_output: the "sent more" output (used by Prompt stereotyping)
:param sent_less_output: the "sent less" output (used by Prompt stereotyping)</p>
<h2 id="important">Important</h2>
<p>The attributes of this class MUST match the values of the
column name constants in COLUMN_NAMES in src/constants.py.</p>
<p>Reason:
The <code>from_row</code> method validates the column names included
in its <code>row</code> input, making sure that these column names
match the attribute names of this class (this validation
only occurs for column names that don't correspond to score
names).</p>
<p>Since the <code>row</code> input comes from a Ray Dataset produced by
the <code>evaluate</code> method of an <code>EvalAlgorithmInterface</code>, the column
names in the row must come from COLUMN_NAMES in src/constants.py.</p>
<p>Thus, the attribute names of this class must match the constants
in COLUMN_NAMES in order for the validation to make sense.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class EvalOutputRecord:
    &#34;&#34;&#34;
    The schema used to define the records that get written
    to a JSON Lines file when `save_dataset` is called.

    :param model_input: the model input
    :param model_output: the model output
    :param model_log_probability: the model log probability
    :param target_output: the target output
    :param category: the category
    :param sent_more_input: the &#34;sent more&#34; input (used by Prompt stereotyping)
    :param sent_less_input: the &#34;sent less&#34; input (used by Prompt stereotyping)
    :param sent_more_input_prob: the &#34;sent more&#34; input probability (used by Prompt stereotyping)
    :param sent_less_input_prob: the &#34;sent less&#34; input probability (used by Prompt stereotyping)
    :param sent_more_output: the &#34;sent more&#34; output (used by Prompt stereotyping)
    :param sent_less_output: the &#34;sent less&#34; output (used by Prompt stereotyping)

    IMPORTANT:
        The attributes of this class MUST match the values of the
        column name constants in COLUMN_NAMES in src/constants.py.

        Reason:
        The `from_row` method validates the column names included
        in its `row` input, making sure that these column names
        match the attribute names of this class (this validation
        only occurs for column names that don&#39;t correspond to score
        names).

        Since the `row` input comes from a Ray Dataset produced by
        the `evaluate` method of an `EvalAlgorithmInterface`, the column
        names in the row must come from COLUMN_NAMES in src/constants.py.

        Thus, the attribute names of this class must match the constants
        in COLUMN_NAMES in order for the validation to make sense.
    &#34;&#34;&#34;

    scores: List[EvalScore]
    model_input: Optional[str] = None
    model_output: Optional[str] = None
    model_log_probability: Optional[float] = None
    target_output: Optional[str] = None
    category: Optional[str] = None
    sent_more_input: Optional[str] = None
    sent_less_input: Optional[str] = None
    sent_more_input_prob: Optional[str] = None
    sent_less_input_prob: Optional[str] = None
    sent_more_output: Optional[str] = None
    sent_less_output: Optional[str] = None
    prompt: Optional[str] = None
    sent_more_prompt: Optional[str] = None
    sent_less_prompt: Optional[str] = None

    def __str__(self):
        return json.dumps(self._to_dict())

    def _to_dict(self):
        &#34;&#34;&#34;
        Returns a dictionary representation of this instance,
        to be used when writing this object to JSON Lines.

        Note that attributes with value None are not included
        in the JSON representation. Additionally, we want the
        key &#34;scores&#34; to appear last in the JSON representation,
        but this attribute appears first in EvalOutputRecord&#39;s
        class definition, hence the sorting code below.
        &#34;&#34;&#34;
        attributes = list(self.__dict__.keys())
        attributes.sort(key=lambda x: x == &#34;scores&#34;)
        json_obj = OrderedDict(  # regular Dicts don&#39;t guarantee key order
            (attr, self.__dict__[attr]) for attr in attributes if self.__dict__[attr] is not None
        )
        json_obj[&#34;scores&#34;] = [eval_score.__dict__ for eval_score in json_obj[&#34;scores&#34;]]
        return json_obj

    @staticmethod
    def from_row(row: Dict[str, Union[str, float]], score_names: List[str]) -&gt; &#34;EvalOutputRecord&#34;:
        &#34;&#34;&#34;
        Returns an instance of EvalOutputRecord, created from a Ray Dataset row (represented as a dict).

        Example input:
            row = {
                &#34;model_input&#34;: &#34;input&#34;,
                &#34;model_output&#34;: &#34;output&#34;,
                &#34;rouge&#34;: 0.42,
                &#34;bert&#34;: 0.162
            }

        Corresponding output:
            EvalOutputRecord(
                model_input=&#34;input&#34;,
                model_output=&#34;output&#34;,
                scores=[
                    EvalScore(name=&#34;rouge&#34;, value=0.42),
                    EvalScore(name=&#34;bert&#34;, value=0.162)
                ]
            )

        :param row: a Ray Dataset row represented as a dict
        :param score_names: column names included in the Ray Dataset that `row`
            is a sample of that correspond to evaluation algorithm scores
        :returns: an instance of EvalOutputRecord corresponding to `row`
        &#34;&#34;&#34;
        eval_output_record_attribute_names = set(EvalOutputRecord.__annotations__.keys())
        non_score_columns = {}
        scores = []
        for column_name, value in row.items():
            if column_name not in score_names:  # pragma: no branch
                if column_name in eval_output_record_attribute_names:  # pragma: no branch
                    non_score_columns[column_name] = value
            else:
                assert isinstance(value, float) or isinstance(value, int)  # to satisfy Mypy
                scores.append(EvalScore(name=column_name, value=value))

        return EvalOutputRecord(
            scores=scores,
            **non_score_columns,  # type: ignore
        )</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.category"><code class="name">var <span class="ident">category</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.model_input"><code class="name">var <span class="ident">model_input</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.model_log_probability"><code class="name">var <span class="ident">model_log_probability</span> : Optional[float]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.model_output"><code class="name">var <span class="ident">model_output</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.prompt"><code class="name">var <span class="ident">prompt</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.scores"><code class="name">var <span class="ident">scores</span> : List[amazon_fmeval.eval_algorithms.EvalScore]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_less_input"><code class="name">var <span class="ident">sent_less_input</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_less_input_prob"><code class="name">var <span class="ident">sent_less_input_prob</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_less_output"><code class="name">var <span class="ident">sent_less_output</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_less_prompt"><code class="name">var <span class="ident">sent_less_prompt</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_more_input"><code class="name">var <span class="ident">sent_more_input</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_more_input_prob"><code class="name">var <span class="ident">sent_more_input_prob</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_more_output"><code class="name">var <span class="ident">sent_more_output</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_more_prompt"><code class="name">var <span class="ident">sent_more_prompt</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.target_output"><code class="name">var <span class="ident">target_output</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.from_row"><code class="name flex">
<span>def <span class="ident">from_row</span></span>(<span>row: Dict[str, Union[str, float]], score_names: List[str]) ‑> <a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord">EvalOutputRecord</a></span>
</code></dt>
<dd>
<div class="desc"><p>Returns an instance of EvalOutputRecord, created from a Ray Dataset row (represented as a dict).</p>
<p>Example input:
row = {
"model_input": "input",
"model_output": "output",
"rouge": 0.42,
"bert": 0.162
}</p>
<p>Corresponding output:
EvalOutputRecord(
model_input="input",
model_output="output",
scores=[
EvalScore(name="rouge", value=0.42),
EvalScore(name="bert", value=0.162)
]
)</p>
<p>:param row: a Ray Dataset row represented as a dict
:param score_names: column names included in the Ray Dataset that <code>row</code>
is a sample of that correspond to evaluation algorithm scores
:returns: an instance of EvalOutputRecord corresponding to <code>row</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_row(row: Dict[str, Union[str, float]], score_names: List[str]) -&gt; &#34;EvalOutputRecord&#34;:
    &#34;&#34;&#34;
    Returns an instance of EvalOutputRecord, created from a Ray Dataset row (represented as a dict).

    Example input:
        row = {
            &#34;model_input&#34;: &#34;input&#34;,
            &#34;model_output&#34;: &#34;output&#34;,
            &#34;rouge&#34;: 0.42,
            &#34;bert&#34;: 0.162
        }

    Corresponding output:
        EvalOutputRecord(
            model_input=&#34;input&#34;,
            model_output=&#34;output&#34;,
            scores=[
                EvalScore(name=&#34;rouge&#34;, value=0.42),
                EvalScore(name=&#34;bert&#34;, value=0.162)
            ]
        )

    :param row: a Ray Dataset row represented as a dict
    :param score_names: column names included in the Ray Dataset that `row`
        is a sample of that correspond to evaluation algorithm scores
    :returns: an instance of EvalOutputRecord corresponding to `row`
    &#34;&#34;&#34;
    eval_output_record_attribute_names = set(EvalOutputRecord.__annotations__.keys())
    non_score_columns = {}
    scores = []
    for column_name, value in row.items():
        if column_name not in score_names:  # pragma: no branch
            if column_name in eval_output_record_attribute_names:  # pragma: no branch
                non_score_columns[column_name] = value
        else:
            assert isinstance(value, float) or isinstance(value, int)  # to satisfy Mypy
            scores.append(EvalScore(name=column_name, value=value))

    return EvalOutputRecord(
        scores=scores,
        **non_score_columns,  # type: ignore
    )</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.amazon_fmeval.eval_algorithms" href="index.html">src.amazon_fmeval.eval_algorithms</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.aggregate_evaluation_scores" href="#src.amazon_fmeval.eval_algorithms.util.aggregate_evaluation_scores">aggregate_evaluation_scores</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.category_wise_aggregation" href="#src.amazon_fmeval.eval_algorithms.util.category_wise_aggregation">category_wise_aggregation</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.dataset_aggregation" href="#src.amazon_fmeval.eval_algorithms.util.dataset_aggregation">dataset_aggregation</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.generate_mean_delta_score" href="#src.amazon_fmeval.eval_algorithms.util.generate_mean_delta_score">generate_mean_delta_score</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.generate_model_predict_response_for_dataset" href="#src.amazon_fmeval.eval_algorithms.util.generate_model_predict_response_for_dataset">generate_model_predict_response_for_dataset</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.generate_output_dataset_path" href="#src.amazon_fmeval.eval_algorithms.util.generate_output_dataset_path">generate_output_dataset_path</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.generate_prompt_column_for_dataset" href="#src.amazon_fmeval.eval_algorithms.util.generate_prompt_column_for_dataset">generate_prompt_column_for_dataset</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.get_num_actors" href="#src.amazon_fmeval.eval_algorithms.util.get_num_actors">get_num_actors</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.save_dataset" href="#src.amazon_fmeval.eval_algorithms.util.save_dataset">save_dataset</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.validate_dataset" href="#src.amazon_fmeval.eval_algorithms.util.validate_dataset">validate_dataset</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.verify_model_determinism" href="#src.amazon_fmeval.eval_algorithms.util.verify_model_determinism">verify_model_determinism</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord">EvalOutputRecord</a></code></h4>
<ul class="">
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.category" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.category">category</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.from_row" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.from_row">from_row</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.model_input" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.model_input">model_input</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.model_log_probability" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.model_log_probability">model_log_probability</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.model_output" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.model_output">model_output</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.prompt" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.prompt">prompt</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.scores" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.scores">scores</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_less_input" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_less_input">sent_less_input</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_less_input_prob" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_less_input_prob">sent_less_input_prob</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_less_output" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_less_output">sent_less_output</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_less_prompt" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_less_prompt">sent_less_prompt</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_more_input" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_more_input">sent_more_input</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_more_input_prob" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_more_input_prob">sent_more_input_prob</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_more_output" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_more_output">sent_more_output</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_more_prompt" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.sent_more_prompt">sent_more_prompt</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.target_output" href="#src.amazon_fmeval.eval_algorithms.util.EvalOutputRecord.target_output">target_output</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>