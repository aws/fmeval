<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>src.amazon_fmeval.eval_algorithms.summarization_accuracy API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.amazon_fmeval.eval_algorithms.summarization_accuracy</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import logging
from dataclasses import dataclass
from typing import Optional, List, Callable

import evaluate as hf_evaluate
import nltk
import pandas as pd
from datasets import Dataset
from nltk import word_tokenize
from nltk.translate import meteor_score

import amazon_fmeval.util as util
from amazon_fmeval.constants import (
    TARGET_OUTPUT_COLUMN_NAME,
    MODEL_INPUT_COLUMN_NAME,
    MODEL_OUTPUT_COLUMN_NAME,
    MEAN,
)
from amazon_fmeval.data_loaders.util import DataConfig, get_dataset
from amazon_fmeval.eval_algorithms import (
    EvalAlgorithm,
    EvalScore,
    EvalOutput,
    EVAL_DATASETS,
    DATASET_CONFIGS,
    get_default_prompt_template,
)
from amazon_fmeval.eval_algorithms.eval_algorithm import (
    EvalAlgorithmConfig,
    EvalAlgorithmInterface,
)
from amazon_fmeval.eval_algorithms.helper_models.helper_model import BertscoreHelperModel
from amazon_fmeval.eval_algorithms.util import (
    generate_prompt_column_for_dataset,
    generate_model_predict_response_for_dataset,
    validate_dataset,
    aggregate_evaluation_scores,
    generate_output_dataset_path,
    save_dataset,
)
from amazon_fmeval.exceptions import EvalAlgorithmClientError
from amazon_fmeval.model_runners.model_runner import ModelRunner
from amazon_fmeval.perf_util import timed_block

PROMPT_COLUMN_NAME = &#34;prompt&#34;
METEOR_SCORE = &#34;meteor&#34;
ROUGE_SCORE = &#34;rouge&#34;
BERT_SCORE = &#34;bertscore&#34;

# rouge constants
ROUGE_1 = &#34;rouge1&#34;
ROUGE_2 = &#34;rouge2&#34;
ROUGE_L = &#34;rougeL&#34;

ROUGE_TYPES = [ROUGE_1, ROUGE_2, ROUGE_L]

# bertscore constants
MICROSOFT_DEBERTA_MODEL = &#34;microsoft/deberta-xlarge-mnli&#34;
ROBERTA_MODEL = &#34;roberta-large-mnli&#34;
DEFAULT_MODEL_TYPE = MICROSOFT_DEBERTA_MODEL
MODEL_TYPES_SUPPORTED = [MICROSOFT_DEBERTA_MODEL, ROBERTA_MODEL]

logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class SummarizationAccuracyConfig(EvalAlgorithmConfig):
    &#34;&#34;&#34;
    Configuration for the summarization accuracy eval algorithm

    :param rouge_type: Type of rouge metric in eval results
    :param use_stemmer_for_rouge: bool value to set using stemmer for rouge metric
    :param model_type_for_bertscore: model to use for bert score
    &#34;&#34;&#34;

    rouge_type: str = ROUGE_2
    use_stemmer_for_rouge: bool = True
    model_type_for_bertscore: str = DEFAULT_MODEL_TYPE

    def __post_init__(self):
        if not self.rouge_type in ROUGE_TYPES:
            raise EvalAlgorithmClientError(
                f&#34;Invalid rouge_type: {self.rouge_type} requested in SummarizationAccuracyConfig, &#34;
                f&#34;please choose from acceptable values: {ROUGE_TYPES}&#34;
            )

        if not self.model_type_for_bertscore in MODEL_TYPES_SUPPORTED:
            raise EvalAlgorithmClientError(
                f&#34;Invalid model_type_for_bertscore: {self.model_type_for_bertscore} requested in &#34;
                f&#34;SummarizationAccuracyConfig, please choose from acceptable values: {MODEL_TYPES_SUPPORTED}&#34;
            )


class SummarizationAccuracy(EvalAlgorithmInterface):
    &#34;&#34;&#34;
    Summarization Accuracy Eval algorithm

    The aim of this eval algo is to evaluate how well a model can summarise text.
    The algo uses a reference summary to compare the output generated by the model and a series
    of quality metrics based on overlapping between words (ROUGE and METEOR) and similarity scores (bert scores)
    &#34;&#34;&#34;

    def __init__(self, eval_algorithm_config: SummarizationAccuracyConfig = SummarizationAccuracyConfig()):
        &#34;&#34;&#34;Default constructor

        :param eval_algorithm_config: Summarization Accuracy eval algorithm config.
        &#34;&#34;&#34;
        super().__init__(eval_algorithm_config)
        self.eval_name = EvalAlgorithm.SUMMARIZATION_ACCURACY.value
        self._eval_algorithm_config = eval_algorithm_config
        self._load_eval_helpers()
        self._score_eval_func_mapping = {
            METEOR_SCORE: get_meteor_score,
            ROUGE_SCORE: get_rouge_score,
            BERT_SCORE: get_bert_score,
        }

    def _load_eval_helpers(self):
        &#34;&#34;&#34;
        Method to download required helpers for eval_algo in constructor call
        &#34;&#34;&#34;
        # load helper modules for meteor
        nltk.download(&#34;wordnet&#34;)
        nltk.download(&#34;punkt&#34;)
        nltk.download(&#34;omw-1.4&#34;)

        # load HelperMode for bertscore
        BertscoreHelperModel(model_type=self._eval_algorithm_config.model_type_for_bertscore)

    def evaluate_sample(self, target_output: str, model_output: str) -&gt; List[EvalScore]:  # type: ignore[override]
        &#34;&#34;&#34;
        Summarization Accuracy evaluate sample.

        :param target_output: The expected responses from the model
        :param model_output: The output of a model that we want to evaluate.
        :return: list of EvalScore objects
        &#34;&#34;&#34;
        if target_output is None:
            raise EvalAlgorithmClientError(
                &#34;Missing required input: target_output, for Summarization Accuracy evaluate_sample&#34;
            )
        if model_output is None:
            raise EvalAlgorithmClientError(
                &#34;Missing required input: model_output, for Summarization Accuracy evaluate_sample&#34;
            )

        return [
            EvalScore(
                name=eval_score,
                value=eval_fn(
                    target_output=target_output, model_output=model_output, config=self._eval_algorithm_config
                ),
            )
            for eval_score, eval_fn in self._score_eval_func_mapping.items()
        ]

    def evaluate(
        self,
        model: Optional[ModelRunner] = None,
        dataset_config: Optional[DataConfig] = None,
        prompt_template: Optional[str] = None,
        save: bool = False,
        num_records=100,
    ) -&gt; List[EvalOutput]:
        &#34;&#34;&#34;
        Summarization Accuracy evaluate

        :param model: An instance of ModelRunner which is the model under evaluation
        :param dataset_config: Configures the single dataset used for evaluation. If not provided,
            evaluation will use all of it&#39;s supported built-in datasets
        :param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults
            will be used.
        :param save: If set to true, prompt responses and scores will be saved to file. The output is written to
                     EvalAlgorithmInterface.EVAL_RESULTS_PATH
        :param num_records: The number of records to be sampled randomly from the input dataset to perform the
                            evaluation

        :return: List of EvalOutput objects.
        &#34;&#34;&#34;
        if dataset_config:
            dataset_configs = [dataset_config]
        else:
            dataset_configs = [DATASET_CONFIGS[dataset_name] for dataset_name in EVAL_DATASETS[self.eval_name]]

        eval_outputs = []
        for dataset_config in dataset_configs:
            dataset = get_dataset(dataset_config, num_records)
            validate_dataset(dataset, [TARGET_OUTPUT_COLUMN_NAME, MODEL_INPUT_COLUMN_NAME])
            dataset_prompt_template = None
            if MODEL_OUTPUT_COLUMN_NAME not in dataset.columns():
                util.require(model, &#34;No ModelRunner provided. ModelRunner is required for inference on model_inputs&#34;)
                dataset_prompt_template = (
                    get_default_prompt_template(dataset_config.dataset_name) if not prompt_template else prompt_template
                )
                dataset = generate_prompt_column_for_dataset(
                    dataset_prompt_template, dataset, MODEL_INPUT_COLUMN_NAME, PROMPT_COLUMN_NAME
                )
                assert model  # to satisfy mypy
                dataset = generate_model_predict_response_for_dataset(
                    model, dataset, PROMPT_COLUMN_NAME, MODEL_OUTPUT_COLUMN_NAME
                )
            with timed_block(f&#34;Computing score and aggregation on dataset {dataset_config.dataset_name}&#34;, logger):
                for eval_score, eval_func in self._score_eval_func_mapping.items():
                    dataset = add_score_to_dataset(
                        dataset=dataset,
                        eval_func=eval_func,
                        score_column_name=eval_score,
                        config=self._eval_algorithm_config,
                    )

                dataset_scores, category_scores = aggregate_evaluation_scores(
                    dataset, [METEOR_SCORE, ROUGE_SCORE, BERT_SCORE], agg_method=MEAN
                )
                eval_outputs.append(
                    EvalOutput(
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                        prompt_template=dataset_prompt_template,
                        dataset_scores=dataset_scores,
                        category_scores=category_scores,
                        output_path=generate_output_dataset_path(
                            path_to_parent_dir=self._eval_results_path,
                            eval_name=self.eval_name,
                            dataset_name=dataset_config.dataset_name,
                        ),
                    )
                )
            if save:
                save_dataset(
                    dataset=dataset,
                    score_names=[METEOR_SCORE, ROUGE_SCORE, BERT_SCORE],
                    path=generate_output_dataset_path(
                        path_to_parent_dir=self._eval_results_path,
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                    ),
                )

        return eval_outputs


def get_meteor_score(target_output: str, model_output: str, config: SummarizationAccuracyConfig) -&gt; float:
    &#34;&#34;&#34;
    METEOR, an automatic metric for machine translation evaluation
    that is based on a generalized concept of unigram matching between the
    machine-produced translation and human-produced reference translations.
    Unigrams can be matched based on their surface forms, stemmed forms,
    and meanings; furthermore, METEOR can be easily extended to include more
    advanced matching strategies. Once all generalized unigram matches
    between the two strings have been found, METEOR computes a score for
    this matching using a combination of unigram-precision, unigram-recall, and
    a measure of fragmentation that is designed to directly capture how
    well-ordered the matched words in the machine translation are in relation
    to the reference.

    METEOR gets an R correlation value of 0.347 with human evaluation on the Arabic
    data and 0.331 on the Chinese data. This is shown to be an improvement on
    using simply unigram-precision, unigram-recall and their harmonic F1
    combination.

    :param target_output: The expected responses from the model
    :param model_output: The output of a model that we want to evaluate.
    :returns: meteor score
    &#34;&#34;&#34;
    return meteor_score.single_meteor_score(
        reference=word_tokenize(target_output), hypothesis=word_tokenize(model_output)
    )


def get_rouge_score(target_output: str, model_output: str, config: SummarizationAccuracyConfig) -&gt; float:
    &#34;&#34;&#34;
    The ROUGE-N, where N=[1,2,L], score is a standard metric for summarization quality.
    It computes the word overlap between the reference and model summary. Given that this metric is based on simple
    word overlap statistics, it works best for extractive summaries.
    Note that if we rephrase the summary without changing its meaning the ROUGE-N score will drop.

    Reference: https://huggingface.co/spaces/evaluate-metric/rouge

    :param target_output: The expected responses from the model
    :param model_output: The output of a model that we want to evaluate.
    :param config: Eval algo config
    :returns: rouge score: boolean indicating using stemmer for rouge
    &#34;&#34;&#34;
    rouge = hf_evaluate.load(&#34;rouge&#34;)
    return rouge.compute(
        predictions=[model_output],
        references=[target_output],
        use_stemmer=config.use_stemmer_for_rouge,
        rouge_types=[config.rouge_type],
    )[config.rouge_type]


def get_bert_score(target_output: str, model_output: str, config: SummarizationAccuracyConfig) -&gt; float:
    &#34;&#34;&#34;
    BERTscore is a similarity-based metric that compares the embedding of the prediction and target sentences
    under a (learned) model, typically, from the BERT family.
    This score may lead to increased flexibility compared to rouge and METEOR in terms of rephrasing since
    semantically similar sentences are (typically) embedded similarly.

    https://huggingface.co/spaces/evaluate-metric/bertscore

    :param target_output: The expected responses from the model
    :param model_output: The output of a model that we want to evaluate.
    :param config: Eval algo config
    :returns: rouge score: boolean indicating using stemmer for rouge
    &#34;&#34;&#34;
    bertscore = BertscoreHelperModel(model_type=config.model_type_for_bertscore)
    return bertscore.get_helper_scores(target_output, model_output)


def add_score_to_dataset(
    dataset: Dataset, eval_func: Callable, score_column_name: str, config: SummarizationAccuracyConfig
):
    &#34;&#34;&#34;
    Util method to add a score column to a ray dataset.

    :param dataset: ray Dataset to be used for eval score generation
    :param eval_func: eval function callable method
    :param score_column_name: column name for score to be added
    :param config: Eval algo config
    :returns: ray Dataset with score column
    &#34;&#34;&#34;

    def _generate_eval_scores(df: pd.DataFrame) -&gt; pd.Series:  # pragma: no cover
        &#34;&#34;&#34;
        Map function generating the scores for every input record in input dataset
        &#34;&#34;&#34;
        return pd.Series(
            data=[
                eval_func(row[TARGET_OUTPUT_COLUMN_NAME], row[MODEL_OUTPUT_COLUMN_NAME], config)
                for index, row in df.iterrows()
            ]
        )

    return dataset.add_column(score_column_name, _generate_eval_scores).materialize()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.summarization_accuracy.add_score_to_dataset"><code class="name flex">
<span>def <span class="ident">add_score_to_dataset</span></span>(<span>dataset: datasets.arrow_dataset.Dataset, eval_func: Callable, score_column_name: str, config: <a title="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig" href="#src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig">SummarizationAccuracyConfig</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Util method to add a score column to a ray dataset.</p>
<p>:param dataset: ray Dataset to be used for eval score generation
:param eval_func: eval function callable method
:param score_column_name: column name for score to be added
:param config: Eval algo config
:returns: ray Dataset with score column</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_score_to_dataset(
    dataset: Dataset, eval_func: Callable, score_column_name: str, config: SummarizationAccuracyConfig
):
    &#34;&#34;&#34;
    Util method to add a score column to a ray dataset.

    :param dataset: ray Dataset to be used for eval score generation
    :param eval_func: eval function callable method
    :param score_column_name: column name for score to be added
    :param config: Eval algo config
    :returns: ray Dataset with score column
    &#34;&#34;&#34;

    def _generate_eval_scores(df: pd.DataFrame) -&gt; pd.Series:  # pragma: no cover
        &#34;&#34;&#34;
        Map function generating the scores for every input record in input dataset
        &#34;&#34;&#34;
        return pd.Series(
            data=[
                eval_func(row[TARGET_OUTPUT_COLUMN_NAME], row[MODEL_OUTPUT_COLUMN_NAME], config)
                for index, row in df.iterrows()
            ]
        )

    return dataset.add_column(score_column_name, _generate_eval_scores).materialize()</code></pre>
</details>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.summarization_accuracy.get_bert_score"><code class="name flex">
<span>def <span class="ident">get_bert_score</span></span>(<span>target_output: str, model_output: str, config: <a title="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig" href="#src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig">SummarizationAccuracyConfig</a>) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>BERTscore is a similarity-based metric that compares the embedding of the prediction and target sentences
under a (learned) model, typically, from the BERT family.
This score may lead to increased flexibility compared to rouge and METEOR in terms of rephrasing since
semantically similar sentences are (typically) embedded similarly.</p>
<p><a href="https://huggingface.co/spaces/evaluate-metric/bertscore">https://huggingface.co/spaces/evaluate-metric/bertscore</a></p>
<p>:param target_output: The expected responses from the model
:param model_output: The output of a model that we want to evaluate.
:param config: Eval algo config
:returns: rouge score: boolean indicating using stemmer for rouge</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_bert_score(target_output: str, model_output: str, config: SummarizationAccuracyConfig) -&gt; float:
    &#34;&#34;&#34;
    BERTscore is a similarity-based metric that compares the embedding of the prediction and target sentences
    under a (learned) model, typically, from the BERT family.
    This score may lead to increased flexibility compared to rouge and METEOR in terms of rephrasing since
    semantically similar sentences are (typically) embedded similarly.

    https://huggingface.co/spaces/evaluate-metric/bertscore

    :param target_output: The expected responses from the model
    :param model_output: The output of a model that we want to evaluate.
    :param config: Eval algo config
    :returns: rouge score: boolean indicating using stemmer for rouge
    &#34;&#34;&#34;
    bertscore = BertscoreHelperModel(model_type=config.model_type_for_bertscore)
    return bertscore.get_helper_scores(target_output, model_output)</code></pre>
</details>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.summarization_accuracy.get_meteor_score"><code class="name flex">
<span>def <span class="ident">get_meteor_score</span></span>(<span>target_output: str, model_output: str, config: <a title="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig" href="#src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig">SummarizationAccuracyConfig</a>) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>METEOR, an automatic metric for machine translation evaluation
that is based on a generalized concept of unigram matching between the
machine-produced translation and human-produced reference translations.
Unigrams can be matched based on their surface forms, stemmed forms,
and meanings; furthermore, METEOR can be easily extended to include more
advanced matching strategies. Once all generalized unigram matches
between the two strings have been found, METEOR computes a score for
this matching using a combination of unigram-precision, unigram-recall, and
a measure of fragmentation that is designed to directly capture how
well-ordered the matched words in the machine translation are in relation
to the reference.</p>
<p>METEOR gets an R correlation value of 0.347 with human evaluation on the Arabic
data and 0.331 on the Chinese data. This is shown to be an improvement on
using simply unigram-precision, unigram-recall and their harmonic F1
combination.</p>
<p>:param target_output: The expected responses from the model
:param model_output: The output of a model that we want to evaluate.
:returns: meteor score</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_meteor_score(target_output: str, model_output: str, config: SummarizationAccuracyConfig) -&gt; float:
    &#34;&#34;&#34;
    METEOR, an automatic metric for machine translation evaluation
    that is based on a generalized concept of unigram matching between the
    machine-produced translation and human-produced reference translations.
    Unigrams can be matched based on their surface forms, stemmed forms,
    and meanings; furthermore, METEOR can be easily extended to include more
    advanced matching strategies. Once all generalized unigram matches
    between the two strings have been found, METEOR computes a score for
    this matching using a combination of unigram-precision, unigram-recall, and
    a measure of fragmentation that is designed to directly capture how
    well-ordered the matched words in the machine translation are in relation
    to the reference.

    METEOR gets an R correlation value of 0.347 with human evaluation on the Arabic
    data and 0.331 on the Chinese data. This is shown to be an improvement on
    using simply unigram-precision, unigram-recall and their harmonic F1
    combination.

    :param target_output: The expected responses from the model
    :param model_output: The output of a model that we want to evaluate.
    :returns: meteor score
    &#34;&#34;&#34;
    return meteor_score.single_meteor_score(
        reference=word_tokenize(target_output), hypothesis=word_tokenize(model_output)
    )</code></pre>
</details>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.summarization_accuracy.get_rouge_score"><code class="name flex">
<span>def <span class="ident">get_rouge_score</span></span>(<span>target_output: str, model_output: str, config: <a title="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig" href="#src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig">SummarizationAccuracyConfig</a>) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>The ROUGE-N, where N=[1,2,L], score is a standard metric for summarization quality.
It computes the word overlap between the reference and model summary. Given that this metric is based on simple
word overlap statistics, it works best for extractive summaries.
Note that if we rephrase the summary without changing its meaning the ROUGE-N score will drop.</p>
<p>Reference: <a href="https://huggingface.co/spaces/evaluate-metric/rouge">https://huggingface.co/spaces/evaluate-metric/rouge</a></p>
<p>:param target_output: The expected responses from the model
:param model_output: The output of a model that we want to evaluate.
:param config: Eval algo config
:returns: rouge score: boolean indicating using stemmer for rouge</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_rouge_score(target_output: str, model_output: str, config: SummarizationAccuracyConfig) -&gt; float:
    &#34;&#34;&#34;
    The ROUGE-N, where N=[1,2,L], score is a standard metric for summarization quality.
    It computes the word overlap between the reference and model summary. Given that this metric is based on simple
    word overlap statistics, it works best for extractive summaries.
    Note that if we rephrase the summary without changing its meaning the ROUGE-N score will drop.

    Reference: https://huggingface.co/spaces/evaluate-metric/rouge

    :param target_output: The expected responses from the model
    :param model_output: The output of a model that we want to evaluate.
    :param config: Eval algo config
    :returns: rouge score: boolean indicating using stemmer for rouge
    &#34;&#34;&#34;
    rouge = hf_evaluate.load(&#34;rouge&#34;)
    return rouge.compute(
        predictions=[model_output],
        references=[target_output],
        use_stemmer=config.use_stemmer_for_rouge,
        rouge_types=[config.rouge_type],
    )[config.rouge_type]</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracy"><code class="flex name class">
<span>class <span class="ident">SummarizationAccuracy</span></span>
<span>(</span><span>eval_algorithm_config: <a title="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig" href="#src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig">SummarizationAccuracyConfig</a> = SummarizationAccuracyConfig(rouge_type='rouge2', use_stemmer_for_rouge=True, model_type_for_bertscore='microsoft/deberta-xlarge-mnli'))</span>
</code></dt>
<dd>
<div class="desc"><p>Summarization Accuracy Eval algorithm</p>
<p>The aim of this eval algo is to evaluate how well a model can summarise text.
The algo uses a reference summary to compare the output generated by the model and a series
of quality metrics based on overlapping between words (ROUGE and METEOR) and similarity scores (bert scores)</p>
<p>Default constructor</p>
<p>:param eval_algorithm_config: Summarization Accuracy eval algorithm config.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SummarizationAccuracy(EvalAlgorithmInterface):
    &#34;&#34;&#34;
    Summarization Accuracy Eval algorithm

    The aim of this eval algo is to evaluate how well a model can summarise text.
    The algo uses a reference summary to compare the output generated by the model and a series
    of quality metrics based on overlapping between words (ROUGE and METEOR) and similarity scores (bert scores)
    &#34;&#34;&#34;

    def __init__(self, eval_algorithm_config: SummarizationAccuracyConfig = SummarizationAccuracyConfig()):
        &#34;&#34;&#34;Default constructor

        :param eval_algorithm_config: Summarization Accuracy eval algorithm config.
        &#34;&#34;&#34;
        super().__init__(eval_algorithm_config)
        self.eval_name = EvalAlgorithm.SUMMARIZATION_ACCURACY.value
        self._eval_algorithm_config = eval_algorithm_config
        self._load_eval_helpers()
        self._score_eval_func_mapping = {
            METEOR_SCORE: get_meteor_score,
            ROUGE_SCORE: get_rouge_score,
            BERT_SCORE: get_bert_score,
        }

    def _load_eval_helpers(self):
        &#34;&#34;&#34;
        Method to download required helpers for eval_algo in constructor call
        &#34;&#34;&#34;
        # load helper modules for meteor
        nltk.download(&#34;wordnet&#34;)
        nltk.download(&#34;punkt&#34;)
        nltk.download(&#34;omw-1.4&#34;)

        # load HelperMode for bertscore
        BertscoreHelperModel(model_type=self._eval_algorithm_config.model_type_for_bertscore)

    def evaluate_sample(self, target_output: str, model_output: str) -&gt; List[EvalScore]:  # type: ignore[override]
        &#34;&#34;&#34;
        Summarization Accuracy evaluate sample.

        :param target_output: The expected responses from the model
        :param model_output: The output of a model that we want to evaluate.
        :return: list of EvalScore objects
        &#34;&#34;&#34;
        if target_output is None:
            raise EvalAlgorithmClientError(
                &#34;Missing required input: target_output, for Summarization Accuracy evaluate_sample&#34;
            )
        if model_output is None:
            raise EvalAlgorithmClientError(
                &#34;Missing required input: model_output, for Summarization Accuracy evaluate_sample&#34;
            )

        return [
            EvalScore(
                name=eval_score,
                value=eval_fn(
                    target_output=target_output, model_output=model_output, config=self._eval_algorithm_config
                ),
            )
            for eval_score, eval_fn in self._score_eval_func_mapping.items()
        ]

    def evaluate(
        self,
        model: Optional[ModelRunner] = None,
        dataset_config: Optional[DataConfig] = None,
        prompt_template: Optional[str] = None,
        save: bool = False,
        num_records=100,
    ) -&gt; List[EvalOutput]:
        &#34;&#34;&#34;
        Summarization Accuracy evaluate

        :param model: An instance of ModelRunner which is the model under evaluation
        :param dataset_config: Configures the single dataset used for evaluation. If not provided,
            evaluation will use all of it&#39;s supported built-in datasets
        :param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults
            will be used.
        :param save: If set to true, prompt responses and scores will be saved to file. The output is written to
                     EvalAlgorithmInterface.EVAL_RESULTS_PATH
        :param num_records: The number of records to be sampled randomly from the input dataset to perform the
                            evaluation

        :return: List of EvalOutput objects.
        &#34;&#34;&#34;
        if dataset_config:
            dataset_configs = [dataset_config]
        else:
            dataset_configs = [DATASET_CONFIGS[dataset_name] for dataset_name in EVAL_DATASETS[self.eval_name]]

        eval_outputs = []
        for dataset_config in dataset_configs:
            dataset = get_dataset(dataset_config, num_records)
            validate_dataset(dataset, [TARGET_OUTPUT_COLUMN_NAME, MODEL_INPUT_COLUMN_NAME])
            dataset_prompt_template = None
            if MODEL_OUTPUT_COLUMN_NAME not in dataset.columns():
                util.require(model, &#34;No ModelRunner provided. ModelRunner is required for inference on model_inputs&#34;)
                dataset_prompt_template = (
                    get_default_prompt_template(dataset_config.dataset_name) if not prompt_template else prompt_template
                )
                dataset = generate_prompt_column_for_dataset(
                    dataset_prompt_template, dataset, MODEL_INPUT_COLUMN_NAME, PROMPT_COLUMN_NAME
                )
                assert model  # to satisfy mypy
                dataset = generate_model_predict_response_for_dataset(
                    model, dataset, PROMPT_COLUMN_NAME, MODEL_OUTPUT_COLUMN_NAME
                )
            with timed_block(f&#34;Computing score and aggregation on dataset {dataset_config.dataset_name}&#34;, logger):
                for eval_score, eval_func in self._score_eval_func_mapping.items():
                    dataset = add_score_to_dataset(
                        dataset=dataset,
                        eval_func=eval_func,
                        score_column_name=eval_score,
                        config=self._eval_algorithm_config,
                    )

                dataset_scores, category_scores = aggregate_evaluation_scores(
                    dataset, [METEOR_SCORE, ROUGE_SCORE, BERT_SCORE], agg_method=MEAN
                )
                eval_outputs.append(
                    EvalOutput(
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                        prompt_template=dataset_prompt_template,
                        dataset_scores=dataset_scores,
                        category_scores=category_scores,
                        output_path=generate_output_dataset_path(
                            path_to_parent_dir=self._eval_results_path,
                            eval_name=self.eval_name,
                            dataset_name=dataset_config.dataset_name,
                        ),
                    )
                )
            if save:
                save_dataset(
                    dataset=dataset,
                    score_names=[METEOR_SCORE, ROUGE_SCORE, BERT_SCORE],
                    path=generate_output_dataset_path(
                        path_to_parent_dir=self._eval_results_path,
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                    ),
                )

        return eval_outputs</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>amazon_fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracy.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, model: Optional[amazon_fmeval.model_runners.model_runner.ModelRunner] = None, dataset_config: Optional[amazon_fmeval.data_loaders.data_config.DataConfig] = None, prompt_template: Optional[str] = None, save: bool = False, num_records=100) ‑> List[amazon_fmeval.eval_algorithms.EvalOutput]</span>
</code></dt>
<dd>
<div class="desc"><p>Summarization Accuracy evaluate</p>
<p>:param model: An instance of ModelRunner which is the model under evaluation
:param dataset_config: Configures the single dataset used for evaluation. If not provided,
evaluation will use all of it's supported built-in datasets
:param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults
will be used.
:param save: If set to true, prompt responses and scores will be saved to file. The output is written to
EvalAlgorithmInterface.EVAL_RESULTS_PATH
:param num_records: The number of records to be sampled randomly from the input dataset to perform the
evaluation</p>
<p>:return: List of EvalOutput objects.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(
    self,
    model: Optional[ModelRunner] = None,
    dataset_config: Optional[DataConfig] = None,
    prompt_template: Optional[str] = None,
    save: bool = False,
    num_records=100,
) -&gt; List[EvalOutput]:
    &#34;&#34;&#34;
    Summarization Accuracy evaluate

    :param model: An instance of ModelRunner which is the model under evaluation
    :param dataset_config: Configures the single dataset used for evaluation. If not provided,
        evaluation will use all of it&#39;s supported built-in datasets
    :param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults
        will be used.
    :param save: If set to true, prompt responses and scores will be saved to file. The output is written to
                 EvalAlgorithmInterface.EVAL_RESULTS_PATH
    :param num_records: The number of records to be sampled randomly from the input dataset to perform the
                        evaluation

    :return: List of EvalOutput objects.
    &#34;&#34;&#34;
    if dataset_config:
        dataset_configs = [dataset_config]
    else:
        dataset_configs = [DATASET_CONFIGS[dataset_name] for dataset_name in EVAL_DATASETS[self.eval_name]]

    eval_outputs = []
    for dataset_config in dataset_configs:
        dataset = get_dataset(dataset_config, num_records)
        validate_dataset(dataset, [TARGET_OUTPUT_COLUMN_NAME, MODEL_INPUT_COLUMN_NAME])
        dataset_prompt_template = None
        if MODEL_OUTPUT_COLUMN_NAME not in dataset.columns():
            util.require(model, &#34;No ModelRunner provided. ModelRunner is required for inference on model_inputs&#34;)
            dataset_prompt_template = (
                get_default_prompt_template(dataset_config.dataset_name) if not prompt_template else prompt_template
            )
            dataset = generate_prompt_column_for_dataset(
                dataset_prompt_template, dataset, MODEL_INPUT_COLUMN_NAME, PROMPT_COLUMN_NAME
            )
            assert model  # to satisfy mypy
            dataset = generate_model_predict_response_for_dataset(
                model, dataset, PROMPT_COLUMN_NAME, MODEL_OUTPUT_COLUMN_NAME
            )
        with timed_block(f&#34;Computing score and aggregation on dataset {dataset_config.dataset_name}&#34;, logger):
            for eval_score, eval_func in self._score_eval_func_mapping.items():
                dataset = add_score_to_dataset(
                    dataset=dataset,
                    eval_func=eval_func,
                    score_column_name=eval_score,
                    config=self._eval_algorithm_config,
                )

            dataset_scores, category_scores = aggregate_evaluation_scores(
                dataset, [METEOR_SCORE, ROUGE_SCORE, BERT_SCORE], agg_method=MEAN
            )
            eval_outputs.append(
                EvalOutput(
                    eval_name=self.eval_name,
                    dataset_name=dataset_config.dataset_name,
                    prompt_template=dataset_prompt_template,
                    dataset_scores=dataset_scores,
                    category_scores=category_scores,
                    output_path=generate_output_dataset_path(
                        path_to_parent_dir=self._eval_results_path,
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                    ),
                )
            )
        if save:
            save_dataset(
                dataset=dataset,
                score_names=[METEOR_SCORE, ROUGE_SCORE, BERT_SCORE],
                path=generate_output_dataset_path(
                    path_to_parent_dir=self._eval_results_path,
                    eval_name=self.eval_name,
                    dataset_name=dataset_config.dataset_name,
                ),
            )

    return eval_outputs</code></pre>
</details>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracy.evaluate_sample"><code class="name flex">
<span>def <span class="ident">evaluate_sample</span></span>(<span>self, target_output: str, model_output: str) ‑> List[amazon_fmeval.eval_algorithms.EvalScore]</span>
</code></dt>
<dd>
<div class="desc"><p>Summarization Accuracy evaluate sample.</p>
<p>:param target_output: The expected responses from the model
:param model_output: The output of a model that we want to evaluate.
:return: list of EvalScore objects</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_sample(self, target_output: str, model_output: str) -&gt; List[EvalScore]:  # type: ignore[override]
    &#34;&#34;&#34;
    Summarization Accuracy evaluate sample.

    :param target_output: The expected responses from the model
    :param model_output: The output of a model that we want to evaluate.
    :return: list of EvalScore objects
    &#34;&#34;&#34;
    if target_output is None:
        raise EvalAlgorithmClientError(
            &#34;Missing required input: target_output, for Summarization Accuracy evaluate_sample&#34;
        )
    if model_output is None:
        raise EvalAlgorithmClientError(
            &#34;Missing required input: model_output, for Summarization Accuracy evaluate_sample&#34;
        )

    return [
        EvalScore(
            name=eval_score,
            value=eval_fn(
                target_output=target_output, model_output=model_output, config=self._eval_algorithm_config
            ),
        )
        for eval_score, eval_fn in self._score_eval_func_mapping.items()
    ]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig"><code class="flex name class">
<span>class <span class="ident">SummarizationAccuracyConfig</span></span>
<span>(</span><span>rouge_type: str = 'rouge2', use_stemmer_for_rouge: bool = True, model_type_for_bertscore: str = 'microsoft/deberta-xlarge-mnli')</span>
</code></dt>
<dd>
<div class="desc"><p>Configuration for the summarization accuracy eval algorithm</p>
<p>:param rouge_type: Type of rouge metric in eval results
:param use_stemmer_for_rouge: bool value to set using stemmer for rouge metric
:param model_type_for_bertscore: model to use for bert score</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass(frozen=True)
class SummarizationAccuracyConfig(EvalAlgorithmConfig):
    &#34;&#34;&#34;
    Configuration for the summarization accuracy eval algorithm

    :param rouge_type: Type of rouge metric in eval results
    :param use_stemmer_for_rouge: bool value to set using stemmer for rouge metric
    :param model_type_for_bertscore: model to use for bert score
    &#34;&#34;&#34;

    rouge_type: str = ROUGE_2
    use_stemmer_for_rouge: bool = True
    model_type_for_bertscore: str = DEFAULT_MODEL_TYPE

    def __post_init__(self):
        if not self.rouge_type in ROUGE_TYPES:
            raise EvalAlgorithmClientError(
                f&#34;Invalid rouge_type: {self.rouge_type} requested in SummarizationAccuracyConfig, &#34;
                f&#34;please choose from acceptable values: {ROUGE_TYPES}&#34;
            )

        if not self.model_type_for_bertscore in MODEL_TYPES_SUPPORTED:
            raise EvalAlgorithmClientError(
                f&#34;Invalid model_type_for_bertscore: {self.model_type_for_bertscore} requested in &#34;
                f&#34;SummarizationAccuracyConfig, please choose from acceptable values: {MODEL_TYPES_SUPPORTED}&#34;
            )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>amazon_fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmConfig</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig.model_type_for_bertscore"><code class="name">var <span class="ident">model_type_for_bertscore</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig.rouge_type"><code class="name">var <span class="ident">rouge_type</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig.use_stemmer_for_rouge"><code class="name">var <span class="ident">use_stemmer_for_rouge</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.amazon_fmeval.eval_algorithms" href="index.html">src.amazon_fmeval.eval_algorithms</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.amazon_fmeval.eval_algorithms.summarization_accuracy.add_score_to_dataset" href="#src.amazon_fmeval.eval_algorithms.summarization_accuracy.add_score_to_dataset">add_score_to_dataset</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.summarization_accuracy.get_bert_score" href="#src.amazon_fmeval.eval_algorithms.summarization_accuracy.get_bert_score">get_bert_score</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.summarization_accuracy.get_meteor_score" href="#src.amazon_fmeval.eval_algorithms.summarization_accuracy.get_meteor_score">get_meteor_score</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.summarization_accuracy.get_rouge_score" href="#src.amazon_fmeval.eval_algorithms.summarization_accuracy.get_rouge_score">get_rouge_score</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracy" href="#src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracy">SummarizationAccuracy</a></code></h4>
<ul class="">
<li><code><a title="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracy.evaluate" href="#src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracy.evaluate">evaluate</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracy.evaluate_sample" href="#src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracy.evaluate_sample">evaluate_sample</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig" href="#src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig">SummarizationAccuracyConfig</a></code></h4>
<ul class="">
<li><code><a title="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig.model_type_for_bertscore" href="#src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig.model_type_for_bertscore">model_type_for_bertscore</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig.rouge_type" href="#src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig.rouge_type">rouge_type</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig.use_stemmer_for_rouge" href="#src.amazon_fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig.use_stemmer_for_rouge">use_stemmer_for_rouge</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>