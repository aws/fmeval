<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>src.amazon_fmeval.eval_algorithms.qa_accuracy API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.amazon_fmeval.eval_algorithms.qa_accuracy</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import logging
import string
from functools import partial

import pandas as pd

from typing import Any, Callable, List, Optional, Dict

from dataclasses import dataclass

from nltk.metrics.scores import f_measure

import amazon_fmeval.util as util
from amazon_fmeval.constants import (
    MODEL_INPUT_COLUMN_NAME,
    MODEL_OUTPUT_COLUMN_NAME,
    TARGET_OUTPUT_COLUMN_NAME,
    MODEL_LOG_PROBABILITY_COLUMN_NAME,
    MEAN,
)
from amazon_fmeval.data_loaders.util import get_dataset
from amazon_fmeval.data_loaders.data_config import DataConfig
from amazon_fmeval.eval_algorithms.util import (
    generate_model_predict_response_for_dataset,
    generate_prompt_column_for_dataset,
    aggregate_evaluation_scores,
    validate_dataset,
    save_dataset,
    generate_output_dataset_path,
)
from amazon_fmeval.eval_algorithms.eval_algorithm import (
    EvalAlgorithmInterface,
    EvalAlgorithmConfig,
)
from amazon_fmeval.eval_algorithms import (
    EvalAlgorithm,
    EvalOutput,
    EvalScore,
    EVAL_DATASETS,
    DATASET_CONFIGS,
    get_default_prompt_template,
)
from amazon_fmeval.exceptions import EvalAlgorithmClientError
from amazon_fmeval.model_runners.model_runner import ModelRunner
from amazon_fmeval.perf_util import timed_block

ENGLISH_ARTICLES = [&#34;a&#34;, &#34;an&#34;, &#34;the&#34;]
ENGLISH_PUNCTUATIONS = string.punctuation

F1_SCORE = &#34;f1_score&#34;
EXACT_MATCH_SCORE = &#34;exact_match_score&#34;
QUASI_EXACT_MATCH_SCORE = &#34;quasi_exact_match_score&#34;

PROMPT_COLUMN_NAME = &#34;prompt&#34;
logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class QAAccuracyConfig(EvalAlgorithmConfig):
    &#34;&#34;&#34;
    Configuration for the QA Accuracy Evaluation

    :param target_output_delimiter: Target Output can have multiple answers. We expect customer to combine all the
        possible answers into a single string and use the delimiter to separate them. For instance,
        if the answers are [&#34;UK&#34;, &#34;England&#34;] and the delimiter=&#34;&lt;OR&gt;&#34;, then the target_output should be &#34;UK&lt;OR&gt;England&#34;.
    &#34;&#34;&#34;

    target_output_delimiter: Optional[str] = &#34;&lt;OR&gt;&#34;

    def __post_init__(self):
        if self.target_output_delimiter == &#34;&#34;:
            raise EvalAlgorithmClientError(
                &#34;Empty target_output_delimiter is provided. Please either provide a non-empty string, or set it to None.&#34;
            )


def _normalize_text_quac_protocol(text: str) -&gt; str:
    &#34;&#34;&#34;
    Inspired by HELM: https://github.com/stanford-crfm/helm/blob/62f817eb695a31e8389e3f7be30609d3f0871837/src/helm/benchmark/metrics/basic_metrics.py
    Given a text, normalize it using the SQUAD / QUAC protocol. That is remove punctuations, excess whitespaces and articles, and return the lowercased tokens.
    SQUAD (https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/) and
    QuAC benchmarks (https://s3.amazonaws.com/my89public/quac/scorer.py) use this protocol to normalize text before evaluating it.
    HELM (https://github.com/stanford-crfm/helm/blob/62f817eb695a31e8389e3f7be30609d3f0871837/src/helm/benchmark/metrics/basic_metrics.py#L116)
    and HuggingFace evaluate (https://github.com/huggingface/evaluate/blob/775555d80af30d83dc6e9f42051840d29a34f31b/metrics/squad/compute_score.py#L11)
    also use this to normalization procedure.

    :param text: The text that needs to be normalized.
    :returns: The normalized text.
    &#34;&#34;&#34;

    text = text.lower()
    text = &#34;&#34;.join(character for character in text if character not in ENGLISH_PUNCTUATIONS)
    return &#34; &#34;.join([word for word in text.split(&#34; &#34;) if (word != &#34;&#34; and word not in ENGLISH_ARTICLES)])


def _f1_score(model_output: str, target_output: str, *, normalize_text: bool = False) -&gt; float:
    &#34;&#34;&#34;
    Inspired by the implementation in HELM: https://github.com/stanford-crfm/helm/blob/62f817eb695a31e8389e3f7be30609d3f0871837/src/helm/benchmark/metrics/basic_metrics.py#L182

    Given the model output and the target output, compute the f1 score between the two.
    F1-score is the harmonic mean of precision and recall where precision is the number of
    words in the prediction that are also found in the target output and recall is the number
    of words in the target output that are also found in the answer. We normalize the text following
    the QuAC protocol above.

    :param model_output: The output of a model that we want to evaluate.
    :param target_output: The reference or the &#34;ground truth&#34; output.
    :param normalize_text: Normalize the text before computing f1.
    :returns: The F1 score.
    &#34;&#34;&#34;
    if normalize_text:  # pragma: no branch
        model_output, target_output = (_normalize_text_quac_protocol(text) for text in (model_output, target_output))
    ret = f_measure(set(model_output.split(&#34; &#34;)), set(target_output.split(&#34; &#34;)))
    if ret is None:  # pragma: no cover
        return 0.0
    else:
        return float(ret)


def _exact_match_score(model_output: str, target_output: str) -&gt; float:
    &#34;&#34;&#34;
    Inspired by HELM: https://github.com/stanford-crfm/helm/blob/62f817eb695a31e8389e3f7be30609d3f0871837/src/helm/benchmark/metrics/basic_metrics.py#L137
    Computes if the two strings exactly match.

    :param model_output: The output of a model that we want to evaluate.
    :param target_output: The reference or the &#34;ground truth&#34; output.
    :returns: 0 is the two inputs do not match, else 1.
    &#34;&#34;&#34;
    return float(model_output.strip() == target_output.strip())


def _quasi_exact_match_score(model_output: str, target_output: str) -&gt; float:
    &#34;&#34;&#34;
    Inspired by HELM: https://github.com/stanford-crfm/helm/blob/62f817eb695a31e8389e3f7be30609d3f0871837/src/helm/benchmark/metrics/basic_metrics.py#L144
    Computes if the two strings exactly match after normalizing them.

    :param model_output: The output of a model that we want to evaluate.
    :param target_output: The reference or the &#34;ground truth&#34; output.
    :returns: 1 if the two strings match after normalization else 0.
    &#34;&#34;&#34;
    return float(
        _normalize_text_quac_protocol(model_output.strip()) == _normalize_text_quac_protocol(target_output.strip())
    )


QA_ACCURACY_SCORES_TO_FUNCS: Dict[str, Callable[..., float]] = {
    F1_SCORE: partial(_f1_score, normalize_text=True),
    EXACT_MATCH_SCORE: _exact_match_score,
    QUASI_EXACT_MATCH_SCORE: _quasi_exact_match_score,
}


class QAAccuracy(EvalAlgorithmInterface):
    &#34;&#34;&#34;
    QA Accuracy Eval algorithm
    &#34;&#34;&#34;

    eval_name = EvalAlgorithm.QA_ACCURACY.value

    def __init__(self, eval_algorithm_config: QAAccuracyConfig):
        &#34;&#34;&#34;Default constructor

        :param eval_algorithm_config: QA Accuracy eval algorithm config.
        &#34;&#34;&#34;
        super().__init__(eval_algorithm_config)
        self._eval_algorithm_config = eval_algorithm_config

    def evaluate(
        self,
        model: Optional[ModelRunner] = None,
        dataset_config: Optional[DataConfig] = None,
        prompt_template: Optional[str] = None,
        save: bool = False,
        num_records=100,
    ) -&gt; List[EvalOutput]:
        &#34;&#34;&#34;
        QA Accuracy evaluate.

        :param model: An instance of ModelRunner which is the model under evaluation
        :param dataset_config: The config to load the dataset to use for evaluation. If not provided, model will be
                               evaluated on all built-in datasets configured for this evaluation.
        :param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults
            will be used.
        :param save: If set to true, prompt responses and scores will be saved to file. The output is written to
                     EvalAlgorithmInterface.EVAL_RESULTS_PATH
        :param num_records: The number of records to be sampled randomly from the input dataset to perform the
                            evaluation
        :returns: List of EvalOutput objects. Current implementation returns only one score.
        &#34;&#34;&#34;
        if dataset_config:
            dataset_configs = [dataset_config]
        else:
            dataset_configs = [DATASET_CONFIGS[dataset_name] for dataset_name in EVAL_DATASETS[self.eval_name]]

        eval_outputs: List[EvalOutput] = []
        for dataset_config in dataset_configs:
            dataset = get_dataset(dataset_config, num_records)
            validate_dataset(dataset, [TARGET_OUTPUT_COLUMN_NAME, MODEL_INPUT_COLUMN_NAME])
            dataset_prompt_template = None
            if MODEL_OUTPUT_COLUMN_NAME not in dataset.columns():
                util.require(model, &#34;No ModelRunner provided. ModelRunner is required for inference on model_inputs&#34;)
                dataset_prompt_template = (
                    get_default_prompt_template(dataset_config.dataset_name) if not prompt_template else prompt_template
                )
                dataset = generate_prompt_column_for_dataset(
                    prompt_template=dataset_prompt_template,
                    data=dataset,
                    model_input_column_name=MODEL_INPUT_COLUMN_NAME,
                    prompt_column_name=PROMPT_COLUMN_NAME,
                )
                assert model  # to satisfy mypy
                dataset = generate_model_predict_response_for_dataset(
                    model=model,
                    data=dataset,
                    model_input_column_name=PROMPT_COLUMN_NAME,
                    model_output_column_name=MODEL_OUTPUT_COLUMN_NAME,
                    model_log_probability_column_name=MODEL_LOG_PROBABILITY_COLUMN_NAME,
                )
            with timed_block(f&#34;Computing score and aggregation on dataset {dataset_config.dataset_name}&#34;, logger):
                for eval_score, eval_fn in QA_ACCURACY_SCORES_TO_FUNCS.items():

                    def _generate_eval_scores(df: pd.DataFrame) -&gt; pd.Series:  # pragma: no cover
                        &#34;&#34;&#34;
                        Map function generating the scores for every input record in input dataset
                        &#34;&#34;&#34;
                        return pd.Series(
                            data=[
                                self._get_score(
                                    target_output=row[TARGET_OUTPUT_COLUMN_NAME],
                                    model_output=row[MODEL_OUTPUT_COLUMN_NAME],
                                    eval_fn=eval_fn,
                                )
                                for index, row in df.iterrows()
                            ]
                        )

                    dataset = dataset.add_column(eval_score, _generate_eval_scores)
                    dataset = dataset.materialize()

                dataset_scores, category_scores = aggregate_evaluation_scores(
                    dataset, [F1_SCORE, EXACT_MATCH_SCORE, QUASI_EXACT_MATCH_SCORE], agg_method=MEAN
                )

                eval_outputs.append(
                    EvalOutput(
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                        prompt_template=dataset_prompt_template,
                        dataset_scores=dataset_scores,
                        category_scores=category_scores,
                        output_path=generate_output_dataset_path(
                            path_to_parent_dir=self._eval_results_path,
                            eval_name=self.eval_name,
                            dataset_name=dataset_config.dataset_name,
                        ),
                    )
                )
            if save:
                save_dataset(
                    dataset=dataset,
                    score_names=list(QA_ACCURACY_SCORES_TO_FUNCS.keys()),
                    path=generate_output_dataset_path(
                        path_to_parent_dir=self._eval_results_path,
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                    ),
                )

        return eval_outputs

    def _get_score(
        self, target_output: str, model_output: str, eval_fn: Callable[..., float], **fn_kwargs: Any
    ) -&gt; float:
        &#34;&#34;&#34;
        Method to generate accuracy score for a target_output and model_output

        :param target_output: Target output
        :param model_output: Model output
        :returns: Computed score
        &#34;&#34;&#34;
        possible_targets = target_output.split(self._eval_algorithm_config.target_output_delimiter)

        return max([eval_fn(model_output, target, **fn_kwargs) for target in possible_targets])

    def evaluate_sample(self, target_output: str, model_output: str) -&gt; List[EvalScore]:  # type: ignore[override]
        &#34;&#34;&#34;
        Evaluate a single QA record.

        :param target_output: The expected responses from the model.
        :param model_output: An instance of ModelOutput which contains the responses from the model needed for this
                             evaluation.
        :returns: A List of EvalScores computed for prompts and responses.
        &#34;&#34;&#34;
        if target_output is None:
            raise EvalAlgorithmClientError(&#34;Missing required input: target_output, for QA Accuracy evaluate_sample&#34;)
        if model_output is None:
            raise EvalAlgorithmClientError(&#34;Missing required input: model_output, for QA Accuracy evaluate_sample&#34;)

        return [
            EvalScore(
                name=eval_score,
                value=self._get_score(target_output=target_output, model_output=model_output, eval_fn=eval_fn),
            )
            for eval_score, eval_fn in QA_ACCURACY_SCORES_TO_FUNCS.items()
        ]</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracy"><code class="flex name class">
<span>class <span class="ident">QAAccuracy</span></span>
<span>(</span><span>eval_algorithm_config: <a title="src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracyConfig" href="#src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracyConfig">QAAccuracyConfig</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>QA Accuracy Eval algorithm</p>
<p>Default constructor</p>
<p>:param eval_algorithm_config: QA Accuracy eval algorithm config.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class QAAccuracy(EvalAlgorithmInterface):
    &#34;&#34;&#34;
    QA Accuracy Eval algorithm
    &#34;&#34;&#34;

    eval_name = EvalAlgorithm.QA_ACCURACY.value

    def __init__(self, eval_algorithm_config: QAAccuracyConfig):
        &#34;&#34;&#34;Default constructor

        :param eval_algorithm_config: QA Accuracy eval algorithm config.
        &#34;&#34;&#34;
        super().__init__(eval_algorithm_config)
        self._eval_algorithm_config = eval_algorithm_config

    def evaluate(
        self,
        model: Optional[ModelRunner] = None,
        dataset_config: Optional[DataConfig] = None,
        prompt_template: Optional[str] = None,
        save: bool = False,
        num_records=100,
    ) -&gt; List[EvalOutput]:
        &#34;&#34;&#34;
        QA Accuracy evaluate.

        :param model: An instance of ModelRunner which is the model under evaluation
        :param dataset_config: The config to load the dataset to use for evaluation. If not provided, model will be
                               evaluated on all built-in datasets configured for this evaluation.
        :param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults
            will be used.
        :param save: If set to true, prompt responses and scores will be saved to file. The output is written to
                     EvalAlgorithmInterface.EVAL_RESULTS_PATH
        :param num_records: The number of records to be sampled randomly from the input dataset to perform the
                            evaluation
        :returns: List of EvalOutput objects. Current implementation returns only one score.
        &#34;&#34;&#34;
        if dataset_config:
            dataset_configs = [dataset_config]
        else:
            dataset_configs = [DATASET_CONFIGS[dataset_name] for dataset_name in EVAL_DATASETS[self.eval_name]]

        eval_outputs: List[EvalOutput] = []
        for dataset_config in dataset_configs:
            dataset = get_dataset(dataset_config, num_records)
            validate_dataset(dataset, [TARGET_OUTPUT_COLUMN_NAME, MODEL_INPUT_COLUMN_NAME])
            dataset_prompt_template = None
            if MODEL_OUTPUT_COLUMN_NAME not in dataset.columns():
                util.require(model, &#34;No ModelRunner provided. ModelRunner is required for inference on model_inputs&#34;)
                dataset_prompt_template = (
                    get_default_prompt_template(dataset_config.dataset_name) if not prompt_template else prompt_template
                )
                dataset = generate_prompt_column_for_dataset(
                    prompt_template=dataset_prompt_template,
                    data=dataset,
                    model_input_column_name=MODEL_INPUT_COLUMN_NAME,
                    prompt_column_name=PROMPT_COLUMN_NAME,
                )
                assert model  # to satisfy mypy
                dataset = generate_model_predict_response_for_dataset(
                    model=model,
                    data=dataset,
                    model_input_column_name=PROMPT_COLUMN_NAME,
                    model_output_column_name=MODEL_OUTPUT_COLUMN_NAME,
                    model_log_probability_column_name=MODEL_LOG_PROBABILITY_COLUMN_NAME,
                )
            with timed_block(f&#34;Computing score and aggregation on dataset {dataset_config.dataset_name}&#34;, logger):
                for eval_score, eval_fn in QA_ACCURACY_SCORES_TO_FUNCS.items():

                    def _generate_eval_scores(df: pd.DataFrame) -&gt; pd.Series:  # pragma: no cover
                        &#34;&#34;&#34;
                        Map function generating the scores for every input record in input dataset
                        &#34;&#34;&#34;
                        return pd.Series(
                            data=[
                                self._get_score(
                                    target_output=row[TARGET_OUTPUT_COLUMN_NAME],
                                    model_output=row[MODEL_OUTPUT_COLUMN_NAME],
                                    eval_fn=eval_fn,
                                )
                                for index, row in df.iterrows()
                            ]
                        )

                    dataset = dataset.add_column(eval_score, _generate_eval_scores)
                    dataset = dataset.materialize()

                dataset_scores, category_scores = aggregate_evaluation_scores(
                    dataset, [F1_SCORE, EXACT_MATCH_SCORE, QUASI_EXACT_MATCH_SCORE], agg_method=MEAN
                )

                eval_outputs.append(
                    EvalOutput(
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                        prompt_template=dataset_prompt_template,
                        dataset_scores=dataset_scores,
                        category_scores=category_scores,
                        output_path=generate_output_dataset_path(
                            path_to_parent_dir=self._eval_results_path,
                            eval_name=self.eval_name,
                            dataset_name=dataset_config.dataset_name,
                        ),
                    )
                )
            if save:
                save_dataset(
                    dataset=dataset,
                    score_names=list(QA_ACCURACY_SCORES_TO_FUNCS.keys()),
                    path=generate_output_dataset_path(
                        path_to_parent_dir=self._eval_results_path,
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                    ),
                )

        return eval_outputs

    def _get_score(
        self, target_output: str, model_output: str, eval_fn: Callable[..., float], **fn_kwargs: Any
    ) -&gt; float:
        &#34;&#34;&#34;
        Method to generate accuracy score for a target_output and model_output

        :param target_output: Target output
        :param model_output: Model output
        :returns: Computed score
        &#34;&#34;&#34;
        possible_targets = target_output.split(self._eval_algorithm_config.target_output_delimiter)

        return max([eval_fn(model_output, target, **fn_kwargs) for target in possible_targets])

    def evaluate_sample(self, target_output: str, model_output: str) -&gt; List[EvalScore]:  # type: ignore[override]
        &#34;&#34;&#34;
        Evaluate a single QA record.

        :param target_output: The expected responses from the model.
        :param model_output: An instance of ModelOutput which contains the responses from the model needed for this
                             evaluation.
        :returns: A List of EvalScores computed for prompts and responses.
        &#34;&#34;&#34;
        if target_output is None:
            raise EvalAlgorithmClientError(&#34;Missing required input: target_output, for QA Accuracy evaluate_sample&#34;)
        if model_output is None:
            raise EvalAlgorithmClientError(&#34;Missing required input: model_output, for QA Accuracy evaluate_sample&#34;)

        return [
            EvalScore(
                name=eval_score,
                value=self._get_score(target_output=target_output, model_output=model_output, eval_fn=eval_fn),
            )
            for eval_score, eval_fn in QA_ACCURACY_SCORES_TO_FUNCS.items()
        ]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>amazon_fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface</li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracy.eval_name"><code class="name">var <span class="ident">eval_name</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracy.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, model: Optional[amazon_fmeval.model_runners.model_runner.ModelRunner] = None, dataset_config: Optional[amazon_fmeval.data_loaders.data_config.DataConfig] = None, prompt_template: Optional[str] = None, save: bool = False, num_records=100) ‑> List[amazon_fmeval.eval_algorithms.EvalOutput]</span>
</code></dt>
<dd>
<div class="desc"><p>QA Accuracy evaluate.</p>
<p>:param model: An instance of ModelRunner which is the model under evaluation
:param dataset_config: The config to load the dataset to use for evaluation. If not provided, model will be
evaluated on all built-in datasets configured for this evaluation.
:param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults
will be used.
:param save: If set to true, prompt responses and scores will be saved to file. The output is written to
EvalAlgorithmInterface.EVAL_RESULTS_PATH
:param num_records: The number of records to be sampled randomly from the input dataset to perform the
evaluation
:returns: List of EvalOutput objects. Current implementation returns only one score.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(
    self,
    model: Optional[ModelRunner] = None,
    dataset_config: Optional[DataConfig] = None,
    prompt_template: Optional[str] = None,
    save: bool = False,
    num_records=100,
) -&gt; List[EvalOutput]:
    &#34;&#34;&#34;
    QA Accuracy evaluate.

    :param model: An instance of ModelRunner which is the model under evaluation
    :param dataset_config: The config to load the dataset to use for evaluation. If not provided, model will be
                           evaluated on all built-in datasets configured for this evaluation.
    :param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults
        will be used.
    :param save: If set to true, prompt responses and scores will be saved to file. The output is written to
                 EvalAlgorithmInterface.EVAL_RESULTS_PATH
    :param num_records: The number of records to be sampled randomly from the input dataset to perform the
                        evaluation
    :returns: List of EvalOutput objects. Current implementation returns only one score.
    &#34;&#34;&#34;
    if dataset_config:
        dataset_configs = [dataset_config]
    else:
        dataset_configs = [DATASET_CONFIGS[dataset_name] for dataset_name in EVAL_DATASETS[self.eval_name]]

    eval_outputs: List[EvalOutput] = []
    for dataset_config in dataset_configs:
        dataset = get_dataset(dataset_config, num_records)
        validate_dataset(dataset, [TARGET_OUTPUT_COLUMN_NAME, MODEL_INPUT_COLUMN_NAME])
        dataset_prompt_template = None
        if MODEL_OUTPUT_COLUMN_NAME not in dataset.columns():
            util.require(model, &#34;No ModelRunner provided. ModelRunner is required for inference on model_inputs&#34;)
            dataset_prompt_template = (
                get_default_prompt_template(dataset_config.dataset_name) if not prompt_template else prompt_template
            )
            dataset = generate_prompt_column_for_dataset(
                prompt_template=dataset_prompt_template,
                data=dataset,
                model_input_column_name=MODEL_INPUT_COLUMN_NAME,
                prompt_column_name=PROMPT_COLUMN_NAME,
            )
            assert model  # to satisfy mypy
            dataset = generate_model_predict_response_for_dataset(
                model=model,
                data=dataset,
                model_input_column_name=PROMPT_COLUMN_NAME,
                model_output_column_name=MODEL_OUTPUT_COLUMN_NAME,
                model_log_probability_column_name=MODEL_LOG_PROBABILITY_COLUMN_NAME,
            )
        with timed_block(f&#34;Computing score and aggregation on dataset {dataset_config.dataset_name}&#34;, logger):
            for eval_score, eval_fn in QA_ACCURACY_SCORES_TO_FUNCS.items():

                def _generate_eval_scores(df: pd.DataFrame) -&gt; pd.Series:  # pragma: no cover
                    &#34;&#34;&#34;
                    Map function generating the scores for every input record in input dataset
                    &#34;&#34;&#34;
                    return pd.Series(
                        data=[
                            self._get_score(
                                target_output=row[TARGET_OUTPUT_COLUMN_NAME],
                                model_output=row[MODEL_OUTPUT_COLUMN_NAME],
                                eval_fn=eval_fn,
                            )
                            for index, row in df.iterrows()
                        ]
                    )

                dataset = dataset.add_column(eval_score, _generate_eval_scores)
                dataset = dataset.materialize()

            dataset_scores, category_scores = aggregate_evaluation_scores(
                dataset, [F1_SCORE, EXACT_MATCH_SCORE, QUASI_EXACT_MATCH_SCORE], agg_method=MEAN
            )

            eval_outputs.append(
                EvalOutput(
                    eval_name=self.eval_name,
                    dataset_name=dataset_config.dataset_name,
                    prompt_template=dataset_prompt_template,
                    dataset_scores=dataset_scores,
                    category_scores=category_scores,
                    output_path=generate_output_dataset_path(
                        path_to_parent_dir=self._eval_results_path,
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                    ),
                )
            )
        if save:
            save_dataset(
                dataset=dataset,
                score_names=list(QA_ACCURACY_SCORES_TO_FUNCS.keys()),
                path=generate_output_dataset_path(
                    path_to_parent_dir=self._eval_results_path,
                    eval_name=self.eval_name,
                    dataset_name=dataset_config.dataset_name,
                ),
            )

    return eval_outputs</code></pre>
</details>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracy.evaluate_sample"><code class="name flex">
<span>def <span class="ident">evaluate_sample</span></span>(<span>self, target_output: str, model_output: str) ‑> List[amazon_fmeval.eval_algorithms.EvalScore]</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate a single QA record.</p>
<p>:param target_output: The expected responses from the model.
:param model_output: An instance of ModelOutput which contains the responses from the model needed for this
evaluation.
:returns: A List of EvalScores computed for prompts and responses.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_sample(self, target_output: str, model_output: str) -&gt; List[EvalScore]:  # type: ignore[override]
    &#34;&#34;&#34;
    Evaluate a single QA record.

    :param target_output: The expected responses from the model.
    :param model_output: An instance of ModelOutput which contains the responses from the model needed for this
                         evaluation.
    :returns: A List of EvalScores computed for prompts and responses.
    &#34;&#34;&#34;
    if target_output is None:
        raise EvalAlgorithmClientError(&#34;Missing required input: target_output, for QA Accuracy evaluate_sample&#34;)
    if model_output is None:
        raise EvalAlgorithmClientError(&#34;Missing required input: model_output, for QA Accuracy evaluate_sample&#34;)

    return [
        EvalScore(
            name=eval_score,
            value=self._get_score(target_output=target_output, model_output=model_output, eval_fn=eval_fn),
        )
        for eval_score, eval_fn in QA_ACCURACY_SCORES_TO_FUNCS.items()
    ]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracyConfig"><code class="flex name class">
<span>class <span class="ident">QAAccuracyConfig</span></span>
<span>(</span><span>target_output_delimiter: Optional[str] = &#x27;&lt;OR&gt;&#x27;)</span>
</code></dt>
<dd>
<div class="desc"><p>Configuration for the QA Accuracy Evaluation</p>
<p>:param target_output_delimiter: Target Output can have multiple answers. We expect customer to combine all the
possible answers into a single string and use the delimiter to separate them. For instance,
if the answers are ["UK", "England"] and the delimiter="<OR>", then the target_output should be "UK<OR>England".</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass(frozen=True)
class QAAccuracyConfig(EvalAlgorithmConfig):
    &#34;&#34;&#34;
    Configuration for the QA Accuracy Evaluation

    :param target_output_delimiter: Target Output can have multiple answers. We expect customer to combine all the
        possible answers into a single string and use the delimiter to separate them. For instance,
        if the answers are [&#34;UK&#34;, &#34;England&#34;] and the delimiter=&#34;&lt;OR&gt;&#34;, then the target_output should be &#34;UK&lt;OR&gt;England&#34;.
    &#34;&#34;&#34;

    target_output_delimiter: Optional[str] = &#34;&lt;OR&gt;&#34;

    def __post_init__(self):
        if self.target_output_delimiter == &#34;&#34;:
            raise EvalAlgorithmClientError(
                &#34;Empty target_output_delimiter is provided. Please either provide a non-empty string, or set it to None.&#34;
            )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>amazon_fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmConfig</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracyConfig.target_output_delimiter"><code class="name">var <span class="ident">target_output_delimiter</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.amazon_fmeval.eval_algorithms" href="index.html">src.amazon_fmeval.eval_algorithms</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracy" href="#src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracy">QAAccuracy</a></code></h4>
<ul class="">
<li><code><a title="src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracy.eval_name" href="#src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracy.eval_name">eval_name</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracy.evaluate" href="#src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracy.evaluate">evaluate</a></code></li>
<li><code><a title="src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracy.evaluate_sample" href="#src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracy.evaluate_sample">evaluate_sample</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracyConfig" href="#src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracyConfig">QAAccuracyConfig</a></code></h4>
<ul class="">
<li><code><a title="src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracyConfig.target_output_delimiter" href="#src.amazon_fmeval.eval_algorithms.qa_accuracy.QAAccuracyConfig.target_output_delimiter">target_output_delimiter</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>