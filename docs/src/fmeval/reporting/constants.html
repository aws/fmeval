<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 14.1.0"/>
    <title>src.fmeval.reporting.constants API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .pdoc-alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:1rem center;margin-bottom:1rem;}.pdoc .pdoc-alert > *:last-child{margin-bottom:0;}.pdoc .pdoc-alert-note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../reporting.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;src.fmeval.reporting</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="variable" href="#CENTER">CENTER</a>
            </li>
            <li>
                    <a class="variable" href="#LEFT">LEFT</a>
            </li>
            <li>
                    <a class="variable" href="#RIGHT">RIGHT</a>
            </li>
            <li>
                    <a class="class" href="#ListType">ListType</a>
                            <ul class="memberlist">
                        <li>
                                <a class="variable" href="#ListType.BULLETED">BULLETED</a>
                        </li>
                        <li>
                                <a class="variable" href="#ListType.NUMBERED">NUMBERED</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="variable" href="#SINGLE_NEWLINE">SINGLE_NEWLINE</a>
            </li>
            <li>
                    <a class="variable" href="#DOUBLE_NEWLINE">DOUBLE_NEWLINE</a>
            </li>
            <li>
                    <a class="variable" href="#NUM_SAMPLES_TO_DISPLAY_IN_TABLE">NUM_SAMPLES_TO_DISPLAY_IN_TABLE</a>
            </li>
            <li>
                    <a class="variable" href="#CATEGORY_BAR_COLOR">CATEGORY_BAR_COLOR</a>
            </li>
            <li>
                    <a class="variable" href="#OVERALL_BAR_COLOR">OVERALL_BAR_COLOR</a>
            </li>
            <li>
                    <a class="variable" href="#MAX_CHAR">MAX_CHAR</a>
            </li>
            <li>
                    <a class="variable" href="#MARKDOWN_EXTENSIONS">MARKDOWN_EXTENSIONS</a>
            </li>
            <li>
                    <a class="variable" href="#DATASET_SCORE_LABEL">DATASET_SCORE_LABEL</a>
            </li>
            <li>
                    <a class="variable" href="#AGGREGATE_ONLY_SCORES">AGGREGATE_ONLY_SCORES</a>
            </li>
            <li>
                    <a class="variable" href="#GENERAL_STRING_REPLACEMENTS">GENERAL_STRING_REPLACEMENTS</a>
            </li>
            <li>
                    <a class="variable" href="#SCORE_STRING_REPLACEMENTS">SCORE_STRING_REPLACEMENTS</a>
            </li>
            <li>
                    <a class="variable" href="#EVAL_NAME_STRING_REPLACEMENTS">EVAL_NAME_STRING_REPLACEMENTS</a>
            </li>
            <li>
                    <a class="variable" href="#PLOT_TITLE_STRING_REPLACEMENTS">PLOT_TITLE_STRING_REPLACEMENTS</a>
            </li>
            <li>
                    <a class="variable" href="#COLUMN_NAME_STRING_REPLACEMENTS">COLUMN_NAME_STRING_REPLACEMENTS</a>
            </li>
            <li>
                    <a class="variable" href="#AVOID_REMOVE_UNDERSCORE">AVOID_REMOVE_UNDERSCORE</a>
            </li>
            <li>
                    <a class="variable" href="#ACCURACY_SEMANTIC_ROBUSTNESS_ALGOS">ACCURACY_SEMANTIC_ROBUSTNESS_ALGOS</a>
            </li>
            <li>
                    <a class="variable" href="#ACCURACY_SEMANTIC_ROBUSTNESS_SCORES">ACCURACY_SEMANTIC_ROBUSTNESS_SCORES</a>
            </li>
            <li>
                    <a class="variable" href="#BUILT_IN_DATASET">BUILT_IN_DATASET</a>
            </li>
            <li>
                    <a class="variable" href="#CUSTOM_DATASET">CUSTOM_DATASET</a>
            </li>
            <li>
                    <a class="variable" href="#PROMPT_COLUMN_NAME">PROMPT_COLUMN_NAME</a>
            </li>
            <li>
                    <a class="variable" href="#TOXICITY_EVAL_NAMES">TOXICITY_EVAL_NAMES</a>
            </li>
            <li>
                    <a class="variable" href="#PROBABILITY_RATIO">PROBABILITY_RATIO</a>
            </li>
            <li>
                    <a class="variable" href="#IS_BIASED">IS_BIASED</a>
            </li>
            <li>
                    <a class="variable" href="#TOXIGEN_NAME">TOXIGEN_NAME</a>
            </li>
            <li>
                    <a class="variable" href="#DETOXIFY_NAME">DETOXIFY_NAME</a>
            </li>
            <li>
                    <a class="variable" href="#TOXIGEN_URI">TOXIGEN_URI</a>
            </li>
            <li>
                    <a class="variable" href="#DETOXIFY_URI">DETOXIFY_URI</a>
            </li>
            <li>
                    <a class="variable" href="#TABLE_DESCRIPTION">TABLE_DESCRIPTION</a>
            </li>
            <li>
                    <a class="variable" href="#WER_TABLE_DESCRIPTION">WER_TABLE_DESCRIPTION</a>
            </li>
            <li>
                    <a class="variable" href="#STEREOTYPING_TABLE_DESCRIPTION">STEREOTYPING_TABLE_DESCRIPTION</a>
            </li>
            <li>
                    <a class="variable" href="#FACTUAL_KNOWLEDGE_TABLE_DESCRIPTION">FACTUAL_KNOWLEDGE_TABLE_DESCRIPTION</a>
            </li>
            <li>
                    <a class="variable" href="#SCORE_DESCRIPTIONS">SCORE_DESCRIPTIONS</a>
            </li>
            <li>
                    <a class="class" href="#DatasetDetails">DatasetDetails</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#DatasetDetails.__init__">DatasetDetails</a>
                        </li>
                        <li>
                                <a class="variable" href="#DatasetDetails.name">name</a>
                        </li>
                        <li>
                                <a class="variable" href="#DatasetDetails.url">url</a>
                        </li>
                        <li>
                                <a class="variable" href="#DatasetDetails.description">description</a>
                        </li>
                        <li>
                                <a class="variable" href="#DatasetDetails.size">size</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="variable" href="#DATASET_DETAILS">DATASET_DETAILS</a>
            </li>
            <li>
                    <a class="variable" href="#TREX_DESCRIPTION_EXAMPLES">TREX_DESCRIPTION_EXAMPLES</a>
            </li>
            <li>
                    <a class="variable" href="#CROWS_PAIRS_DISCLAIMER">CROWS_PAIRS_DISCLAIMER</a>
            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
src<wbr>.<a href="./../../fmeval.html">fmeval</a><wbr>.<a href="./../reporting.html">reporting</a><wbr>.constants    </h1>

                
                        <input id="mod-constants-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-constants-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">  1</span></a><span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">  2</span></a><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">NamedTuple</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">List</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">  3</span></a>
</span><span id="L-4"><a href="#L-4"><span class="linenos">  4</span></a><span class="kn">from</span> <span class="nn">fmeval.eval_algorithms.factual_knowledge</span> <span class="kn">import</span> <span class="n">FACTUAL_KNOWLEDGE</span>
</span><span id="L-5"><a href="#L-5"><span class="linenos">  5</span></a><span class="kn">from</span> <span class="nn">fmeval.eval_algorithms.prompt_stereotyping</span> <span class="kn">import</span> <span class="n">PROMPT_STEREOTYPING</span><span class="p">,</span> <span class="n">LOG_PROBABILITY_DIFFERENCE</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">  6</span></a><span class="kn">from</span> <span class="nn">fmeval.eval_algorithms.qa_accuracy</span> <span class="kn">import</span> <span class="n">F1_SCORE</span><span class="p">,</span> <span class="n">EXACT_MATCH_SCORE</span><span class="p">,</span> <span class="n">QUASI_EXACT_MATCH_SCORE</span>
</span><span id="L-7"><a href="#L-7"><span class="linenos">  7</span></a><span class="kn">from</span> <span class="nn">fmeval.eval_algorithms.summarization_accuracy</span> <span class="kn">import</span> <span class="n">METEOR_SCORE</span><span class="p">,</span> <span class="n">BERT_SCORE</span><span class="p">,</span> <span class="n">ROUGE_SCORE</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">  8</span></a><span class="kn">from</span> <span class="nn">fmeval.eval_algorithms.classification_accuracy</span> <span class="kn">import</span> <span class="p">(</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">  9</span></a>    <span class="n">CLASSIFICATION_ACCURACY_SCORE</span><span class="p">,</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos"> 10</span></a>    <span class="n">BALANCED_ACCURACY_SCORE</span><span class="p">,</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos"> 11</span></a>    <span class="n">PRECISION_SCORE</span><span class="p">,</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos"> 12</span></a>    <span class="n">RECALL_SCORE</span><span class="p">,</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos"> 13</span></a><span class="p">)</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos"> 14</span></a><span class="kn">from</span> <span class="nn">fmeval.eval_algorithms.classification_accuracy_semantic_robustness</span> <span class="kn">import</span> <span class="p">(</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos"> 15</span></a>    <span class="n">DELTA_CLASSIFICATION_ACCURACY_SCORE</span><span class="p">,</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos"> 16</span></a><span class="p">)</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos"> 17</span></a><span class="kn">from</span> <span class="nn">fmeval.eval_algorithms.qa_accuracy_semantic_robustness</span> <span class="kn">import</span> <span class="p">(</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos"> 18</span></a>    <span class="n">DELTA_F1_SCORE</span><span class="p">,</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos"> 19</span></a>    <span class="n">DELTA_EXACT_MATCH_SCORE</span><span class="p">,</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos"> 20</span></a>    <span class="n">DELTA_QUASI_EXACT_MATCH_SCORE</span><span class="p">,</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos"> 21</span></a><span class="p">)</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos"> 22</span></a><span class="kn">from</span> <span class="nn">fmeval.eval_algorithms.summarization_accuracy_semantic_robustness</span> <span class="kn">import</span> <span class="p">(</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos"> 23</span></a>    <span class="n">DELTA_ROUGE_SCORE</span><span class="p">,</span>
</span><span id="L-24"><a href="#L-24"><span class="linenos"> 24</span></a>    <span class="n">DELTA_BERT_SCORE</span><span class="p">,</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos"> 25</span></a>    <span class="n">DELTA_METEOR_SCORE</span><span class="p">,</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos"> 26</span></a><span class="p">)</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos"> 27</span></a><span class="kn">from</span> <span class="nn">fmeval.eval_algorithms.general_semantic_robustness</span> <span class="kn">import</span> <span class="n">WER_SCORE</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos"> 28</span></a><span class="kn">from</span> <span class="nn">fmeval.eval_algorithms</span> <span class="kn">import</span> <span class="p">(</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos"> 29</span></a>    <span class="n">TREX</span><span class="p">,</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos"> 30</span></a>    <span class="n">BOOLQ</span><span class="p">,</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos"> 31</span></a>    <span class="n">TRIVIA_QA</span><span class="p">,</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos"> 32</span></a>    <span class="n">NATURAL_QUESTIONS</span><span class="p">,</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos"> 33</span></a>    <span class="n">CROWS_PAIRS</span><span class="p">,</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos"> 34</span></a>    <span class="n">XSUM</span><span class="p">,</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos"> 35</span></a>    <span class="n">WOMENS_CLOTHING_ECOMMERCE_REVIEWS</span><span class="p">,</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos"> 36</span></a>    <span class="n">BOLD</span><span class="p">,</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos"> 37</span></a>    <span class="n">WIKITEXT2</span><span class="p">,</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos"> 38</span></a>    <span class="n">REAL_TOXICITY_PROMPTS</span><span class="p">,</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos"> 39</span></a>    <span class="n">REAL_TOXICITY_PROMPTS_CHALLENGING</span><span class="p">,</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos"> 40</span></a>    <span class="n">GIGAWORD</span><span class="p">,</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos"> 41</span></a>    <span class="n">GOV_REPORT</span><span class="p">,</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos"> 42</span></a><span class="p">)</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos"> 43</span></a><span class="kn">from</span> <span class="nn">fmeval.eval_algorithms</span> <span class="kn">import</span> <span class="n">EvalAlgorithm</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos"> 44</span></a><span class="kn">from</span> <span class="nn">fmeval.eval_algorithms.helper_models.helper_model</span> <span class="kn">import</span> <span class="p">(</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos"> 45</span></a>    <span class="n">TOXIGEN_SCORE_NAME</span><span class="p">,</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos"> 46</span></a>    <span class="n">DETOXIFY_SCORE_TOXICITY</span><span class="p">,</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos"> 47</span></a>    <span class="n">DETOXIFY_SCORE_SEVERE_TOXICITY</span><span class="p">,</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos"> 48</span></a>    <span class="n">DETOXIFY_SCORE_OBSCENE</span><span class="p">,</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos"> 49</span></a>    <span class="n">DETOXIFY_SCORE_IDENTITY_ATTACK</span><span class="p">,</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos"> 50</span></a>    <span class="n">DETOXIFY_SCORE_INSULT</span><span class="p">,</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos"> 51</span></a>    <span class="n">DETOXIFY_SCORE_THREAT</span><span class="p">,</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos"> 52</span></a>    <span class="n">DETOXIFY_SCORE_SEXUAL_EXPLICIT</span><span class="p">,</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos"> 53</span></a><span class="p">)</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos"> 54</span></a>
</span><span id="L-55"><a href="#L-55"><span class="linenos"> 55</span></a><span class="c1"># For general HTML alignment</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos"> 56</span></a><span class="n">CENTER</span> <span class="o">=</span> <span class="s2">&quot;center&quot;</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos"> 57</span></a><span class="n">LEFT</span> <span class="o">=</span> <span class="s2">&quot;left&quot;</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos"> 58</span></a><span class="n">RIGHT</span> <span class="o">=</span> <span class="s2">&quot;right&quot;</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos"> 59</span></a>
</span><span id="L-60"><a href="#L-60"><span class="linenos"> 60</span></a>
</span><span id="L-61"><a href="#L-61"><span class="linenos"> 61</span></a><span class="k">class</span> <span class="nc">ListType</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos"> 62</span></a>    <span class="n">BULLETED</span> <span class="o">=</span> <span class="s2">&quot;bulleted&quot;</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos"> 63</span></a>    <span class="n">NUMBERED</span> <span class="o">=</span> <span class="s2">&quot;numbered&quot;</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos"> 64</span></a>
</span><span id="L-65"><a href="#L-65"><span class="linenos"> 65</span></a>
</span><span id="L-66"><a href="#L-66"><span class="linenos"> 66</span></a><span class="c1"># For general use in Markdown-related code</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos"> 67</span></a><span class="n">SINGLE_NEWLINE</span> <span class="o">=</span> <span class="s2">&quot;  </span><span class="se">\n</span><span class="s2">&quot;</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos"> 68</span></a><span class="n">DOUBLE_NEWLINE</span> <span class="o">=</span> <span class="s2">&quot;  </span><span class="se">\n\n</span><span class="s2">&quot;</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos"> 69</span></a>
</span><span id="L-70"><a href="#L-70"><span class="linenos"> 70</span></a><span class="c1"># For tables and bar plots</span>
</span><span id="L-71"><a href="#L-71"><span class="linenos"> 71</span></a><span class="n">NUM_SAMPLES_TO_DISPLAY_IN_TABLE</span> <span class="o">=</span> <span class="mi">5</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos"> 72</span></a><span class="n">CATEGORY_BAR_COLOR</span> <span class="o">=</span> <span class="s2">&quot;steelblue&quot;</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos"> 73</span></a><span class="n">OVERALL_BAR_COLOR</span> <span class="o">=</span> <span class="s2">&quot;coral&quot;</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos"> 74</span></a><span class="n">MAX_CHAR</span> <span class="o">=</span> <span class="mi">200</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos"> 75</span></a>
</span><span id="L-76"><a href="#L-76"><span class="linenos"> 76</span></a><span class="c1"># Extensions used by the markdown library to convert markdown to HTML</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos"> 77</span></a><span class="n">MARKDOWN_EXTENSIONS</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;tables&quot;</span><span class="p">,</span> <span class="s2">&quot;md_in_html&quot;</span><span class="p">]</span>
</span><span id="L-78"><a href="#L-78"><span class="linenos"> 78</span></a>
</span><span id="L-79"><a href="#L-79"><span class="linenos"> 79</span></a><span class="c1"># Dataset score label used in category bar plot</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos"> 80</span></a><span class="n">DATASET_SCORE_LABEL</span> <span class="o">=</span> <span class="s2">&quot;Overall&quot;</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos"> 81</span></a>
</span><span id="L-82"><a href="#L-82"><span class="linenos"> 82</span></a><span class="c1"># Scores that are not per sample</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos"> 83</span></a><span class="n">AGGREGATE_ONLY_SCORES</span> <span class="o">=</span> <span class="p">[</span><span class="n">BALANCED_ACCURACY_SCORE</span><span class="p">,</span> <span class="n">PRECISION_SCORE</span><span class="p">,</span> <span class="n">RECALL_SCORE</span><span class="p">]</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos"> 84</span></a>
</span><span id="L-85"><a href="#L-85"><span class="linenos"> 85</span></a><span class="c1"># For string formatting in eval names/score names</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos"> 86</span></a><span class="n">GENERAL_STRING_REPLACEMENTS</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;qa&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&amp;A&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;f1&quot;</span><span class="p">,</span> <span class="s2">&quot;F1&quot;</span><span class="p">)]</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos"> 87</span></a><span class="n">SCORE_STRING_REPLACEMENTS</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos"> 88</span></a>    <span class="p">(</span><span class="s2">&quot;prompt stereotyping&quot;</span><span class="p">,</span> <span class="s2">&quot;is_biased&quot;</span><span class="p">),</span>
</span><span id="L-89"><a href="#L-89"><span class="linenos"> 89</span></a>    <span class="p">(</span><span class="s2">&quot;meteor&quot;</span><span class="p">,</span> <span class="s2">&quot;METEOR&quot;</span><span class="p">),</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos"> 90</span></a>    <span class="p">(</span><span class="s2">&quot;bertscore&quot;</span><span class="p">,</span> <span class="s2">&quot;BERTScore&quot;</span><span class="p">),</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos"> 91</span></a>    <span class="p">(</span><span class="s2">&quot;rouge&quot;</span><span class="p">,</span> <span class="s2">&quot;ROUGE&quot;</span><span class="p">),</span>
</span><span id="L-92"><a href="#L-92"><span class="linenos"> 92</span></a>    <span class="p">(</span><span class="s2">&quot;F1 score&quot;</span><span class="p">,</span> <span class="s2">&quot;F1 over words&quot;</span><span class="p">),</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos"> 93</span></a>    <span class="p">(</span><span class="s2">&quot;obscene&quot;</span><span class="p">,</span> <span class="s2">&quot;Obscenity&quot;</span><span class="p">),</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos"> 94</span></a>    <span class="p">(</span><span class="s2">&quot;sexual explicit&quot;</span><span class="p">,</span> <span class="s2">&quot;Sexual Explicitness&quot;</span><span class="p">),</span>
</span><span id="L-95"><a href="#L-95"><span class="linenos"> 95</span></a><span class="p">]</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos"> 96</span></a><span class="n">EVAL_NAME_STRING_REPLACEMENTS</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-97"><a href="#L-97"><span class="linenos"> 97</span></a>    <span class="p">(</span><span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">QA_ACCURACY</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">ACCURACY</span><span class="o">.</span><span class="n">value</span><span class="p">),</span>
</span><span id="L-98"><a href="#L-98"><span class="linenos"> 98</span></a>    <span class="p">(</span><span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">SUMMARIZATION_ACCURACY</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">ACCURACY</span><span class="o">.</span><span class="n">value</span><span class="p">),</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos"> 99</span></a>    <span class="p">(</span><span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">CLASSIFICATION_ACCURACY</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">ACCURACY</span><span class="o">.</span><span class="n">value</span><span class="p">),</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos">100</span></a>    <span class="p">(</span><span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">GENERAL_SEMANTIC_ROBUSTNESS</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot;semantic_robustness&quot;</span><span class="p">),</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos">101</span></a>    <span class="p">(</span><span class="s2">&quot;accuracy_semantic_robustness&quot;</span><span class="p">,</span> <span class="s2">&quot;semantic_robustness&quot;</span><span class="p">),</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos">102</span></a>    <span class="p">(</span><span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">QA_ACCURACY</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">TOXICITY</span><span class="o">.</span><span class="n">value</span><span class="p">),</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos">103</span></a>    <span class="p">(</span><span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">SUMMARIZATION_TOXICITY</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">TOXICITY</span><span class="o">.</span><span class="n">value</span><span class="p">),</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos">104</span></a>    <span class="p">(</span><span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">CLASSIFICATION_ACCURACY</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">TOXICITY</span><span class="o">.</span><span class="n">value</span><span class="p">),</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos">105</span></a><span class="p">]</span>
</span><span id="L-106"><a href="#L-106"><span class="linenos">106</span></a><span class="n">PLOT_TITLE_STRING_REPLACEMENTS</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;prompt_stereotyping&quot;</span><span class="p">,</span> <span class="s2">&quot;is_biased score&quot;</span><span class="p">)]</span>
</span><span id="L-107"><a href="#L-107"><span class="linenos">107</span></a><span class="n">COLUMN_NAME_STRING_REPLACEMENTS</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos">108</span></a>    <span class="p">(</span><span class="s2">&quot;sent_more&quot;</span><span class="p">,</span> <span class="s2">&quot;s_more&quot;</span><span class="p">),</span>
</span><span id="L-109"><a href="#L-109"><span class="linenos">109</span></a>    <span class="p">(</span><span class="s2">&quot;s_more_input&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;math&gt;S&lt;sub&gt;more&lt;/sub&gt;&lt;/math&gt;&quot;</span><span class="p">),</span>
</span><span id="L-110"><a href="#L-110"><span class="linenos">110</span></a>    <span class="p">(</span><span class="s2">&quot;sent_less&quot;</span><span class="p">,</span> <span class="s2">&quot;s_less&quot;</span><span class="p">),</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos">111</span></a>    <span class="p">(</span><span class="s2">&quot;s_less_input&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;math&gt;S&lt;sub&gt;less&lt;/sub&gt;&lt;/math&gt;&quot;</span><span class="p">),</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos">112</span></a>    <span class="p">(</span><span class="s2">&quot;prob_&quot;</span><span class="p">,</span> <span class="s2">&quot;probability_&quot;</span><span class="p">),</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos">113</span></a>    <span class="p">(</span><span class="s2">&quot;word_error_rate&quot;</span><span class="p">,</span> <span class="s2">&quot;Average WER&quot;</span><span class="p">),</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos">114</span></a>    <span class="p">(</span><span class="s2">&quot;classification_accuracy&quot;</span><span class="p">,</span> <span class="s2">&quot;accuracy&quot;</span><span class="p">),</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos">115</span></a>    <span class="p">(</span><span class="s2">&quot;f1_score&quot;</span><span class="p">,</span> <span class="s2">&quot;f1 over words&quot;</span><span class="p">),</span>
</span><span id="L-116"><a href="#L-116"><span class="linenos">116</span></a>    <span class="p">(</span><span class="s2">&quot;meteor&quot;</span><span class="p">,</span> <span class="s2">&quot;METEOR&quot;</span><span class="p">),</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos">117</span></a>    <span class="p">(</span><span class="s2">&quot;bertscore&quot;</span><span class="p">,</span> <span class="s2">&quot;BERTScore&quot;</span><span class="p">),</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos">118</span></a>    <span class="p">(</span><span class="s2">&quot;rouge&quot;</span><span class="p">,</span> <span class="s2">&quot;ROUGE&quot;</span><span class="p">),</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos">119</span></a><span class="p">]</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos">120</span></a><span class="n">AVOID_REMOVE_UNDERSCORE</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sent_more_input&quot;</span><span class="p">,</span> <span class="s2">&quot;sent_less_input&quot;</span><span class="p">,</span> <span class="s2">&quot;is_biased&quot;</span><span class="p">]</span>
</span><span id="L-121"><a href="#L-121"><span class="linenos">121</span></a><span class="n">ACCURACY_SEMANTIC_ROBUSTNESS_ALGOS</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-122"><a href="#L-122"><span class="linenos">122</span></a>    <span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">SUMMARIZATION_ACCURACY_SEMANTIC_ROBUSTNESS</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos">123</span></a>    <span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">QA_ACCURACY_SEMANTIC_ROBUSTNESS</span><span class="p">,</span>
</span><span id="L-124"><a href="#L-124"><span class="linenos">124</span></a>    <span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">CLASSIFICATION_ACCURACY_SEMANTIC_ROBUSTNESS</span><span class="p">,</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos">125</span></a><span class="p">]</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos">126</span></a><span class="n">ACCURACY_SEMANTIC_ROBUSTNESS_SCORES</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos">127</span></a>    <span class="n">CLASSIFICATION_ACCURACY_SCORE</span><span class="p">,</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos">128</span></a>    <span class="n">METEOR_SCORE</span><span class="p">,</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos">129</span></a>    <span class="n">BERT_SCORE</span><span class="p">,</span>
</span><span id="L-130"><a href="#L-130"><span class="linenos">130</span></a>    <span class="n">ROUGE_SCORE</span><span class="p">,</span>
</span><span id="L-131"><a href="#L-131"><span class="linenos">131</span></a>    <span class="n">F1_SCORE</span><span class="p">,</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos">132</span></a>    <span class="n">EXACT_MATCH_SCORE</span><span class="p">,</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos">133</span></a>    <span class="n">QUASI_EXACT_MATCH_SCORE</span><span class="p">,</span>
</span><span id="L-134"><a href="#L-134"><span class="linenos">134</span></a><span class="p">]</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos">135</span></a><span class="c1"># Dataset types</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos">136</span></a><span class="n">BUILT_IN_DATASET</span> <span class="o">=</span> <span class="s2">&quot;Built-in Dataset&quot;</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos">137</span></a><span class="n">CUSTOM_DATASET</span> <span class="o">=</span> <span class="s2">&quot;Custom Dataset&quot;</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos">138</span></a>
</span><span id="L-139"><a href="#L-139"><span class="linenos">139</span></a><span class="n">PROMPT_COLUMN_NAME</span> <span class="o">=</span> <span class="s2">&quot;prompt&quot;</span>
</span><span id="L-140"><a href="#L-140"><span class="linenos">140</span></a>
</span><span id="L-141"><a href="#L-141"><span class="linenos">141</span></a><span class="n">TOXICITY_EVAL_NAMES</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos">142</span></a>    <span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">TOXICITY</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos">143</span></a>    <span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">QA_TOXICITY</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos">144</span></a>    <span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">SUMMARIZATION_TOXICITY</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
</span><span id="L-145"><a href="#L-145"><span class="linenos">145</span></a><span class="p">]</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos">146</span></a>
</span><span id="L-147"><a href="#L-147"><span class="linenos">147</span></a><span class="c1"># Prompt stereotyping table column name</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos">148</span></a><span class="n">PROBABILITY_RATIO</span> <span class="o">=</span> <span class="s2">&quot;&lt;math&gt;&lt;box&gt;p(S&lt;sub&gt;more&lt;/sub&gt;)/p(S&lt;sub&gt;less&lt;/sub&gt;)&lt;/box&gt;&lt;/math&gt;&quot;</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos">149</span></a><span class="n">IS_BIASED</span> <span class="o">=</span> <span class="s2">&quot;is_biased&quot;</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos">150</span></a>
</span><span id="L-151"><a href="#L-151"><span class="linenos">151</span></a><span class="c1"># Toxicity detector names</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos">152</span></a><span class="n">TOXIGEN_NAME</span> <span class="o">=</span> <span class="s2">&quot;Toxigen-roberta&quot;</span>
</span><span id="L-153"><a href="#L-153"><span class="linenos">153</span></a><span class="n">DETOXIFY_NAME</span> <span class="o">=</span> <span class="s2">&quot;UnitaryAI Detoxify-unbiased&quot;</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos">154</span></a><span class="n">TOXIGEN_URI</span> <span class="o">=</span> <span class="s2">&quot;https://github.com/microsoft/TOXIGEN&quot;</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos">155</span></a><span class="n">DETOXIFY_URI</span> <span class="o">=</span> <span class="s2">&quot;https://github.com/unitaryai/detoxify&quot;</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos">156</span></a><span class="c1"># Example table descriptions</span>
</span><span id="L-157"><a href="#L-157"><span class="linenos">157</span></a><span class="n">TABLE_DESCRIPTION</span> <span class="o">=</span> <span class="s2">&quot;Below are a few examples of the highest and lowest-scoring examples across all categories. Some text may be truncated due to length constraints. To view the full prompts, please go to the S3 job output location that you specified when configuring the job. &quot;</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos">158</span></a><span class="n">WER_TABLE_DESCRIPTION</span> <span class="o">=</span> <span class="s2">&quot;Below are a few examples of the highest and lowest-scoring examples across all categories. The lower the word error rate, the better the model performs. Some text may be truncated due to length constraints. To view the full prompts, please go to the S3 job output location that you specified when configuring the job.&quot;</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos">159</span></a><span class="n">STEREOTYPING_TABLE_DESCRIPTION</span> <span class="o">=</span> <span class="s2">&quot;**Probability ratio:** </span><span class="se">\n\n</span><span class="s2">For each sentence pair, we additionally report by how much the model stereotypes. This is given by the ratio &lt;math&gt;&lt;box&gt;p(S&lt;sub&gt;more&lt;/sub&gt;)/p(S&lt;sub&gt;less&lt;/sub&gt;)&lt;/box&gt;&lt;/math&gt;. This ratio will be between 0 and ∞. A ratio of 200, for example, means that the model assigns 200x the probability to the more stereotypical sentence than the less stereotypical sentence.&quot;</span>
</span><span id="L-160"><a href="#L-160"><span class="linenos">160</span></a><span class="n">FACTUAL_KNOWLEDGE_TABLE_DESCRIPTION</span> <span class="o">=</span> <span class="s2">&quot;Below are a few examples of correct and incorrect model responses. Some text may be truncated due to length constraints. To view the full prompts, please go to the S3 job output location that you specified when configuring the job. &quot;</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos">161</span></a>
</span><span id="L-162"><a href="#L-162"><span class="linenos">162</span></a><span class="c1"># Score descriptions for the report</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos">163</span></a><span class="n">SCORE_DESCRIPTIONS</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="L-164"><a href="#L-164"><span class="linenos">164</span></a>    <span class="c1"># Factual knowledge</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos">165</span></a>    <span class="n">FACTUAL_KNOWLEDGE</span><span class="p">:</span> <span class="s2">&quot;A factual knowledge score is a value between 0 and 1, indicating the percentage of correctly retrieved real-world facts across different knowledge categories.&quot;</span><span class="p">,</span>
</span><span id="L-166"><a href="#L-166"><span class="linenos">166</span></a>    <span class="c1"># Prompt stereotyping</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos">167</span></a>    <span class="n">PROMPT_STEREOTYPING</span><span class="p">:</span> <span class="s2">&quot;The is_biased prompt stereotyping score measures whether the model assigns higher probability to the more stereotypical sentence than the less stereotypical one, i.e., &lt;math&gt;&lt;box&gt;p(S&lt;sub&gt;more&lt;/sub&gt;) &gt; p(S&lt;sub&gt;less&lt;/sub&gt;)&lt;/box&gt;&lt;/math&gt;. It is a value between 0 and 1, where 1 indicates that the model always prefers the more stereotypical sentence while 0 means that it never prefers the more stereotypical sentence. An unbiased model prefers more and less stereotypical sentences at equal rates, corresponding to a score of 0.5&quot;</span><span class="p">,</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos">168</span></a>    <span class="n">LOG_PROBABILITY_DIFFERENCE</span><span class="p">:</span> <span class="s2">&quot;For each sentence pair, we report the log probability difference, a value ranging -&amp;#8734; to &amp;#8734;, indicating how much the model stereotypes. &quot;</span><span class="p">,</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos">169</span></a>    <span class="c1"># QA accuracy</span>
</span><span id="L-170"><a href="#L-170"><span class="linenos">170</span></a>    <span class="n">F1_SCORE</span><span class="p">:</span> <span class="s2">&quot;Numerical score between 0 (worst) and 1 (best). F1-score is the harmonic mean of precision and recall. It is computed as follows:  precision = true positives / (true positives + false positives) and recall = true positives / (true positives + false negatives). Then F1 = 2 (precision * recall)/(precision + recall) .&quot;</span><span class="p">,</span>
</span><span id="L-171"><a href="#L-171"><span class="linenos">171</span></a>    <span class="n">EXACT_MATCH_SCORE</span><span class="p">:</span> <span class="s2">&quot;An exact match score is a binary score where 1 indicates the model output and answer match exactly and 0 indicates otherwise.&quot;</span><span class="p">,</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos">172</span></a>    <span class="n">QUASI_EXACT_MATCH_SCORE</span><span class="p">:</span> <span class="s2">&quot;Similar as above, but both model output and answer are normalised first by removing any articles and punctuation. E.g., 1 also for predicted answers “Antarctica.” or “the Antarctica” .&quot;</span><span class="p">,</span>
</span><span id="L-173"><a href="#L-173"><span class="linenos">173</span></a>    <span class="c1"># Summarization accuracy</span>
</span><span id="L-174"><a href="#L-174"><span class="linenos">174</span></a>    <span class="n">ROUGE_SCORE</span><span class="p">:</span> <span class="s2">&quot;A ROUGE-N score computes the N-gram (sequences of n words) word overlaps between the reference and model summary, with the value ranging between 0 (no match) to 1 (perfect match).&quot;</span><span class="p">,</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos">175</span></a>    <span class="n">METEOR_SCORE</span><span class="p">:</span> <span class="s2">&quot;Meteor is similar to ROUGE-N, but it also accounts for rephrasing by using traditional NLP techniques such as stemming (e.g. matching “singing” to “sing”,“sings” etc.) and synonym lists.&quot;</span><span class="p">,</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos">176</span></a>    <span class="n">BERT_SCORE</span><span class="p">:</span> <span class="s2">&quot;BERTScore uses a second ML model (from the BERT family) to compute sentence embeddings and compare their similarity.&quot;</span><span class="p">,</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos">177</span></a>    <span class="c1"># Classification accuracy</span>
</span><span id="L-178"><a href="#L-178"><span class="linenos">178</span></a>    <span class="n">CLASSIFICATION_ACCURACY_SCORE</span><span class="p">:</span> <span class="s2">&quot;The classification accuracy is `predicted_label == true_label`, reported as the mean accuracy over all datapoints.&quot;</span><span class="p">,</span>
</span><span id="L-179"><a href="#L-179"><span class="linenos">179</span></a>    <span class="n">PRECISION_SCORE</span><span class="p">:</span> <span class="s2">&quot;The precision score is computed as `true positives / (true positives + false positives)`. &quot;</span><span class="p">,</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos">180</span></a>    <span class="n">RECALL_SCORE</span><span class="p">:</span> <span class="s2">&quot;The recall score is computed as `true positives / (true positives + false negatives)`&quot;</span><span class="p">,</span>
</span><span id="L-181"><a href="#L-181"><span class="linenos">181</span></a>    <span class="n">BALANCED_ACCURACY_SCORE</span><span class="p">:</span> <span class="s2">&quot;The balanced accuracy score is the same as accuracy in the binary case, otherwise averaged recall per class.&quot;</span><span class="p">,</span>
</span><span id="L-182"><a href="#L-182"><span class="linenos">182</span></a>    <span class="c1"># General semantic robustness</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos">183</span></a>    <span class="n">WER_SCORE</span><span class="p">:</span> <span class="s2">&quot;Word error rate (WER) is a value between 0 and 1, and measures the difference between the model output on the unperturbed input and the output(s) on one or more perturbed versions of the same input. For more details on how word error rate is computed, see the [HuggingFace Article on Word Error Rate](https://huggingface.co/spaces/evaluate-metric/wer).&quot;</span><span class="p">,</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos">184</span></a>    <span class="c1"># Summarization semantic robustness</span>
</span><span id="L-185"><a href="#L-185"><span class="linenos">185</span></a>    <span class="n">DELTA_ROUGE_SCORE</span><span class="p">:</span> <span class="s2">&quot;The performance change of the ROUGE-N score is measured.&quot;</span><span class="p">,</span>
</span><span id="L-186"><a href="#L-186"><span class="linenos">186</span></a>    <span class="n">DELTA_METEOR_SCORE</span><span class="p">:</span> <span class="s2">&quot;The performance change of the METEOR score is measured.&quot;</span><span class="p">,</span>
</span><span id="L-187"><a href="#L-187"><span class="linenos">187</span></a>    <span class="n">DELTA_BERT_SCORE</span><span class="p">:</span> <span class="s2">&quot;The performance change of the BERTscore is measured.&quot;</span><span class="p">,</span>
</span><span id="L-188"><a href="#L-188"><span class="linenos">188</span></a>    <span class="c1"># QA semantic robustness</span>
</span><span id="L-189"><a href="#L-189"><span class="linenos">189</span></a>    <span class="n">DELTA_EXACT_MATCH_SCORE</span><span class="p">:</span> <span class="s2">&quot;The performance change of the Exact Match score is measured.&quot;</span><span class="p">,</span>
</span><span id="L-190"><a href="#L-190"><span class="linenos">190</span></a>    <span class="n">DELTA_QUASI_EXACT_MATCH_SCORE</span><span class="p">:</span> <span class="s2">&quot;The performance change of the Quasi Exact Match score is measured.&quot;</span><span class="p">,</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos">191</span></a>    <span class="n">DELTA_F1_SCORE</span><span class="p">:</span> <span class="s2">&quot;The performance change of the F1 over Words score is measured.&quot;</span><span class="p">,</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos">192</span></a>    <span class="c1"># Classification semantic robustness</span>
</span><span id="L-193"><a href="#L-193"><span class="linenos">193</span></a>    <span class="n">DELTA_CLASSIFICATION_ACCURACY_SCORE</span><span class="p">:</span> <span class="s2">&quot;The score is the binary indicator on whether or not the model answer is correct.&quot;</span><span class="p">,</span>
</span><span id="L-194"><a href="#L-194"><span class="linenos">194</span></a>    <span class="c1"># Toxicity</span>
</span><span id="L-195"><a href="#L-195"><span class="linenos">195</span></a>    <span class="n">TOXIGEN_SCORE_NAME</span><span class="p">:</span> <span class="s2">&quot;A binary score where 0 indicates no toxicity detected and 1 indicates toxicity detected.&quot;</span><span class="p">,</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos">196</span></a>    <span class="n">DETOXIFY_SCORE_TOXICITY</span><span class="p">:</span> <span class="s2">&quot;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **toxicity**&quot;</span><span class="p">,</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos">197</span></a>    <span class="n">DETOXIFY_SCORE_SEVERE_TOXICITY</span><span class="p">:</span> <span class="s2">&quot;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **severe_toxicity**&quot;</span><span class="p">,</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos">198</span></a>    <span class="n">DETOXIFY_SCORE_OBSCENE</span><span class="p">:</span> <span class="s2">&quot;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **obscene**&quot;</span><span class="p">,</span>
</span><span id="L-199"><a href="#L-199"><span class="linenos">199</span></a>    <span class="n">DETOXIFY_SCORE_IDENTITY_ATTACK</span><span class="p">:</span> <span class="s2">&quot;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class **identity_attack**&quot;</span><span class="p">,</span>
</span><span id="L-200"><a href="#L-200"><span class="linenos">200</span></a>    <span class="n">DETOXIFY_SCORE_INSULT</span><span class="p">:</span> <span class="s2">&quot;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **insult**&quot;</span><span class="p">,</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos">201</span></a>    <span class="n">DETOXIFY_SCORE_THREAT</span><span class="p">:</span> <span class="s2">&quot;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **threat**&quot;</span><span class="p">,</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos">202</span></a>    <span class="n">DETOXIFY_SCORE_SEXUAL_EXPLICIT</span><span class="p">:</span> <span class="s2">&quot;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **sexual_explicit**&quot;</span><span class="p">,</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos">203</span></a><span class="p">}</span>
</span><span id="L-204"><a href="#L-204"><span class="linenos">204</span></a>
</span><span id="L-205"><a href="#L-205"><span class="linenos">205</span></a>
</span><span id="L-206"><a href="#L-206"><span class="linenos">206</span></a><span class="k">class</span> <span class="nc">DatasetDetails</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
</span><span id="L-207"><a href="#L-207"><span class="linenos">207</span></a>    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos">208</span></a>    <span class="n">url</span><span class="p">:</span> <span class="nb">str</span>
</span><span id="L-209"><a href="#L-209"><span class="linenos">209</span></a>    <span class="n">description</span><span class="p">:</span> <span class="nb">str</span>
</span><span id="L-210"><a href="#L-210"><span class="linenos">210</span></a>    <span class="n">size</span><span class="p">:</span> <span class="nb">int</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos">211</span></a>
</span><span id="L-212"><a href="#L-212"><span class="linenos">212</span></a>
</span><span id="L-213"><a href="#L-213"><span class="linenos">213</span></a><span class="c1"># Dataset details with the formatted names, URLs, descriptions and size</span>
</span><span id="L-214"><a href="#L-214"><span class="linenos">214</span></a><span class="n">DATASET_DETAILS</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos">215</span></a>    <span class="n">TREX</span><span class="p">:</span> <span class="n">DatasetDetails</span><span class="p">(</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos">216</span></a>        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;T-REx&quot;</span><span class="p">,</span>
</span><span id="L-217"><a href="#L-217"><span class="linenos">217</span></a>        <span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://hadyelsahar.github.io/t-rex/&quot;</span><span class="p">,</span>
</span><span id="L-218"><a href="#L-218"><span class="linenos">218</span></a>        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;A dataset which consists of knowledge triplets extracted from Wikipedia. The triplets take the form (subject, predicate, object), for instance, (Berlin, capital of, Germany) or (Tata Motors, subsidiary of, Tata Group). &quot;</span><span class="p">,</span>
</span><span id="L-219"><a href="#L-219"><span class="linenos">219</span></a>        <span class="n">size</span><span class="o">=</span><span class="mi">32260</span><span class="p">,</span>
</span><span id="L-220"><a href="#L-220"><span class="linenos">220</span></a>    <span class="p">),</span>
</span><span id="L-221"><a href="#L-221"><span class="linenos">221</span></a>    <span class="n">BOOLQ</span><span class="p">:</span> <span class="n">DatasetDetails</span><span class="p">(</span>
</span><span id="L-222"><a href="#L-222"><span class="linenos">222</span></a>        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;BoolQ&quot;</span><span class="p">,</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos">223</span></a>        <span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://github.com/google-research-datasets/boolean-questions&quot;</span><span class="p">,</span>
</span><span id="L-224"><a href="#L-224"><span class="linenos">224</span></a>        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;A dataset consisting of question-passage-answer triplets. The question can be answered with yes/no, and the answer is contained in the passage. The questions are provided anonymously and unsolicited by users of the Google search engine, and afterwards paired with a paragraph from a Wikipedia article containing the answer.&quot;</span><span class="p">,</span>
</span><span id="L-225"><a href="#L-225"><span class="linenos">225</span></a>        <span class="n">size</span><span class="o">=</span><span class="mi">12697</span><span class="p">,</span>
</span><span id="L-226"><a href="#L-226"><span class="linenos">226</span></a>    <span class="p">),</span>
</span><span id="L-227"><a href="#L-227"><span class="linenos">227</span></a>    <span class="n">TRIVIA_QA</span><span class="p">:</span> <span class="n">DatasetDetails</span><span class="p">(</span>
</span><span id="L-228"><a href="#L-228"><span class="linenos">228</span></a>        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;TriviaQA&quot;</span><span class="p">,</span>
</span><span id="L-229"><a href="#L-229"><span class="linenos">229</span></a>        <span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://nlp.cs.washington.edu/triviaqa/&quot;</span><span class="p">,</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos">230</span></a>        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;A dataset consisting of 95K question-answer pairs with with on average six supporting evidence documents per question, leading to ~650K question-passage-answer triplets. The questions are authored by trivia enthusiasts and the evidence documents are independently gathered. &quot;</span><span class="p">,</span>
</span><span id="L-231"><a href="#L-231"><span class="linenos">231</span></a>        <span class="n">size</span><span class="o">=</span><span class="mi">156328</span><span class="p">,</span>
</span><span id="L-232"><a href="#L-232"><span class="linenos">232</span></a>    <span class="p">),</span>
</span><span id="L-233"><a href="#L-233"><span class="linenos">233</span></a>    <span class="n">NATURAL_QUESTIONS</span><span class="p">:</span> <span class="n">DatasetDetails</span><span class="p">(</span>
</span><span id="L-234"><a href="#L-234"><span class="linenos">234</span></a>        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Natural Questions&quot;</span><span class="p">,</span>
</span><span id="L-235"><a href="#L-235"><span class="linenos">235</span></a>        <span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://github.com/google-research-datasets/natural-questions&quot;</span><span class="p">,</span>
</span><span id="L-236"><a href="#L-236"><span class="linenos">236</span></a>        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;A dataset consisting of ~320K question-passage-answer triplets. The questions are factual naturally-occurring questions. The passages are extracts from wikipedia articles (referred to as “long answers” in the original dataset). As before, providing the passage is optional depending on whether the open-book or closed-book case should be evaluated.&quot;</span><span class="p">,</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos">237</span></a>        <span class="n">size</span><span class="o">=</span><span class="mi">4289</span><span class="p">,</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos">238</span></a>    <span class="p">),</span>
</span><span id="L-239"><a href="#L-239"><span class="linenos">239</span></a>    <span class="n">CROWS_PAIRS</span><span class="p">:</span> <span class="n">DatasetDetails</span><span class="p">(</span>
</span><span id="L-240"><a href="#L-240"><span class="linenos">240</span></a>        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;CrowS-Pairs&quot;</span><span class="p">,</span>
</span><span id="L-241"><a href="#L-241"><span class="linenos">241</span></a>        <span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://github.com/nyu-mll/crows-pairs&quot;</span><span class="p">,</span>
</span><span id="L-242"><a href="#L-242"><span class="linenos">242</span></a>        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;This dataset provides crowdsourced sentence pairs for the different categories along which stereotyping is to be measured.&quot;</span><span class="p">,</span>
</span><span id="L-243"><a href="#L-243"><span class="linenos">243</span></a>        <span class="n">size</span><span class="o">=</span><span class="mi">1508</span><span class="p">,</span>
</span><span id="L-244"><a href="#L-244"><span class="linenos">244</span></a>    <span class="p">),</span>
</span><span id="L-245"><a href="#L-245"><span class="linenos">245</span></a>    <span class="n">XSUM</span><span class="p">:</span> <span class="n">DatasetDetails</span><span class="p">(</span>
</span><span id="L-246"><a href="#L-246"><span class="linenos">246</span></a>        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;XSUM&quot;</span><span class="p">,</span>
</span><span id="L-247"><a href="#L-247"><span class="linenos">247</span></a>        <span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset&quot;</span><span class="p">,</span>
</span><span id="L-248"><a href="#L-248"><span class="linenos">248</span></a>        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;A dataset consisting of newspaper articles from the BBC and their reference summaries. The reference summaries consist of a single sentence: the boldfaced sentence at the begininning of each BBC article, provided by article’s authors.&quot;</span><span class="p">,</span>
</span><span id="L-249"><a href="#L-249"><span class="linenos">249</span></a>        <span class="n">size</span><span class="o">=</span><span class="mi">204045</span><span class="p">,</span>
</span><span id="L-250"><a href="#L-250"><span class="linenos">250</span></a>    <span class="p">),</span>
</span><span id="L-251"><a href="#L-251"><span class="linenos">251</span></a>    <span class="n">WOMENS_CLOTHING_ECOMMERCE_REVIEWS</span><span class="p">:</span> <span class="n">DatasetDetails</span><span class="p">(</span>
</span><span id="L-252"><a href="#L-252"><span class="linenos">252</span></a>        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Women&#39;s E-commerce Clothing Reviews&quot;</span><span class="p">,</span>
</span><span id="L-253"><a href="#L-253"><span class="linenos">253</span></a>        <span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews&quot;</span><span class="p">,</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos">254</span></a>        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;This dataset consists of clothing reviews, both as a text and numerical scores.&quot;</span><span class="p">,</span>
</span><span id="L-255"><a href="#L-255"><span class="linenos">255</span></a>        <span class="n">size</span><span class="o">=</span><span class="mi">23486</span><span class="p">,</span>
</span><span id="L-256"><a href="#L-256"><span class="linenos">256</span></a>    <span class="p">),</span>
</span><span id="L-257"><a href="#L-257"><span class="linenos">257</span></a>    <span class="n">BOLD</span><span class="p">:</span> <span class="n">DatasetDetails</span><span class="p">(</span>
</span><span id="L-258"><a href="#L-258"><span class="linenos">258</span></a>        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;BOLD&quot;</span><span class="p">,</span>
</span><span id="L-259"><a href="#L-259"><span class="linenos">259</span></a>        <span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://github.com/amazon-science/bold&quot;</span><span class="p">,</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos">260</span></a>        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;A large-scale dataset that consists of English prompts aimed at testing bias and toxicity generation across five domains: profession, gender, race, religion, and political ideology.&quot;</span><span class="p">,</span>
</span><span id="L-261"><a href="#L-261"><span class="linenos">261</span></a>        <span class="n">size</span><span class="o">=</span><span class="mi">23679</span><span class="p">,</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos">262</span></a>    <span class="p">),</span>
</span><span id="L-263"><a href="#L-263"><span class="linenos">263</span></a>    <span class="n">WIKITEXT2</span><span class="p">:</span> <span class="n">DatasetDetails</span><span class="p">(</span>
</span><span id="L-264"><a href="#L-264"><span class="linenos">264</span></a>        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;WikiText2&quot;</span><span class="p">,</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos">265</span></a>        <span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://huggingface.co/datasets/wikitext&quot;</span><span class="p">,</span>
</span><span id="L-266"><a href="#L-266"><span class="linenos">266</span></a>        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;A dataset which consists of Good and Featured articles from Wikipedia. To create prompts, we broke each article down into sentences and extracted first 6 tokens from each sentence as the prompt.&quot;</span><span class="p">,</span>
</span><span id="L-267"><a href="#L-267"><span class="linenos">267</span></a>        <span class="n">size</span><span class="o">=</span><span class="mi">86007</span><span class="p">,</span>
</span><span id="L-268"><a href="#L-268"><span class="linenos">268</span></a>    <span class="p">),</span>
</span><span id="L-269"><a href="#L-269"><span class="linenos">269</span></a>    <span class="n">REAL_TOXICITY_PROMPTS</span><span class="p">:</span> <span class="n">DatasetDetails</span><span class="p">(</span>
</span><span id="L-270"><a href="#L-270"><span class="linenos">270</span></a>        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Real Toxicity Prompts&quot;</span><span class="p">,</span>
</span><span id="L-271"><a href="#L-271"><span class="linenos">271</span></a>        <span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://github.com/allenai/real-toxicity-prompts&quot;</span><span class="p">,</span>
</span><span id="L-272"><a href="#L-272"><span class="linenos">272</span></a>        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;A dataset of truncated sentence snippets from the web. &quot;</span><span class="p">,</span>
</span><span id="L-273"><a href="#L-273"><span class="linenos">273</span></a>        <span class="n">size</span><span class="o">=</span><span class="mi">98243</span><span class="p">,</span>
</span><span id="L-274"><a href="#L-274"><span class="linenos">274</span></a>    <span class="p">),</span>
</span><span id="L-275"><a href="#L-275"><span class="linenos">275</span></a>    <span class="n">REAL_TOXICITY_PROMPTS_CHALLENGING</span><span class="p">:</span> <span class="n">DatasetDetails</span><span class="p">(</span>
</span><span id="L-276"><a href="#L-276"><span class="linenos">276</span></a>        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Real Toxicity Prompts Challenging&quot;</span><span class="p">,</span>
</span><span id="L-277"><a href="#L-277"><span class="linenos">277</span></a>        <span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://github.com/allenai/real-toxicity-prompts&quot;</span><span class="p">,</span>
</span><span id="L-278"><a href="#L-278"><span class="linenos">278</span></a>        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;A dataset of truncated sentence snippets from the web. Prompts marked as “challenging” have been found by the authors to consistently lead to generation of toxic continuation by tested models (i.e., GPT-1, GPT-2, GPT-3, CTRL, CTRL-WIKI).&quot;</span><span class="p">,</span>
</span><span id="L-279"><a href="#L-279"><span class="linenos">279</span></a>        <span class="n">size</span><span class="o">=</span><span class="mi">1199</span><span class="p">,</span>
</span><span id="L-280"><a href="#L-280"><span class="linenos">280</span></a>    <span class="p">),</span>
</span><span id="L-281"><a href="#L-281"><span class="linenos">281</span></a>    <span class="n">GIGAWORD</span><span class="p">:</span> <span class="n">DatasetDetails</span><span class="p">(</span>
</span><span id="L-282"><a href="#L-282"><span class="linenos">282</span></a>        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Gigaword&quot;</span><span class="p">,</span>
</span><span id="L-283"><a href="#L-283"><span class="linenos">283</span></a>        <span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://huggingface.co/datasets/gigaword&quot;</span><span class="p">,</span>
</span><span id="L-284"><a href="#L-284"><span class="linenos">284</span></a>        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;A dataset with around 4 million news articles with their summaries. We use the “validation set”, which includes 190k entries.&quot;</span><span class="p">,</span>
</span><span id="L-285"><a href="#L-285"><span class="linenos">285</span></a>        <span class="n">size</span><span class="o">=</span><span class="mi">189651</span><span class="p">,</span>
</span><span id="L-286"><a href="#L-286"><span class="linenos">286</span></a>    <span class="p">),</span>
</span><span id="L-287"><a href="#L-287"><span class="linenos">287</span></a>    <span class="n">GOV_REPORT</span><span class="p">:</span> <span class="n">DatasetDetails</span><span class="p">(</span>
</span><span id="L-288"><a href="#L-288"><span class="linenos">288</span></a>        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Government Report&quot;</span><span class="p">,</span>
</span><span id="L-289"><a href="#L-289"><span class="linenos">289</span></a>        <span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://gov-report-data.github.io/&quot;</span><span class="p">,</span>
</span><span id="L-290"><a href="#L-290"><span class="linenos">290</span></a>        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;A dataset including a long-form summarization benchmark. It contains significantly longer documents (9.4k words) and summaries (553 words) than most existing datasets.&quot;</span><span class="p">,</span>
</span><span id="L-291"><a href="#L-291"><span class="linenos">291</span></a>        <span class="n">size</span><span class="o">=</span><span class="mi">7238</span><span class="p">,</span>
</span><span id="L-292"><a href="#L-292"><span class="linenos">292</span></a>    <span class="p">),</span>
</span><span id="L-293"><a href="#L-293"><span class="linenos">293</span></a><span class="p">}</span>
</span><span id="L-294"><a href="#L-294"><span class="linenos">294</span></a>
</span><span id="L-295"><a href="#L-295"><span class="linenos">295</span></a><span class="n">TREX_DESCRIPTION_EXAMPLES</span> <span class="o">=</span> <span class="s2">&quot;We convert these predicates to prompts, e.g., Berlin is the capital of ___ (expected answer: Germany) and Tata Motors is a subsidiary of ___ (expected answer: Tata Group).&quot;</span>
</span><span id="L-296"><a href="#L-296"><span class="linenos">296</span></a>
</span><span id="L-297"><a href="#L-297"><span class="linenos">297</span></a><span class="n">CROWS_PAIRS_DISCLAIMER</span> <span class="o">=</span> <span class="s2">&quot;**Disclaimer**: 1) The crowdsourced CrowS dataset is noisy. While it gives a good indication of overall model performance, individual pairs may be invalid. 2) CrowS measures U.S.-typical stereotypes. Specifically, the bias categories are taken from the US Equal Employment Opportunities Commission’s list of protected categories and the sentence pairs are produced by Amazon Mechanical Turk workers in the United States.&quot;</span>
</span></pre></div>


            </section>
                <section id="CENTER">
                    <div class="attr variable">
            <span class="name">CENTER</span>        =
<span class="default_value">&#39;center&#39;</span>

        
    </div>
    <a class="headerlink" href="#CENTER"></a>
    
    

                </section>
                <section id="LEFT">
                    <div class="attr variable">
            <span class="name">LEFT</span>        =
<span class="default_value">&#39;left&#39;</span>

        
    </div>
    <a class="headerlink" href="#LEFT"></a>
    
    

                </section>
                <section id="RIGHT">
                    <div class="attr variable">
            <span class="name">RIGHT</span>        =
<span class="default_value">&#39;right&#39;</span>

        
    </div>
    <a class="headerlink" href="#RIGHT"></a>
    
    

                </section>
                <section id="ListType">
                            <input id="ListType-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">ListType</span><wbr>(<span class="base">enum.Enum</span>):

                <label class="view-source-button" for="ListType-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ListType"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ListType-62"><a href="#ListType-62"><span class="linenos">62</span></a><span class="k">class</span> <span class="nc">ListType</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
</span><span id="ListType-63"><a href="#ListType-63"><span class="linenos">63</span></a>    <span class="n">BULLETED</span> <span class="o">=</span> <span class="s2">&quot;bulleted&quot;</span>
</span><span id="ListType-64"><a href="#ListType-64"><span class="linenos">64</span></a>    <span class="n">NUMBERED</span> <span class="o">=</span> <span class="s2">&quot;numbered&quot;</span>
</span></pre></div>


            <div class="docstring"><p>An enumeration.</p>
</div>


                            <div id="ListType.BULLETED" class="classattr">
                                <div class="attr variable">
            <span class="name">BULLETED</span>        =
<span class="default_value">&lt;<a href="#ListType.BULLETED">ListType.BULLETED</a>: &#39;bulleted&#39;&gt;</span>

        
    </div>
    <a class="headerlink" href="#ListType.BULLETED"></a>
    
    

                            </div>
                            <div id="ListType.NUMBERED" class="classattr">
                                <div class="attr variable">
            <span class="name">NUMBERED</span>        =
<span class="default_value">&lt;<a href="#ListType.NUMBERED">ListType.NUMBERED</a>: &#39;numbered&#39;&gt;</span>

        
    </div>
    <a class="headerlink" href="#ListType.NUMBERED"></a>
    
    

                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>enum.Enum</dt>
                                <dd id="ListType.name" class="variable">name</dd>
                <dd id="ListType.value" class="variable">value</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="SINGLE_NEWLINE">
                    <div class="attr variable">
            <span class="name">SINGLE_NEWLINE</span>        =
<span class="default_value">&#39;  \n&#39;</span>

        
    </div>
    <a class="headerlink" href="#SINGLE_NEWLINE"></a>
    
    

                </section>
                <section id="DOUBLE_NEWLINE">
                    <div class="attr variable">
            <span class="name">DOUBLE_NEWLINE</span>        =
<span class="default_value">&#39;  \n\n&#39;</span>

        
    </div>
    <a class="headerlink" href="#DOUBLE_NEWLINE"></a>
    
    

                </section>
                <section id="NUM_SAMPLES_TO_DISPLAY_IN_TABLE">
                    <div class="attr variable">
            <span class="name">NUM_SAMPLES_TO_DISPLAY_IN_TABLE</span>        =
<span class="default_value">5</span>

        
    </div>
    <a class="headerlink" href="#NUM_SAMPLES_TO_DISPLAY_IN_TABLE"></a>
    
    

                </section>
                <section id="CATEGORY_BAR_COLOR">
                    <div class="attr variable">
            <span class="name">CATEGORY_BAR_COLOR</span>        =
<span class="default_value">&#39;steelblue&#39;</span>

        
    </div>
    <a class="headerlink" href="#CATEGORY_BAR_COLOR"></a>
    
    

                </section>
                <section id="OVERALL_BAR_COLOR">
                    <div class="attr variable">
            <span class="name">OVERALL_BAR_COLOR</span>        =
<span class="default_value">&#39;coral&#39;</span>

        
    </div>
    <a class="headerlink" href="#OVERALL_BAR_COLOR"></a>
    
    

                </section>
                <section id="MAX_CHAR">
                    <div class="attr variable">
            <span class="name">MAX_CHAR</span>        =
<span class="default_value">200</span>

        
    </div>
    <a class="headerlink" href="#MAX_CHAR"></a>
    
    

                </section>
                <section id="MARKDOWN_EXTENSIONS">
                    <div class="attr variable">
            <span class="name">MARKDOWN_EXTENSIONS</span>        =
<span class="default_value">[&#39;tables&#39;, &#39;md_in_html&#39;]</span>

        
    </div>
    <a class="headerlink" href="#MARKDOWN_EXTENSIONS"></a>
    
    

                </section>
                <section id="DATASET_SCORE_LABEL">
                    <div class="attr variable">
            <span class="name">DATASET_SCORE_LABEL</span>        =
<span class="default_value">&#39;Overall&#39;</span>

        
    </div>
    <a class="headerlink" href="#DATASET_SCORE_LABEL"></a>
    
    

                </section>
                <section id="AGGREGATE_ONLY_SCORES">
                    <div class="attr variable">
            <span class="name">AGGREGATE_ONLY_SCORES</span>        =
<span class="default_value">[&#39;balanced_accuracy_score&#39;, &#39;precision_score&#39;, &#39;recall_score&#39;]</span>

        
    </div>
    <a class="headerlink" href="#AGGREGATE_ONLY_SCORES"></a>
    
    

                </section>
                <section id="GENERAL_STRING_REPLACEMENTS">
                    <div class="attr variable">
            <span class="name">GENERAL_STRING_REPLACEMENTS</span><span class="annotation">: List[Tuple[str, str]]</span>        =
<span class="default_value">[(&#39;qa&#39;, &#39;Q&amp;A&#39;), (&#39;f1&#39;, &#39;F1&#39;)]</span>

        
    </div>
    <a class="headerlink" href="#GENERAL_STRING_REPLACEMENTS"></a>
    
    

                </section>
                <section id="SCORE_STRING_REPLACEMENTS">
                    <div class="attr variable">
            <span class="name">SCORE_STRING_REPLACEMENTS</span><span class="annotation">: List[Tuple[str, str]]</span>        =
<input id="SCORE_STRING_REPLACEMENTS-view-value" class="view-value-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
            <label class="view-value-button pdoc-button" for="SCORE_STRING_REPLACEMENTS-view-value"></label><span class="default_value">[(&#39;prompt stereotyping&#39;, &#39;is_biased&#39;), (&#39;meteor&#39;, &#39;METEOR&#39;), (&#39;bertscore&#39;, &#39;BERTScore&#39;), (&#39;rouge&#39;, &#39;ROUGE&#39;), (&#39;F1 score&#39;, &#39;F1 over words&#39;), (&#39;obscene&#39;, &#39;Obscenity&#39;), (&#39;sexual explicit&#39;, &#39;Sexual Explicitness&#39;)]</span>

        
    </div>
    <a class="headerlink" href="#SCORE_STRING_REPLACEMENTS"></a>
    
    

                </section>
                <section id="EVAL_NAME_STRING_REPLACEMENTS">
                    <div class="attr variable">
            <span class="name">EVAL_NAME_STRING_REPLACEMENTS</span><span class="annotation">: List[Tuple[str, str]]</span>        =
<input id="EVAL_NAME_STRING_REPLACEMENTS-view-value" class="view-value-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
            <label class="view-value-button pdoc-button" for="EVAL_NAME_STRING_REPLACEMENTS-view-value"></label><span class="default_value">[(&#39;qa_accuracy&#39;, &#39;accuracy&#39;), (&#39;summarization_accuracy&#39;, &#39;accuracy&#39;), (&#39;classification_accuracy&#39;, &#39;accuracy&#39;), (&#39;general_semantic_robustness&#39;, &#39;semantic_robustness&#39;), (&#39;accuracy_semantic_robustness&#39;, &#39;semantic_robustness&#39;), (&#39;qa_accuracy&#39;, &#39;toxicity&#39;), (&#39;summarization_toxicity&#39;, &#39;toxicity&#39;), (&#39;classification_accuracy&#39;, &#39;toxicity&#39;)]</span>

        
    </div>
    <a class="headerlink" href="#EVAL_NAME_STRING_REPLACEMENTS"></a>
    
    

                </section>
                <section id="PLOT_TITLE_STRING_REPLACEMENTS">
                    <div class="attr variable">
            <span class="name">PLOT_TITLE_STRING_REPLACEMENTS</span><span class="annotation">: List[Tuple[str, str]]</span>        =
<span class="default_value">[(&#39;prompt_stereotyping&#39;, &#39;is_biased score&#39;)]</span>

        
    </div>
    <a class="headerlink" href="#PLOT_TITLE_STRING_REPLACEMENTS"></a>
    
    

                </section>
                <section id="COLUMN_NAME_STRING_REPLACEMENTS">
                    <div class="attr variable">
            <span class="name">COLUMN_NAME_STRING_REPLACEMENTS</span><span class="annotation">: List[Tuple[str, str]]</span>        =
<input id="COLUMN_NAME_STRING_REPLACEMENTS-view-value" class="view-value-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
            <label class="view-value-button pdoc-button" for="COLUMN_NAME_STRING_REPLACEMENTS-view-value"></label><span class="default_value">[(&#39;sent_more&#39;, &#39;s_more&#39;), (&#39;s_more_input&#39;, &#39;&lt;math&gt;S&lt;sub&gt;more&lt;/sub&gt;&lt;/math&gt;&#39;), (&#39;sent_less&#39;, &#39;s_less&#39;), (&#39;s_less_input&#39;, &#39;&lt;math&gt;S&lt;sub&gt;less&lt;/sub&gt;&lt;/math&gt;&#39;), (&#39;prob_&#39;, &#39;probability_&#39;), (&#39;word_error_rate&#39;, &#39;Average WER&#39;), (&#39;classification_accuracy&#39;, &#39;accuracy&#39;), (&#39;f1_score&#39;, &#39;f1 over words&#39;), (&#39;meteor&#39;, &#39;METEOR&#39;), (&#39;bertscore&#39;, &#39;BERTScore&#39;), (&#39;rouge&#39;, &#39;ROUGE&#39;)]</span>

        
    </div>
    <a class="headerlink" href="#COLUMN_NAME_STRING_REPLACEMENTS"></a>
    
    

                </section>
                <section id="AVOID_REMOVE_UNDERSCORE">
                    <div class="attr variable">
            <span class="name">AVOID_REMOVE_UNDERSCORE</span>        =
<span class="default_value">[&#39;sent_more_input&#39;, &#39;sent_less_input&#39;, &#39;is_biased&#39;]</span>

        
    </div>
    <a class="headerlink" href="#AVOID_REMOVE_UNDERSCORE"></a>
    
    

                </section>
                <section id="ACCURACY_SEMANTIC_ROBUSTNESS_ALGOS">
                    <div class="attr variable">
            <span class="name">ACCURACY_SEMANTIC_ROBUSTNESS_ALGOS</span>        =
<input id="ACCURACY_SEMANTIC_ROBUSTNESS_ALGOS-view-value" class="view-value-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
            <label class="view-value-button pdoc-button" for="ACCURACY_SEMANTIC_ROBUSTNESS_ALGOS-view-value"></label><span class="default_value">[&#39;summarization_accuracy_semantic_robustness&#39;, &lt;EvalAlgorithm.QA_ACCURACY_SEMANTIC_ROBUSTNESS: &#39;qa_accuracy_semantic_robustness&#39;&gt;, &lt;EvalAlgorithm.CLASSIFICATION_ACCURACY_SEMANTIC_ROBUSTNESS: &#39;classification_accuracy_semantic_robustness&#39;&gt;]</span>

        
    </div>
    <a class="headerlink" href="#ACCURACY_SEMANTIC_ROBUSTNESS_ALGOS"></a>
    
    

                </section>
                <section id="ACCURACY_SEMANTIC_ROBUSTNESS_SCORES">
                    <div class="attr variable">
            <span class="name">ACCURACY_SEMANTIC_ROBUSTNESS_SCORES</span>        =
<input id="ACCURACY_SEMANTIC_ROBUSTNESS_SCORES-view-value" class="view-value-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
            <label class="view-value-button pdoc-button" for="ACCURACY_SEMANTIC_ROBUSTNESS_SCORES-view-value"></label><span class="default_value">[&#39;classification_accuracy_score&#39;, &#39;meteor&#39;, &#39;bertscore&#39;, &#39;rouge&#39;, &#39;f1_score&#39;, &#39;exact_match_score&#39;, &#39;quasi_exact_match_score&#39;]</span>

        
    </div>
    <a class="headerlink" href="#ACCURACY_SEMANTIC_ROBUSTNESS_SCORES"></a>
    
    

                </section>
                <section id="BUILT_IN_DATASET">
                    <div class="attr variable">
            <span class="name">BUILT_IN_DATASET</span>        =
<span class="default_value">&#39;Built-in Dataset&#39;</span>

        
    </div>
    <a class="headerlink" href="#BUILT_IN_DATASET"></a>
    
    

                </section>
                <section id="CUSTOM_DATASET">
                    <div class="attr variable">
            <span class="name">CUSTOM_DATASET</span>        =
<span class="default_value">&#39;Custom Dataset&#39;</span>

        
    </div>
    <a class="headerlink" href="#CUSTOM_DATASET"></a>
    
    

                </section>
                <section id="PROMPT_COLUMN_NAME">
                    <div class="attr variable">
            <span class="name">PROMPT_COLUMN_NAME</span>        =
<span class="default_value">&#39;prompt&#39;</span>

        
    </div>
    <a class="headerlink" href="#PROMPT_COLUMN_NAME"></a>
    
    

                </section>
                <section id="TOXICITY_EVAL_NAMES">
                    <div class="attr variable">
            <span class="name">TOXICITY_EVAL_NAMES</span>        =
<span class="default_value">[&#39;toxicity&#39;, &#39;qa_toxicity&#39;, &#39;summarization_toxicity&#39;]</span>

        
    </div>
    <a class="headerlink" href="#TOXICITY_EVAL_NAMES"></a>
    
    

                </section>
                <section id="PROBABILITY_RATIO">
                    <div class="attr variable">
            <span class="name">PROBABILITY_RATIO</span>        =
<span class="default_value">&#39;&lt;math&gt;&lt;box&gt;p(S&lt;sub&gt;more&lt;/sub&gt;)/p(S&lt;sub&gt;less&lt;/sub&gt;)&lt;/box&gt;&lt;/math&gt;&#39;</span>

        
    </div>
    <a class="headerlink" href="#PROBABILITY_RATIO"></a>
    
    

                </section>
                <section id="IS_BIASED">
                    <div class="attr variable">
            <span class="name">IS_BIASED</span>        =
<span class="default_value">&#39;is_biased&#39;</span>

        
    </div>
    <a class="headerlink" href="#IS_BIASED"></a>
    
    

                </section>
                <section id="TOXIGEN_NAME">
                    <div class="attr variable">
            <span class="name">TOXIGEN_NAME</span>        =
<span class="default_value">&#39;Toxigen-roberta&#39;</span>

        
    </div>
    <a class="headerlink" href="#TOXIGEN_NAME"></a>
    
    

                </section>
                <section id="DETOXIFY_NAME">
                    <div class="attr variable">
            <span class="name">DETOXIFY_NAME</span>        =
<span class="default_value">&#39;UnitaryAI Detoxify-unbiased&#39;</span>

        
    </div>
    <a class="headerlink" href="#DETOXIFY_NAME"></a>
    
    

                </section>
                <section id="TOXIGEN_URI">
                    <div class="attr variable">
            <span class="name">TOXIGEN_URI</span>        =
<span class="default_value">&#39;https://github.com/microsoft/TOXIGEN&#39;</span>

        
    </div>
    <a class="headerlink" href="#TOXIGEN_URI"></a>
    
    

                </section>
                <section id="DETOXIFY_URI">
                    <div class="attr variable">
            <span class="name">DETOXIFY_URI</span>        =
<span class="default_value">&#39;https://github.com/unitaryai/detoxify&#39;</span>

        
    </div>
    <a class="headerlink" href="#DETOXIFY_URI"></a>
    
    

                </section>
                <section id="TABLE_DESCRIPTION">
                    <div class="attr variable">
            <span class="name">TABLE_DESCRIPTION</span>        =
<input id="TABLE_DESCRIPTION-view-value" class="view-value-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
            <label class="view-value-button pdoc-button" for="TABLE_DESCRIPTION-view-value"></label><span class="default_value">&#39;Below are a few examples of the highest and lowest-scoring examples across all categories. Some text may be truncated due to length constraints. To view the full prompts, please go to the S3 job output location that you specified when configuring the job. &#39;</span>

        
    </div>
    <a class="headerlink" href="#TABLE_DESCRIPTION"></a>
    
    

                </section>
                <section id="WER_TABLE_DESCRIPTION">
                    <div class="attr variable">
            <span class="name">WER_TABLE_DESCRIPTION</span>        =
<input id="WER_TABLE_DESCRIPTION-view-value" class="view-value-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
            <label class="view-value-button pdoc-button" for="WER_TABLE_DESCRIPTION-view-value"></label><span class="default_value">&#39;Below are a few examples of the highest and lowest-scoring examples across all categories. The lower the word error rate, the better the model performs. Some text may be truncated due to length constraints. To view the full prompts, please go to the S3 job output location that you specified when configuring the job.&#39;</span>

        
    </div>
    <a class="headerlink" href="#WER_TABLE_DESCRIPTION"></a>
    
    

                </section>
                <section id="STEREOTYPING_TABLE_DESCRIPTION">
                    <div class="attr variable">
            <span class="name">STEREOTYPING_TABLE_DESCRIPTION</span>        =
<input id="STEREOTYPING_TABLE_DESCRIPTION-view-value" class="view-value-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
            <label class="view-value-button pdoc-button" for="STEREOTYPING_TABLE_DESCRIPTION-view-value"></label><span class="default_value">&#39;**Probability ratio:** \n\nFor each sentence pair, we additionally report by how much the model stereotypes. This is given by the ratio &lt;math&gt;&lt;box&gt;p(S&lt;sub&gt;more&lt;/sub&gt;)/p(S&lt;sub&gt;less&lt;/sub&gt;)&lt;/box&gt;&lt;/math&gt;. This ratio will be between 0 and ∞. A ratio of 200, for example, means that the model assigns 200x the probability to the more stereotypical sentence than the less stereotypical sentence.&#39;</span>

        
    </div>
    <a class="headerlink" href="#STEREOTYPING_TABLE_DESCRIPTION"></a>
    
    

                </section>
                <section id="FACTUAL_KNOWLEDGE_TABLE_DESCRIPTION">
                    <div class="attr variable">
            <span class="name">FACTUAL_KNOWLEDGE_TABLE_DESCRIPTION</span>        =
<input id="FACTUAL_KNOWLEDGE_TABLE_DESCRIPTION-view-value" class="view-value-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
            <label class="view-value-button pdoc-button" for="FACTUAL_KNOWLEDGE_TABLE_DESCRIPTION-view-value"></label><span class="default_value">&#39;Below are a few examples of correct and incorrect model responses. Some text may be truncated due to length constraints. To view the full prompts, please go to the S3 job output location that you specified when configuring the job. &#39;</span>

        
    </div>
    <a class="headerlink" href="#FACTUAL_KNOWLEDGE_TABLE_DESCRIPTION"></a>
    
    

                </section>
                <section id="SCORE_DESCRIPTIONS">
                    <div class="attr variable">
            <span class="name">SCORE_DESCRIPTIONS</span>        =
<input id="SCORE_DESCRIPTIONS-view-value" class="view-value-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
            <label class="view-value-button pdoc-button" for="SCORE_DESCRIPTIONS-view-value"></label><span class="default_value">{&#39;factual_knowledge&#39;: &#39;A factual knowledge score is a value between 0 and 1, indicating the percentage of correctly retrieved real-world facts across different knowledge categories.&#39;, &#39;prompt_stereotyping&#39;: &#39;The is_biased prompt stereotyping score measures whether the model assigns higher probability to the more stereotypical sentence than the less stereotypical one, i.e., &lt;math&gt;&lt;box&gt;p(S&lt;sub&gt;more&lt;/sub&gt;) &gt; p(S&lt;sub&gt;less&lt;/sub&gt;)&lt;/box&gt;&lt;/math&gt;. It is a value between 0 and 1, where 1 indicates that the model always prefers the more stereotypical sentence while 0 means that it never prefers the more stereotypical sentence. An unbiased model prefers more and less stereotypical sentences at equal rates, corresponding to a score of 0.5&#39;, &#39;log_probability_difference&#39;: &#39;For each sentence pair, we report the log probability difference, a value ranging -&amp;#8734; to &amp;#8734;, indicating how much the model stereotypes. &#39;, &#39;f1_score&#39;: &#39;Numerical score between 0 (worst) and 1 (best). F1-score is the harmonic mean of precision and recall. It is computed as follows:  precision = true positives / (true positives + false positives) and recall = true positives / (true positives + false negatives). Then F1 = 2 (precision * recall)/(precision + recall) .&#39;, &#39;exact_match_score&#39;: &#39;An exact match score is a binary score where 1 indicates the model output and answer match exactly and 0 indicates otherwise.&#39;, &#39;quasi_exact_match_score&#39;: &#39;Similar as above, but both model output and answer are normalised first by removing any articles and punctuation. E.g., 1 also for predicted answers “Antarctica.” or “the Antarctica” .&#39;, &#39;rouge&#39;: &#39;A ROUGE-N score computes the N-gram (sequences of n words) word overlaps between the reference and model summary, with the value ranging between 0 (no match) to 1 (perfect match).&#39;, &#39;meteor&#39;: &#39;Meteor is similar to ROUGE-N, but it also accounts for rephrasing by using traditional NLP techniques such as stemming (e.g. matching “singing” to “sing”,“sings” etc.) and synonym lists.&#39;, &#39;bertscore&#39;: &#39;BERTScore uses a second ML model (from the BERT family) to compute sentence embeddings and compare their similarity.&#39;, &#39;classification_accuracy_score&#39;: &#39;The classification accuracy is `predicted_label == true_label`, reported as the mean accuracy over all datapoints.&#39;, &#39;precision_score&#39;: &#39;The precision score is computed as `true positives / (true positives + false positives)`. &#39;, &#39;recall_score&#39;: &#39;The recall score is computed as `true positives / (true positives + false negatives)`&#39;, &#39;balanced_accuracy_score&#39;: &#39;The balanced accuracy score is the same as accuracy in the binary case, otherwise averaged recall per class.&#39;, &#39;word_error_rate&#39;: &#39;Word error rate (WER) is a value between 0 and 1, and measures the difference between the model output on the unperturbed input and the output(s) on one or more perturbed versions of the same input. For more details on how word error rate is computed, see the [HuggingFace Article on Word Error Rate](https://huggingface.co/spaces/evaluate-metric/wer).&#39;, &#39;delta_rouge&#39;: &#39;The performance change of the ROUGE-N score is measured.&#39;, &#39;delta_meteor&#39;: &#39;The performance change of the METEOR score is measured.&#39;, &#39;delta_bertscore&#39;: &#39;The performance change of the BERTscore is measured.&#39;, &#39;delta_exact_match_score&#39;: &#39;The performance change of the Exact Match score is measured.&#39;, &#39;delta_quasi_exact_match_score&#39;: &#39;The performance change of the Quasi Exact Match score is measured.&#39;, &#39;delta_f1_score&#39;: &#39;The performance change of the F1 over Words score is measured.&#39;, &#39;delta_classification_accuracy_score&#39;: &#39;The score is the binary indicator on whether or not the model answer is correct.&#39;, &#39;toxicity&#39;: &#39;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **toxicity**&#39;, &#39;severe_toxicity&#39;: &#39;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **severe_toxicity**&#39;, &#39;obscene&#39;: &#39;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **obscene**&#39;, &#39;identity_attack&#39;: &#39;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class **identity_attack**&#39;, &#39;insult&#39;: &#39;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **insult**&#39;, &#39;threat&#39;: &#39;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **threat**&#39;, &#39;sexual_explicit&#39;: &#39;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **sexual_explicit**&#39;}</span>

        
    </div>
    <a class="headerlink" href="#SCORE_DESCRIPTIONS"></a>
    
    

                </section>
                <section id="DatasetDetails">
                            <input id="DatasetDetails-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">DatasetDetails</span><wbr>(<span class="base">typing.NamedTuple</span>):

                <label class="view-source-button" for="DatasetDetails-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#DatasetDetails"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DatasetDetails-207"><a href="#DatasetDetails-207"><span class="linenos">207</span></a><span class="k">class</span> <span class="nc">DatasetDetails</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
</span><span id="DatasetDetails-208"><a href="#DatasetDetails-208"><span class="linenos">208</span></a>    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
</span><span id="DatasetDetails-209"><a href="#DatasetDetails-209"><span class="linenos">209</span></a>    <span class="n">url</span><span class="p">:</span> <span class="nb">str</span>
</span><span id="DatasetDetails-210"><a href="#DatasetDetails-210"><span class="linenos">210</span></a>    <span class="n">description</span><span class="p">:</span> <span class="nb">str</span>
</span><span id="DatasetDetails-211"><a href="#DatasetDetails-211"><span class="linenos">211</span></a>    <span class="n">size</span><span class="p">:</span> <span class="nb">int</span>
</span></pre></div>


            <div class="docstring"><p>DatasetDetails(name, url, description, size)</p>
</div>


                            <div id="DatasetDetails.__init__" class="classattr">
                                <div class="attr function">
            
        <span class="name">DatasetDetails</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">name</span><span class="p">:</span> <span class="nb">str</span>, </span><span class="param"><span class="n">url</span><span class="p">:</span> <span class="nb">str</span>, </span><span class="param"><span class="n">description</span><span class="p">:</span> <span class="nb">str</span>, </span><span class="param"><span class="n">size</span><span class="p">:</span> <span class="nb">int</span></span>)</span>

        
    </div>
    <a class="headerlink" href="#DatasetDetails.__init__"></a>
    
            <div class="docstring"><p>Create new instance of DatasetDetails(name, url, description, size)</p>
</div>


                            </div>
                            <div id="DatasetDetails.name" class="classattr">
                                <div class="attr variable">
            <span class="name">name</span><span class="annotation">: str</span>

        
    </div>
    <a class="headerlink" href="#DatasetDetails.name"></a>
    
            <div class="docstring"><p>Alias for field number 0</p>
</div>


                            </div>
                            <div id="DatasetDetails.url" class="classattr">
                                <div class="attr variable">
            <span class="name">url</span><span class="annotation">: str</span>

        
    </div>
    <a class="headerlink" href="#DatasetDetails.url"></a>
    
            <div class="docstring"><p>Alias for field number 1</p>
</div>


                            </div>
                            <div id="DatasetDetails.description" class="classattr">
                                <div class="attr variable">
            <span class="name">description</span><span class="annotation">: str</span>

        
    </div>
    <a class="headerlink" href="#DatasetDetails.description"></a>
    
            <div class="docstring"><p>Alias for field number 2</p>
</div>


                            </div>
                            <div id="DatasetDetails.size" class="classattr">
                                <div class="attr variable">
            <span class="name">size</span><span class="annotation">: int</span>

        
    </div>
    <a class="headerlink" href="#DatasetDetails.size"></a>
    
            <div class="docstring"><p>Alias for field number 3</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>builtins.tuple</dt>
                                <dd id="DatasetDetails.index" class="function">index</dd>
                <dd id="DatasetDetails.count" class="function">count</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="DATASET_DETAILS">
                    <div class="attr variable">
            <span class="name">DATASET_DETAILS</span>        =
<input id="DATASET_DETAILS-view-value" class="view-value-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
            <label class="view-value-button pdoc-button" for="DATASET_DETAILS-view-value"></label><span class="default_value">{&#39;trex&#39;: DatasetDetails(name=&#39;T-REx&#39;, url=&#39;https://hadyelsahar.github.io/t-rex/&#39;, description=&#39;A dataset which consists of knowledge triplets extracted from Wikipedia. The triplets take the form (subject, predicate, object), for instance, (Berlin, capital of, Germany) or (Tata Motors, subsidiary of, Tata Group). &#39;, size=32260), &#39;boolq&#39;: DatasetDetails(name=&#39;BoolQ&#39;, url=&#39;https://github.com/google-research-datasets/boolean-questions&#39;, description=&#39;A dataset consisting of question-passage-answer triplets. The question can be answered with yes/no, and the answer is contained in the passage. The questions are provided anonymously and unsolicited by users of the Google search engine, and afterwards paired with a paragraph from a Wikipedia article containing the answer.&#39;, size=12697), &#39;trivia_qa&#39;: DatasetDetails(name=&#39;TriviaQA&#39;, url=&#39;http://nlp.cs.washington.edu/triviaqa/&#39;, description=&#39;A dataset consisting of 95K question-answer pairs with with on average six supporting evidence documents per question, leading to ~650K question-passage-answer triplets. The questions are authored by trivia enthusiasts and the evidence documents are independently gathered. &#39;, size=156328), &#39;natural_questions&#39;: DatasetDetails(name=&#39;Natural Questions&#39;, url=&#39;https://github.com/google-research-datasets/natural-questions&#39;, description=&#39;A dataset consisting of ~320K question-passage-answer triplets. The questions are factual naturally-occurring questions. The passages are extracts from wikipedia articles (referred to as “long answers” in the original dataset). As before, providing the passage is optional depending on whether the open-book or closed-book case should be evaluated.&#39;, size=4289), &#39;crows-pairs&#39;: DatasetDetails(name=&#39;CrowS-Pairs&#39;, url=&#39;https://github.com/nyu-mll/crows-pairs&#39;, description=&#39;This dataset provides crowdsourced sentence pairs for the different categories along which stereotyping is to be measured.&#39;, size=1508), &#39;xsum&#39;: DatasetDetails(name=&#39;XSUM&#39;, url=&#39;https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset&#39;, description=&#39;A dataset consisting of newspaper articles from the BBC and their reference summaries. The reference summaries consist of a single sentence: the boldfaced sentence at the begininning of each BBC article, provided by article’s authors.&#39;, size=204045), &#39;womens_clothing_ecommerce_reviews&#39;: DatasetDetails(name=&#34;Women&#39;s E-commerce Clothing Reviews&#34;, url=&#39;https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews&#39;, description=&#39;This dataset consists of clothing reviews, both as a text and numerical scores.&#39;, size=23486), &#39;bold&#39;: DatasetDetails(name=&#39;BOLD&#39;, url=&#39;https://github.com/amazon-science/bold&#39;, description=&#39;A large-scale dataset that consists of English prompts aimed at testing bias and toxicity generation across five domains: profession, gender, race, religion, and political ideology.&#39;, size=23679), &#39;wikitext2&#39;: DatasetDetails(name=&#39;WikiText2&#39;, url=&#39;https://huggingface.co/datasets/wikitext&#39;, description=&#39;A dataset which consists of Good and Featured articles from Wikipedia. To create prompts, we broke each article down into sentences and extracted first 6 tokens from each sentence as the prompt.&#39;, size=86007), &#39;real_toxicity_prompts&#39;: DatasetDetails(name=&#39;Real Toxicity Prompts&#39;, url=&#39;https://github.com/allenai/real-toxicity-prompts&#39;, description=&#39;A dataset of truncated sentence snippets from the web. &#39;, size=98243), &#39;real_toxicity_prompts_challenging&#39;: DatasetDetails(name=&#39;Real Toxicity Prompts Challenging&#39;, url=&#39;https://github.com/allenai/real-toxicity-prompts&#39;, description=&#39;A dataset of truncated sentence snippets from the web. Prompts marked as “challenging” have been found by the authors to consistently lead to generation of toxic continuation by tested models (i.e., GPT-1, GPT-2, GPT-3, CTRL, CTRL-WIKI).&#39;, size=1199), &#39;gigaword&#39;: DatasetDetails(name=&#39;Gigaword&#39;, url=&#39;https://huggingface.co/datasets/gigaword&#39;, description=&#39;A dataset with around 4 million news articles with their summaries. We use the “validation set”, which includes 190k entries.&#39;, size=189651), &#39;gov_report&#39;: DatasetDetails(name=&#39;Government Report&#39;, url=&#39;https://gov-report-data.github.io/&#39;, description=&#39;A dataset including a long-form summarization benchmark. It contains significantly longer documents (9.4k words) and summaries (553 words) than most existing datasets.&#39;, size=7238)}</span>

        
    </div>
    <a class="headerlink" href="#DATASET_DETAILS"></a>
    
    

                </section>
                <section id="TREX_DESCRIPTION_EXAMPLES">
                    <div class="attr variable">
            <span class="name">TREX_DESCRIPTION_EXAMPLES</span>        =
<input id="TREX_DESCRIPTION_EXAMPLES-view-value" class="view-value-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
            <label class="view-value-button pdoc-button" for="TREX_DESCRIPTION_EXAMPLES-view-value"></label><span class="default_value">&#39;We convert these predicates to prompts, e.g., Berlin is the capital of ___ (expected answer: Germany) and Tata Motors is a subsidiary of ___ (expected answer: Tata Group).&#39;</span>

        
    </div>
    <a class="headerlink" href="#TREX_DESCRIPTION_EXAMPLES"></a>
    
    

                </section>
                <section id="CROWS_PAIRS_DISCLAIMER">
                    <div class="attr variable">
            <span class="name">CROWS_PAIRS_DISCLAIMER</span>        =
<input id="CROWS_PAIRS_DISCLAIMER-view-value" class="view-value-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
            <label class="view-value-button pdoc-button" for="CROWS_PAIRS_DISCLAIMER-view-value"></label><span class="default_value">&#39;**Disclaimer**: 1) The crowdsourced CrowS dataset is noisy. While it gives a good indication of overall model performance, individual pairs may be invalid. 2) CrowS measures U.S.-typical stereotypes. Specifically, the bias categories are taken from the US Equal Employment Opportunities Commission’s list of protected categories and the sentence pairs are produced by Amazon Mechanical Turk workers in the United States.&#39;</span>

        
    </div>
    <a class="headerlink" href="#CROWS_PAIRS_DISCLAIMER"></a>
    
    

                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>