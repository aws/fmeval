<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 14.1.0"/>
    <title>src.amazon_fmeval.eval_algorithms.summarization_accuracy API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .pdoc-alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:1rem center;margin-bottom:1rem;}.pdoc .pdoc-alert > *:last-child{margin-bottom:0;}.pdoc .pdoc-alert-note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../eval_algorithms.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;src.amazon_fmeval.eval_algorithms</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="variable" href="#PROMPT_COLUMN_NAME">PROMPT_COLUMN_NAME</a>
            </li>
            <li>
                    <a class="variable" href="#METEOR_SCORE">METEOR_SCORE</a>
            </li>
            <li>
                    <a class="variable" href="#ROUGE_SCORE">ROUGE_SCORE</a>
            </li>
            <li>
                    <a class="variable" href="#BERT_SCORE">BERT_SCORE</a>
            </li>
            <li>
                    <a class="variable" href="#ROUGE_1">ROUGE_1</a>
            </li>
            <li>
                    <a class="variable" href="#ROUGE_2">ROUGE_2</a>
            </li>
            <li>
                    <a class="variable" href="#ROUGE_L">ROUGE_L</a>
            </li>
            <li>
                    <a class="variable" href="#ROUGE_TYPES">ROUGE_TYPES</a>
            </li>
            <li>
                    <a class="variable" href="#MICROSOFT_DEBERTA_MODEL">MICROSOFT_DEBERTA_MODEL</a>
            </li>
            <li>
                    <a class="variable" href="#ROBERTA_MODEL">ROBERTA_MODEL</a>
            </li>
            <li>
                    <a class="variable" href="#DEFAULT_MODEL_TYPE">DEFAULT_MODEL_TYPE</a>
            </li>
            <li>
                    <a class="variable" href="#MODEL_TYPES_SUPPORTED">MODEL_TYPES_SUPPORTED</a>
            </li>
            <li>
                    <a class="variable" href="#logger">logger</a>
            </li>
            <li>
                    <a class="class" href="#SummarizationAccuracyConfig">SummarizationAccuracyConfig</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#SummarizationAccuracyConfig.__init__">SummarizationAccuracyConfig</a>
                        </li>
                        <li>
                                <a class="variable" href="#SummarizationAccuracyConfig.rouge_type">rouge_type</a>
                        </li>
                        <li>
                                <a class="variable" href="#SummarizationAccuracyConfig.use_stemmer_for_rouge">use_stemmer_for_rouge</a>
                        </li>
                        <li>
                                <a class="variable" href="#SummarizationAccuracyConfig.model_type_for_bertscore">model_type_for_bertscore</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#SummarizationAccuracy">SummarizationAccuracy</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#SummarizationAccuracy.__init__">SummarizationAccuracy</a>
                        </li>
                        <li>
                                <a class="variable" href="#SummarizationAccuracy.eval_name">eval_name</a>
                        </li>
                        <li>
                                <a class="function" href="#SummarizationAccuracy.evaluate_sample">evaluate_sample</a>
                        </li>
                        <li>
                                <a class="function" href="#SummarizationAccuracy.evaluate">evaluate</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="function" href="#get_meteor_score">get_meteor_score</a>
            </li>
            <li>
                    <a class="function" href="#get_rouge_score">get_rouge_score</a>
            </li>
            <li>
                    <a class="function" href="#get_bert_score">get_bert_score</a>
            </li>
            <li>
                    <a class="function" href="#add_score_to_dataset">add_score_to_dataset</a>
            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
src<wbr>.<a href="./../../amazon_fmeval.html">amazon_fmeval</a><wbr>.<a href="./../eval_algorithms.html">eval_algorithms</a><wbr>.summarization_accuracy    </h1>

                
                        <input id="mod-summarization_accuracy-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-summarization_accuracy-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">  1</span></a><span class="kn">import</span> <span class="nn">logging</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">  2</span></a><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">  3</span></a><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Callable</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos">  4</span></a>
</span><span id="L-5"><a href="#L-5"><span class="linenos">  5</span></a><span class="kn">import</span> <span class="nn">evaluate</span> <span class="k">as</span> <span class="nn">hf_evaluate</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">  6</span></a><span class="kn">import</span> <span class="nn">nltk</span>
</span><span id="L-7"><a href="#L-7"><span class="linenos">  7</span></a><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">  8</span></a><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">  9</span></a><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos"> 10</span></a><span class="kn">from</span> <span class="nn">nltk.translate</span> <span class="kn">import</span> <span class="n">meteor_score</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos"> 11</span></a>
</span><span id="L-12"><a href="#L-12"><span class="linenos"> 12</span></a><span class="kn">import</span> <span class="nn">amazon_fmeval.util</span> <span class="k">as</span> <span class="nn">util</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos"> 13</span></a><span class="kn">from</span> <span class="nn">amazon_fmeval.constants</span> <span class="kn">import</span> <span class="p">(</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos"> 14</span></a>    <span class="n">TARGET_OUTPUT_COLUMN_NAME</span><span class="p">,</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos"> 15</span></a>    <span class="n">MODEL_INPUT_COLUMN_NAME</span><span class="p">,</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos"> 16</span></a>    <span class="n">MODEL_OUTPUT_COLUMN_NAME</span><span class="p">,</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos"> 17</span></a>    <span class="n">MEAN</span><span class="p">,</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos"> 18</span></a><span class="p">)</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos"> 19</span></a><span class="kn">from</span> <span class="nn">amazon_fmeval.data_loaders.util</span> <span class="kn">import</span> <span class="n">DataConfig</span><span class="p">,</span> <span class="n">get_dataset</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos"> 20</span></a><span class="kn">from</span> <span class="nn">amazon_fmeval.eval_algorithms</span> <span class="kn">import</span> <span class="p">(</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos"> 21</span></a>    <span class="n">EvalAlgorithm</span><span class="p">,</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos"> 22</span></a>    <span class="n">EvalScore</span><span class="p">,</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos"> 23</span></a>    <span class="n">EvalOutput</span><span class="p">,</span>
</span><span id="L-24"><a href="#L-24"><span class="linenos"> 24</span></a>    <span class="n">EVAL_DATASETS</span><span class="p">,</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos"> 25</span></a>    <span class="n">DATASET_CONFIGS</span><span class="p">,</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos"> 26</span></a>    <span class="n">get_default_prompt_template</span><span class="p">,</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos"> 27</span></a><span class="p">)</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos"> 28</span></a><span class="kn">from</span> <span class="nn">amazon_fmeval.eval_algorithms.eval_algorithm</span> <span class="kn">import</span> <span class="p">(</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos"> 29</span></a>    <span class="n">EvalAlgorithmConfig</span><span class="p">,</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos"> 30</span></a>    <span class="n">EvalAlgorithmInterface</span><span class="p">,</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos"> 31</span></a><span class="p">)</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos"> 32</span></a><span class="kn">from</span> <span class="nn">amazon_fmeval.eval_algorithms.helper_models.helper_model</span> <span class="kn">import</span> <span class="n">BertscoreHelperModel</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos"> 33</span></a><span class="kn">from</span> <span class="nn">amazon_fmeval.eval_algorithms.util</span> <span class="kn">import</span> <span class="p">(</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos"> 34</span></a>    <span class="n">generate_prompt_column_for_dataset</span><span class="p">,</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos"> 35</span></a>    <span class="n">generate_model_predict_response_for_dataset</span><span class="p">,</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos"> 36</span></a>    <span class="n">validate_dataset</span><span class="p">,</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos"> 37</span></a>    <span class="n">aggregate_evaluation_scores</span><span class="p">,</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos"> 38</span></a>    <span class="n">generate_output_dataset_path</span><span class="p">,</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos"> 39</span></a>    <span class="n">save_dataset</span><span class="p">,</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos"> 40</span></a><span class="p">)</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos"> 41</span></a><span class="kn">from</span> <span class="nn">amazon_fmeval.exceptions</span> <span class="kn">import</span> <span class="n">EvalAlgorithmClientError</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos"> 42</span></a><span class="kn">from</span> <span class="nn">amazon_fmeval.model_runners.model_runner</span> <span class="kn">import</span> <span class="n">ModelRunner</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos"> 43</span></a><span class="kn">from</span> <span class="nn">amazon_fmeval.perf_util</span> <span class="kn">import</span> <span class="n">timed_block</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos"> 44</span></a>
</span><span id="L-45"><a href="#L-45"><span class="linenos"> 45</span></a><span class="n">PROMPT_COLUMN_NAME</span> <span class="o">=</span> <span class="s2">&quot;prompt&quot;</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos"> 46</span></a><span class="n">METEOR_SCORE</span> <span class="o">=</span> <span class="s2">&quot;meteor&quot;</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos"> 47</span></a><span class="n">ROUGE_SCORE</span> <span class="o">=</span> <span class="s2">&quot;rouge&quot;</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos"> 48</span></a><span class="n">BERT_SCORE</span> <span class="o">=</span> <span class="s2">&quot;bertscore&quot;</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos"> 49</span></a>
</span><span id="L-50"><a href="#L-50"><span class="linenos"> 50</span></a><span class="c1"># rouge constants</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos"> 51</span></a><span class="n">ROUGE_1</span> <span class="o">=</span> <span class="s2">&quot;rouge1&quot;</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos"> 52</span></a><span class="n">ROUGE_2</span> <span class="o">=</span> <span class="s2">&quot;rouge2&quot;</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos"> 53</span></a><span class="n">ROUGE_L</span> <span class="o">=</span> <span class="s2">&quot;rougeL&quot;</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos"> 54</span></a>
</span><span id="L-55"><a href="#L-55"><span class="linenos"> 55</span></a><span class="n">ROUGE_TYPES</span> <span class="o">=</span> <span class="p">[</span><span class="n">ROUGE_1</span><span class="p">,</span> <span class="n">ROUGE_2</span><span class="p">,</span> <span class="n">ROUGE_L</span><span class="p">]</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos"> 56</span></a>
</span><span id="L-57"><a href="#L-57"><span class="linenos"> 57</span></a><span class="c1"># bertscore constants</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos"> 58</span></a><span class="n">MICROSOFT_DEBERTA_MODEL</span> <span class="o">=</span> <span class="s2">&quot;microsoft/deberta-xlarge-mnli&quot;</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos"> 59</span></a><span class="n">ROBERTA_MODEL</span> <span class="o">=</span> <span class="s2">&quot;roberta-large-mnli&quot;</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos"> 60</span></a><span class="n">DEFAULT_MODEL_TYPE</span> <span class="o">=</span> <span class="n">MICROSOFT_DEBERTA_MODEL</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos"> 61</span></a><span class="n">MODEL_TYPES_SUPPORTED</span> <span class="o">=</span> <span class="p">[</span><span class="n">MICROSOFT_DEBERTA_MODEL</span><span class="p">,</span> <span class="n">ROBERTA_MODEL</span><span class="p">]</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos"> 62</span></a>
</span><span id="L-63"><a href="#L-63"><span class="linenos"> 63</span></a><span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos"> 64</span></a>
</span><span id="L-65"><a href="#L-65"><span class="linenos"> 65</span></a>
</span><span id="L-66"><a href="#L-66"><span class="linenos"> 66</span></a><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos"> 67</span></a><span class="k">class</span> <span class="nc">SummarizationAccuracyConfig</span><span class="p">(</span><span class="n">EvalAlgorithmConfig</span><span class="p">):</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos"> 68</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos"> 69</span></a><span class="sd">    Configuration for the summarization accuracy eval algorithm</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos"> 70</span></a>
</span><span id="L-71"><a href="#L-71"><span class="linenos"> 71</span></a><span class="sd">    :param rouge_type: Type of rouge metric in eval results</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos"> 72</span></a><span class="sd">    :param use_stemmer_for_rouge: bool value to set using stemmer for rouge metric</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos"> 73</span></a><span class="sd">    :param model_type_for_bertscore: model to use for bert score</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos"> 74</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos"> 75</span></a>
</span><span id="L-76"><a href="#L-76"><span class="linenos"> 76</span></a>    <span class="n">rouge_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">ROUGE_2</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos"> 77</span></a>    <span class="n">use_stemmer_for_rouge</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="L-78"><a href="#L-78"><span class="linenos"> 78</span></a>    <span class="n">model_type_for_bertscore</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">DEFAULT_MODEL_TYPE</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos"> 79</span></a>
</span><span id="L-80"><a href="#L-80"><span class="linenos"> 80</span></a>    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos"> 81</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">rouge_type</span> <span class="ow">in</span> <span class="n">ROUGE_TYPES</span><span class="p">:</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos"> 82</span></a>            <span class="k">raise</span> <span class="n">EvalAlgorithmClientError</span><span class="p">(</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos"> 83</span></a>                <span class="sa">f</span><span class="s2">&quot;Invalid rouge_type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rouge_type</span><span class="si">}</span><span class="s2"> requested in SummarizationAccuracyConfig, &quot;</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos"> 84</span></a>                <span class="sa">f</span><span class="s2">&quot;please choose from acceptable values: </span><span class="si">{</span><span class="n">ROUGE_TYPES</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="L-85"><a href="#L-85"><span class="linenos"> 85</span></a>            <span class="p">)</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos"> 86</span></a>
</span><span id="L-87"><a href="#L-87"><span class="linenos"> 87</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type_for_bertscore</span> <span class="ow">in</span> <span class="n">MODEL_TYPES_SUPPORTED</span><span class="p">:</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos"> 88</span></a>            <span class="k">raise</span> <span class="n">EvalAlgorithmClientError</span><span class="p">(</span>
</span><span id="L-89"><a href="#L-89"><span class="linenos"> 89</span></a>                <span class="sa">f</span><span class="s2">&quot;Invalid model_type_for_bertscore: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_type_for_bertscore</span><span class="si">}</span><span class="s2"> requested in &quot;</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos"> 90</span></a>                <span class="sa">f</span><span class="s2">&quot;SummarizationAccuracyConfig, please choose from acceptable values: </span><span class="si">{</span><span class="n">MODEL_TYPES_SUPPORTED</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos"> 91</span></a>            <span class="p">)</span>
</span><span id="L-92"><a href="#L-92"><span class="linenos"> 92</span></a>
</span><span id="L-93"><a href="#L-93"><span class="linenos"> 93</span></a>
</span><span id="L-94"><a href="#L-94"><span class="linenos"> 94</span></a><span class="k">class</span> <span class="nc">SummarizationAccuracy</span><span class="p">(</span><span class="n">EvalAlgorithmInterface</span><span class="p">):</span>
</span><span id="L-95"><a href="#L-95"><span class="linenos"> 95</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos"> 96</span></a><span class="sd">    Summarization Accuracy Eval algorithm</span>
</span><span id="L-97"><a href="#L-97"><span class="linenos"> 97</span></a>
</span><span id="L-98"><a href="#L-98"><span class="linenos"> 98</span></a><span class="sd">    The aim of this eval algo is to evaluate how well a model can summarise text.</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos"> 99</span></a><span class="sd">    The algo uses a reference summary to compare the output generated by the model and a series</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos">100</span></a><span class="sd">    of quality metrics based on overlapping between words (ROUGE and METEOR) and similarity scores (bert scores)</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos">101</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos">102</span></a>
</span><span id="L-103"><a href="#L-103"><span class="linenos">103</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eval_algorithm_config</span><span class="p">:</span> <span class="n">SummarizationAccuracyConfig</span> <span class="o">=</span> <span class="n">SummarizationAccuracyConfig</span><span class="p">()):</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos">104</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Default constructor</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos">105</span></a>
</span><span id="L-106"><a href="#L-106"><span class="linenos">106</span></a><span class="sd">        :param eval_algorithm_config: Summarization Accuracy eval algorithm config.</span>
</span><span id="L-107"><a href="#L-107"><span class="linenos">107</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos">108</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">eval_algorithm_config</span><span class="p">)</span>
</span><span id="L-109"><a href="#L-109"><span class="linenos">109</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">eval_name</span> <span class="o">=</span> <span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">SUMMARIZATION_ACCURACY</span><span class="o">.</span><span class="n">value</span>
</span><span id="L-110"><a href="#L-110"><span class="linenos">110</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_eval_algorithm_config</span> <span class="o">=</span> <span class="n">eval_algorithm_config</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos">111</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_load_eval_helpers</span><span class="p">()</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos">112</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_score_eval_func_mapping</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos">113</span></a>            <span class="n">METEOR_SCORE</span><span class="p">:</span> <span class="n">get_meteor_score</span><span class="p">,</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos">114</span></a>            <span class="n">ROUGE_SCORE</span><span class="p">:</span> <span class="n">get_rouge_score</span><span class="p">,</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos">115</span></a>            <span class="n">BERT_SCORE</span><span class="p">:</span> <span class="n">get_bert_score</span><span class="p">,</span>
</span><span id="L-116"><a href="#L-116"><span class="linenos">116</span></a>        <span class="p">}</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos">117</span></a>
</span><span id="L-118"><a href="#L-118"><span class="linenos">118</span></a>    <span class="k">def</span> <span class="nf">_load_eval_helpers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos">119</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos">120</span></a><span class="sd">        Method to download required helpers for eval_algo in constructor call</span>
</span><span id="L-121"><a href="#L-121"><span class="linenos">121</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-122"><a href="#L-122"><span class="linenos">122</span></a>        <span class="c1"># load helper modules for meteor</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos">123</span></a>        <span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;wordnet&quot;</span><span class="p">)</span>
</span><span id="L-124"><a href="#L-124"><span class="linenos">124</span></a>        <span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;punkt&quot;</span><span class="p">)</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos">125</span></a>        <span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;omw-1.4&quot;</span><span class="p">)</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos">126</span></a>
</span><span id="L-127"><a href="#L-127"><span class="linenos">127</span></a>        <span class="c1"># load HelperMode for bertscore</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos">128</span></a>        <span class="n">BertscoreHelperModel</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_eval_algorithm_config</span><span class="o">.</span><span class="n">model_type_for_bertscore</span><span class="p">)</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos">129</span></a>
</span><span id="L-130"><a href="#L-130"><span class="linenos">130</span></a>    <span class="k">def</span> <span class="nf">evaluate_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">EvalScore</span><span class="p">]:</span>  <span class="c1"># type: ignore[override]</span>
</span><span id="L-131"><a href="#L-131"><span class="linenos">131</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos">132</span></a><span class="sd">        Summarization Accuracy evaluate sample.</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos">133</span></a>
</span><span id="L-134"><a href="#L-134"><span class="linenos">134</span></a><span class="sd">        :param target_output: The expected responses from the model</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos">135</span></a><span class="sd">        :param model_output: The output of a model that we want to evaluate.</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos">136</span></a><span class="sd">        :return: list of EvalScore objects</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos">137</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos">138</span></a>        <span class="k">if</span> <span class="n">target_output</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-139"><a href="#L-139"><span class="linenos">139</span></a>            <span class="k">raise</span> <span class="n">EvalAlgorithmClientError</span><span class="p">(</span>
</span><span id="L-140"><a href="#L-140"><span class="linenos">140</span></a>                <span class="s2">&quot;Missing required input: target_output, for Summarization Accuracy evaluate_sample&quot;</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos">141</span></a>            <span class="p">)</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos">142</span></a>        <span class="k">if</span> <span class="n">model_output</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos">143</span></a>            <span class="k">raise</span> <span class="n">EvalAlgorithmClientError</span><span class="p">(</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos">144</span></a>                <span class="s2">&quot;Missing required input: model_output, for Summarization Accuracy evaluate_sample&quot;</span>
</span><span id="L-145"><a href="#L-145"><span class="linenos">145</span></a>            <span class="p">)</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos">146</span></a>
</span><span id="L-147"><a href="#L-147"><span class="linenos">147</span></a>        <span class="k">return</span> <span class="p">[</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos">148</span></a>            <span class="n">EvalScore</span><span class="p">(</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos">149</span></a>                <span class="n">name</span><span class="o">=</span><span class="n">eval_score</span><span class="p">,</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos">150</span></a>                <span class="n">value</span><span class="o">=</span><span class="n">eval_fn</span><span class="p">(</span>
</span><span id="L-151"><a href="#L-151"><span class="linenos">151</span></a>                    <span class="n">target_output</span><span class="o">=</span><span class="n">target_output</span><span class="p">,</span> <span class="n">model_output</span><span class="o">=</span><span class="n">model_output</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_eval_algorithm_config</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos">152</span></a>                <span class="p">),</span>
</span><span id="L-153"><a href="#L-153"><span class="linenos">153</span></a>            <span class="p">)</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos">154</span></a>            <span class="k">for</span> <span class="n">eval_score</span><span class="p">,</span> <span class="n">eval_fn</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_score_eval_func_mapping</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos">155</span></a>        <span class="p">]</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos">156</span></a>
</span><span id="L-157"><a href="#L-157"><span class="linenos">157</span></a>    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos">158</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos">159</span></a>        <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ModelRunner</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-160"><a href="#L-160"><span class="linenos">160</span></a>        <span class="n">dataset_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DataConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos">161</span></a>        <span class="n">prompt_template</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-162"><a href="#L-162"><span class="linenos">162</span></a>        <span class="n">save</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos">163</span></a>        <span class="n">num_records</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
</span><span id="L-164"><a href="#L-164"><span class="linenos">164</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">EvalOutput</span><span class="p">]:</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos">165</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-166"><a href="#L-166"><span class="linenos">166</span></a><span class="sd">        Summarization Accuracy evaluate</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos">167</span></a>
</span><span id="L-168"><a href="#L-168"><span class="linenos">168</span></a><span class="sd">        :param model: An instance of ModelRunner which is the model under evaluation</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos">169</span></a><span class="sd">        :param dataset_config: Configures the single dataset used for evaluation. If not provided,</span>
</span><span id="L-170"><a href="#L-170"><span class="linenos">170</span></a><span class="sd">            evaluation will use all of it&#39;s supported built-in datasets</span>
</span><span id="L-171"><a href="#L-171"><span class="linenos">171</span></a><span class="sd">        :param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos">172</span></a><span class="sd">            will be used.</span>
</span><span id="L-173"><a href="#L-173"><span class="linenos">173</span></a><span class="sd">        :param save: If set to true, prompt responses and scores will be saved to file. The output is written to</span>
</span><span id="L-174"><a href="#L-174"><span class="linenos">174</span></a><span class="sd">                     EvalAlgorithmInterface.EVAL_RESULTS_PATH</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos">175</span></a><span class="sd">        :param num_records: The number of records to be sampled randomly from the input dataset to perform the</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos">176</span></a><span class="sd">                            evaluation</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos">177</span></a>
</span><span id="L-178"><a href="#L-178"><span class="linenos">178</span></a><span class="sd">        :return: List of EvalOutput objects.</span>
</span><span id="L-179"><a href="#L-179"><span class="linenos">179</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos">180</span></a>        <span class="k">if</span> <span class="n">dataset_config</span><span class="p">:</span>
</span><span id="L-181"><a href="#L-181"><span class="linenos">181</span></a>            <span class="n">dataset_configs</span> <span class="o">=</span> <span class="p">[</span><span class="n">dataset_config</span><span class="p">]</span>
</span><span id="L-182"><a href="#L-182"><span class="linenos">182</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos">183</span></a>            <span class="n">dataset_configs</span> <span class="o">=</span> <span class="p">[</span><span class="n">DATASET_CONFIGS</span><span class="p">[</span><span class="n">dataset_name</span><span class="p">]</span> <span class="k">for</span> <span class="n">dataset_name</span> <span class="ow">in</span> <span class="n">EVAL_DATASETS</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_name</span><span class="p">]]</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos">184</span></a>
</span><span id="L-185"><a href="#L-185"><span class="linenos">185</span></a>        <span class="n">eval_outputs</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-186"><a href="#L-186"><span class="linenos">186</span></a>        <span class="k">for</span> <span class="n">dataset_config</span> <span class="ow">in</span> <span class="n">dataset_configs</span><span class="p">:</span>
</span><span id="L-187"><a href="#L-187"><span class="linenos">187</span></a>            <span class="n">dataset</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">dataset_config</span><span class="p">,</span> <span class="n">num_records</span><span class="p">)</span>
</span><span id="L-188"><a href="#L-188"><span class="linenos">188</span></a>            <span class="n">validate_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">TARGET_OUTPUT_COLUMN_NAME</span><span class="p">,</span> <span class="n">MODEL_INPUT_COLUMN_NAME</span><span class="p">])</span>
</span><span id="L-189"><a href="#L-189"><span class="linenos">189</span></a>            <span class="n">dataset_prompt_template</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="L-190"><a href="#L-190"><span class="linenos">190</span></a>            <span class="k">if</span> <span class="n">MODEL_OUTPUT_COLUMN_NAME</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">columns</span><span class="p">():</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos">191</span></a>                <span class="n">util</span><span class="o">.</span><span class="n">require</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;No ModelRunner provided. ModelRunner is required for inference on model_inputs&quot;</span><span class="p">)</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos">192</span></a>                <span class="n">dataset_prompt_template</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-193"><a href="#L-193"><span class="linenos">193</span></a>                    <span class="n">get_default_prompt_template</span><span class="p">(</span><span class="n">dataset_config</span><span class="o">.</span><span class="n">dataset_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">prompt_template</span> <span class="k">else</span> <span class="n">prompt_template</span>
</span><span id="L-194"><a href="#L-194"><span class="linenos">194</span></a>                <span class="p">)</span>
</span><span id="L-195"><a href="#L-195"><span class="linenos">195</span></a>                <span class="n">dataset</span> <span class="o">=</span> <span class="n">generate_prompt_column_for_dataset</span><span class="p">(</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos">196</span></a>                    <span class="n">dataset_prompt_template</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">MODEL_INPUT_COLUMN_NAME</span><span class="p">,</span> <span class="n">PROMPT_COLUMN_NAME</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos">197</span></a>                <span class="p">)</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos">198</span></a>                <span class="k">assert</span> <span class="n">model</span>  <span class="c1"># to satisfy mypy</span>
</span><span id="L-199"><a href="#L-199"><span class="linenos">199</span></a>                <span class="n">dataset</span> <span class="o">=</span> <span class="n">generate_model_predict_response_for_dataset</span><span class="p">(</span>
</span><span id="L-200"><a href="#L-200"><span class="linenos">200</span></a>                    <span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">PROMPT_COLUMN_NAME</span><span class="p">,</span> <span class="n">MODEL_OUTPUT_COLUMN_NAME</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos">201</span></a>                <span class="p">)</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos">202</span></a>            <span class="k">with</span> <span class="n">timed_block</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Computing score and aggregation on dataset </span><span class="si">{</span><span class="n">dataset_config</span><span class="o">.</span><span class="n">dataset_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">logger</span><span class="p">):</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos">203</span></a>                <span class="k">for</span> <span class="n">eval_score</span><span class="p">,</span> <span class="n">eval_func</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_score_eval_func_mapping</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="L-204"><a href="#L-204"><span class="linenos">204</span></a>                    <span class="n">dataset</span> <span class="o">=</span> <span class="n">add_score_to_dataset</span><span class="p">(</span>
</span><span id="L-205"><a href="#L-205"><span class="linenos">205</span></a>                        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
</span><span id="L-206"><a href="#L-206"><span class="linenos">206</span></a>                        <span class="n">eval_func</span><span class="o">=</span><span class="n">eval_func</span><span class="p">,</span>
</span><span id="L-207"><a href="#L-207"><span class="linenos">207</span></a>                        <span class="n">score_column_name</span><span class="o">=</span><span class="n">eval_score</span><span class="p">,</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos">208</span></a>                        <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_eval_algorithm_config</span><span class="p">,</span>
</span><span id="L-209"><a href="#L-209"><span class="linenos">209</span></a>                    <span class="p">)</span>
</span><span id="L-210"><a href="#L-210"><span class="linenos">210</span></a>
</span><span id="L-211"><a href="#L-211"><span class="linenos">211</span></a>                <span class="n">dataset_scores</span><span class="p">,</span> <span class="n">category_scores</span> <span class="o">=</span> <span class="n">aggregate_evaluation_scores</span><span class="p">(</span>
</span><span id="L-212"><a href="#L-212"><span class="linenos">212</span></a>                    <span class="n">dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">METEOR_SCORE</span><span class="p">,</span> <span class="n">ROUGE_SCORE</span><span class="p">,</span> <span class="n">BERT_SCORE</span><span class="p">],</span> <span class="n">agg_method</span><span class="o">=</span><span class="n">MEAN</span>
</span><span id="L-213"><a href="#L-213"><span class="linenos">213</span></a>                <span class="p">)</span>
</span><span id="L-214"><a href="#L-214"><span class="linenos">214</span></a>                <span class="n">eval_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos">215</span></a>                    <span class="n">EvalOutput</span><span class="p">(</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos">216</span></a>                        <span class="n">eval_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_name</span><span class="p">,</span>
</span><span id="L-217"><a href="#L-217"><span class="linenos">217</span></a>                        <span class="n">dataset_name</span><span class="o">=</span><span class="n">dataset_config</span><span class="o">.</span><span class="n">dataset_name</span><span class="p">,</span>
</span><span id="L-218"><a href="#L-218"><span class="linenos">218</span></a>                        <span class="n">prompt_template</span><span class="o">=</span><span class="n">dataset_prompt_template</span><span class="p">,</span>
</span><span id="L-219"><a href="#L-219"><span class="linenos">219</span></a>                        <span class="n">dataset_scores</span><span class="o">=</span><span class="n">dataset_scores</span><span class="p">,</span>
</span><span id="L-220"><a href="#L-220"><span class="linenos">220</span></a>                        <span class="n">category_scores</span><span class="o">=</span><span class="n">category_scores</span><span class="p">,</span>
</span><span id="L-221"><a href="#L-221"><span class="linenos">221</span></a>                        <span class="n">output_path</span><span class="o">=</span><span class="n">generate_output_dataset_path</span><span class="p">(</span>
</span><span id="L-222"><a href="#L-222"><span class="linenos">222</span></a>                            <span class="n">path_to_parent_dir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_eval_results_path</span><span class="p">,</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos">223</span></a>                            <span class="n">eval_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_name</span><span class="p">,</span>
</span><span id="L-224"><a href="#L-224"><span class="linenos">224</span></a>                            <span class="n">dataset_name</span><span class="o">=</span><span class="n">dataset_config</span><span class="o">.</span><span class="n">dataset_name</span><span class="p">,</span>
</span><span id="L-225"><a href="#L-225"><span class="linenos">225</span></a>                        <span class="p">),</span>
</span><span id="L-226"><a href="#L-226"><span class="linenos">226</span></a>                    <span class="p">)</span>
</span><span id="L-227"><a href="#L-227"><span class="linenos">227</span></a>                <span class="p">)</span>
</span><span id="L-228"><a href="#L-228"><span class="linenos">228</span></a>            <span class="k">if</span> <span class="n">save</span><span class="p">:</span>
</span><span id="L-229"><a href="#L-229"><span class="linenos">229</span></a>                <span class="n">save_dataset</span><span class="p">(</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos">230</span></a>                    <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
</span><span id="L-231"><a href="#L-231"><span class="linenos">231</span></a>                    <span class="n">score_names</span><span class="o">=</span><span class="p">[</span><span class="n">METEOR_SCORE</span><span class="p">,</span> <span class="n">ROUGE_SCORE</span><span class="p">,</span> <span class="n">BERT_SCORE</span><span class="p">],</span>
</span><span id="L-232"><a href="#L-232"><span class="linenos">232</span></a>                    <span class="n">path</span><span class="o">=</span><span class="n">generate_output_dataset_path</span><span class="p">(</span>
</span><span id="L-233"><a href="#L-233"><span class="linenos">233</span></a>                        <span class="n">path_to_parent_dir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_eval_results_path</span><span class="p">,</span>
</span><span id="L-234"><a href="#L-234"><span class="linenos">234</span></a>                        <span class="n">eval_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_name</span><span class="p">,</span>
</span><span id="L-235"><a href="#L-235"><span class="linenos">235</span></a>                        <span class="n">dataset_name</span><span class="o">=</span><span class="n">dataset_config</span><span class="o">.</span><span class="n">dataset_name</span><span class="p">,</span>
</span><span id="L-236"><a href="#L-236"><span class="linenos">236</span></a>                    <span class="p">),</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos">237</span></a>                <span class="p">)</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos">238</span></a>
</span><span id="L-239"><a href="#L-239"><span class="linenos">239</span></a>        <span class="k">return</span> <span class="n">eval_outputs</span>
</span><span id="L-240"><a href="#L-240"><span class="linenos">240</span></a>
</span><span id="L-241"><a href="#L-241"><span class="linenos">241</span></a>
</span><span id="L-242"><a href="#L-242"><span class="linenos">242</span></a><span class="k">def</span> <span class="nf">get_meteor_score</span><span class="p">(</span><span class="n">target_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SummarizationAccuracyConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
</span><span id="L-243"><a href="#L-243"><span class="linenos">243</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-244"><a href="#L-244"><span class="linenos">244</span></a><span class="sd">    METEOR, an automatic metric for machine translation evaluation</span>
</span><span id="L-245"><a href="#L-245"><span class="linenos">245</span></a><span class="sd">    that is based on a generalized concept of unigram matching between the</span>
</span><span id="L-246"><a href="#L-246"><span class="linenos">246</span></a><span class="sd">    machine-produced translation and human-produced reference translations.</span>
</span><span id="L-247"><a href="#L-247"><span class="linenos">247</span></a><span class="sd">    Unigrams can be matched based on their surface forms, stemmed forms,</span>
</span><span id="L-248"><a href="#L-248"><span class="linenos">248</span></a><span class="sd">    and meanings; furthermore, METEOR can be easily extended to include more</span>
</span><span id="L-249"><a href="#L-249"><span class="linenos">249</span></a><span class="sd">    advanced matching strategies. Once all generalized unigram matches</span>
</span><span id="L-250"><a href="#L-250"><span class="linenos">250</span></a><span class="sd">    between the two strings have been found, METEOR computes a score for</span>
</span><span id="L-251"><a href="#L-251"><span class="linenos">251</span></a><span class="sd">    this matching using a combination of unigram-precision, unigram-recall, and</span>
</span><span id="L-252"><a href="#L-252"><span class="linenos">252</span></a><span class="sd">    a measure of fragmentation that is designed to directly capture how</span>
</span><span id="L-253"><a href="#L-253"><span class="linenos">253</span></a><span class="sd">    well-ordered the matched words in the machine translation are in relation</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos">254</span></a><span class="sd">    to the reference.</span>
</span><span id="L-255"><a href="#L-255"><span class="linenos">255</span></a>
</span><span id="L-256"><a href="#L-256"><span class="linenos">256</span></a><span class="sd">    METEOR gets an R correlation value of 0.347 with human evaluation on the Arabic</span>
</span><span id="L-257"><a href="#L-257"><span class="linenos">257</span></a><span class="sd">    data and 0.331 on the Chinese data. This is shown to be an improvement on</span>
</span><span id="L-258"><a href="#L-258"><span class="linenos">258</span></a><span class="sd">    using simply unigram-precision, unigram-recall and their harmonic F1</span>
</span><span id="L-259"><a href="#L-259"><span class="linenos">259</span></a><span class="sd">    combination.</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos">260</span></a>
</span><span id="L-261"><a href="#L-261"><span class="linenos">261</span></a><span class="sd">    :param target_output: The expected responses from the model</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos">262</span></a><span class="sd">    :param model_output: The output of a model that we want to evaluate.</span>
</span><span id="L-263"><a href="#L-263"><span class="linenos">263</span></a><span class="sd">    :returns: meteor score</span>
</span><span id="L-264"><a href="#L-264"><span class="linenos">264</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos">265</span></a>    <span class="k">return</span> <span class="n">meteor_score</span><span class="o">.</span><span class="n">single_meteor_score</span><span class="p">(</span>
</span><span id="L-266"><a href="#L-266"><span class="linenos">266</span></a>        <span class="n">reference</span><span class="o">=</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">target_output</span><span class="p">),</span> <span class="n">hypothesis</span><span class="o">=</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">model_output</span><span class="p">)</span>
</span><span id="L-267"><a href="#L-267"><span class="linenos">267</span></a>    <span class="p">)</span>
</span><span id="L-268"><a href="#L-268"><span class="linenos">268</span></a>
</span><span id="L-269"><a href="#L-269"><span class="linenos">269</span></a>
</span><span id="L-270"><a href="#L-270"><span class="linenos">270</span></a><span class="k">def</span> <span class="nf">get_rouge_score</span><span class="p">(</span><span class="n">target_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SummarizationAccuracyConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
</span><span id="L-271"><a href="#L-271"><span class="linenos">271</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-272"><a href="#L-272"><span class="linenos">272</span></a><span class="sd">    The ROUGE-N, where N=[1,2,L], score is a standard metric for summarization quality.</span>
</span><span id="L-273"><a href="#L-273"><span class="linenos">273</span></a><span class="sd">    It computes the word overlap between the reference and model summary. Given that this metric is based on simple</span>
</span><span id="L-274"><a href="#L-274"><span class="linenos">274</span></a><span class="sd">    word overlap statistics, it works best for extractive summaries.</span>
</span><span id="L-275"><a href="#L-275"><span class="linenos">275</span></a><span class="sd">    Note that if we rephrase the summary without changing its meaning the ROUGE-N score will drop.</span>
</span><span id="L-276"><a href="#L-276"><span class="linenos">276</span></a>
</span><span id="L-277"><a href="#L-277"><span class="linenos">277</span></a><span class="sd">    Reference: https://huggingface.co/spaces/evaluate-metric/rouge</span>
</span><span id="L-278"><a href="#L-278"><span class="linenos">278</span></a>
</span><span id="L-279"><a href="#L-279"><span class="linenos">279</span></a><span class="sd">    :param target_output: The expected responses from the model</span>
</span><span id="L-280"><a href="#L-280"><span class="linenos">280</span></a><span class="sd">    :param model_output: The output of a model that we want to evaluate.</span>
</span><span id="L-281"><a href="#L-281"><span class="linenos">281</span></a><span class="sd">    :param config: Eval algo config</span>
</span><span id="L-282"><a href="#L-282"><span class="linenos">282</span></a><span class="sd">    :returns: rouge score: boolean indicating using stemmer for rouge</span>
</span><span id="L-283"><a href="#L-283"><span class="linenos">283</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-284"><a href="#L-284"><span class="linenos">284</span></a>    <span class="n">rouge</span> <span class="o">=</span> <span class="n">hf_evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;rouge&quot;</span><span class="p">)</span>
</span><span id="L-285"><a href="#L-285"><span class="linenos">285</span></a>    <span class="k">return</span> <span class="n">rouge</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span>
</span><span id="L-286"><a href="#L-286"><span class="linenos">286</span></a>        <span class="n">predictions</span><span class="o">=</span><span class="p">[</span><span class="n">model_output</span><span class="p">],</span>
</span><span id="L-287"><a href="#L-287"><span class="linenos">287</span></a>        <span class="n">references</span><span class="o">=</span><span class="p">[</span><span class="n">target_output</span><span class="p">],</span>
</span><span id="L-288"><a href="#L-288"><span class="linenos">288</span></a>        <span class="n">use_stemmer</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_stemmer_for_rouge</span><span class="p">,</span>
</span><span id="L-289"><a href="#L-289"><span class="linenos">289</span></a>        <span class="n">rouge_types</span><span class="o">=</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">rouge_type</span><span class="p">],</span>
</span><span id="L-290"><a href="#L-290"><span class="linenos">290</span></a>    <span class="p">)[</span><span class="n">config</span><span class="o">.</span><span class="n">rouge_type</span><span class="p">]</span>
</span><span id="L-291"><a href="#L-291"><span class="linenos">291</span></a>
</span><span id="L-292"><a href="#L-292"><span class="linenos">292</span></a>
</span><span id="L-293"><a href="#L-293"><span class="linenos">293</span></a><span class="k">def</span> <span class="nf">get_bert_score</span><span class="p">(</span><span class="n">target_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SummarizationAccuracyConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
</span><span id="L-294"><a href="#L-294"><span class="linenos">294</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-295"><a href="#L-295"><span class="linenos">295</span></a><span class="sd">    BERTscore is a similarity-based metric that compares the embedding of the prediction and target sentences</span>
</span><span id="L-296"><a href="#L-296"><span class="linenos">296</span></a><span class="sd">    under a (learned) model, typically, from the BERT family.</span>
</span><span id="L-297"><a href="#L-297"><span class="linenos">297</span></a><span class="sd">    This score may lead to increased flexibility compared to rouge and METEOR in terms of rephrasing since</span>
</span><span id="L-298"><a href="#L-298"><span class="linenos">298</span></a><span class="sd">    semantically similar sentences are (typically) embedded similarly.</span>
</span><span id="L-299"><a href="#L-299"><span class="linenos">299</span></a>
</span><span id="L-300"><a href="#L-300"><span class="linenos">300</span></a><span class="sd">    https://huggingface.co/spaces/evaluate-metric/bertscore</span>
</span><span id="L-301"><a href="#L-301"><span class="linenos">301</span></a>
</span><span id="L-302"><a href="#L-302"><span class="linenos">302</span></a><span class="sd">    :param target_output: The expected responses from the model</span>
</span><span id="L-303"><a href="#L-303"><span class="linenos">303</span></a><span class="sd">    :param model_output: The output of a model that we want to evaluate.</span>
</span><span id="L-304"><a href="#L-304"><span class="linenos">304</span></a><span class="sd">    :param config: Eval algo config</span>
</span><span id="L-305"><a href="#L-305"><span class="linenos">305</span></a><span class="sd">    :returns: rouge score: boolean indicating using stemmer for rouge</span>
</span><span id="L-306"><a href="#L-306"><span class="linenos">306</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-307"><a href="#L-307"><span class="linenos">307</span></a>    <span class="n">bertscore</span> <span class="o">=</span> <span class="n">BertscoreHelperModel</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">model_type_for_bertscore</span><span class="p">)</span>
</span><span id="L-308"><a href="#L-308"><span class="linenos">308</span></a>    <span class="k">return</span> <span class="n">bertscore</span><span class="o">.</span><span class="n">get_helper_scores</span><span class="p">(</span><span class="n">target_output</span><span class="p">,</span> <span class="n">model_output</span><span class="p">)</span>
</span><span id="L-309"><a href="#L-309"><span class="linenos">309</span></a>
</span><span id="L-310"><a href="#L-310"><span class="linenos">310</span></a>
</span><span id="L-311"><a href="#L-311"><span class="linenos">311</span></a><span class="k">def</span> <span class="nf">add_score_to_dataset</span><span class="p">(</span>
</span><span id="L-312"><a href="#L-312"><span class="linenos">312</span></a>    <span class="n">dataset</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">eval_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">score_column_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SummarizationAccuracyConfig</span>
</span><span id="L-313"><a href="#L-313"><span class="linenos">313</span></a><span class="p">):</span>
</span><span id="L-314"><a href="#L-314"><span class="linenos">314</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-315"><a href="#L-315"><span class="linenos">315</span></a><span class="sd">    Util method to add a score column to a ray dataset.</span>
</span><span id="L-316"><a href="#L-316"><span class="linenos">316</span></a>
</span><span id="L-317"><a href="#L-317"><span class="linenos">317</span></a><span class="sd">    :param dataset: ray Dataset to be used for eval score generation</span>
</span><span id="L-318"><a href="#L-318"><span class="linenos">318</span></a><span class="sd">    :param eval_func: eval function callable method</span>
</span><span id="L-319"><a href="#L-319"><span class="linenos">319</span></a><span class="sd">    :param score_column_name: column name for score to be added</span>
</span><span id="L-320"><a href="#L-320"><span class="linenos">320</span></a><span class="sd">    :param config: Eval algo config</span>
</span><span id="L-321"><a href="#L-321"><span class="linenos">321</span></a><span class="sd">    :returns: ray Dataset with score column</span>
</span><span id="L-322"><a href="#L-322"><span class="linenos">322</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-323"><a href="#L-323"><span class="linenos">323</span></a>
</span><span id="L-324"><a href="#L-324"><span class="linenos">324</span></a>    <span class="k">def</span> <span class="nf">_generate_eval_scores</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">:</span>  <span class="c1"># pragma: no cover</span>
</span><span id="L-325"><a href="#L-325"><span class="linenos">325</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-326"><a href="#L-326"><span class="linenos">326</span></a><span class="sd">        Map function generating the scores for every input record in input dataset</span>
</span><span id="L-327"><a href="#L-327"><span class="linenos">327</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="L-328"><a href="#L-328"><span class="linenos">328</span></a>        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span>
</span><span id="L-329"><a href="#L-329"><span class="linenos">329</span></a>            <span class="n">data</span><span class="o">=</span><span class="p">[</span>
</span><span id="L-330"><a href="#L-330"><span class="linenos">330</span></a>                <span class="n">eval_func</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="n">TARGET_OUTPUT_COLUMN_NAME</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="n">MODEL_OUTPUT_COLUMN_NAME</span><span class="p">],</span> <span class="n">config</span><span class="p">)</span>
</span><span id="L-331"><a href="#L-331"><span class="linenos">331</span></a>                <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">()</span>
</span><span id="L-332"><a href="#L-332"><span class="linenos">332</span></a>            <span class="p">]</span>
</span><span id="L-333"><a href="#L-333"><span class="linenos">333</span></a>        <span class="p">)</span>
</span><span id="L-334"><a href="#L-334"><span class="linenos">334</span></a>
</span><span id="L-335"><a href="#L-335"><span class="linenos">335</span></a>    <span class="k">return</span> <span class="n">dataset</span><span class="o">.</span><span class="n">add_column</span><span class="p">(</span><span class="n">score_column_name</span><span class="p">,</span> <span class="n">_generate_eval_scores</span><span class="p">)</span><span class="o">.</span><span class="n">materialize</span><span class="p">()</span>
</span></pre></div>


            </section>
                <section id="PROMPT_COLUMN_NAME">
                    <div class="attr variable">
            <span class="name">PROMPT_COLUMN_NAME</span>        =
<span class="default_value">&#39;prompt&#39;</span>

        
    </div>
    <a class="headerlink" href="#PROMPT_COLUMN_NAME"></a>
    
    

                </section>
                <section id="METEOR_SCORE">
                    <div class="attr variable">
            <span class="name">METEOR_SCORE</span>        =
<span class="default_value">&#39;meteor&#39;</span>

        
    </div>
    <a class="headerlink" href="#METEOR_SCORE"></a>
    
    

                </section>
                <section id="ROUGE_SCORE">
                    <div class="attr variable">
            <span class="name">ROUGE_SCORE</span>        =
<span class="default_value">&#39;rouge&#39;</span>

        
    </div>
    <a class="headerlink" href="#ROUGE_SCORE"></a>
    
    

                </section>
                <section id="BERT_SCORE">
                    <div class="attr variable">
            <span class="name">BERT_SCORE</span>        =
<span class="default_value">&#39;bertscore&#39;</span>

        
    </div>
    <a class="headerlink" href="#BERT_SCORE"></a>
    
    

                </section>
                <section id="ROUGE_1">
                    <div class="attr variable">
            <span class="name">ROUGE_1</span>        =
<span class="default_value">&#39;rouge1&#39;</span>

        
    </div>
    <a class="headerlink" href="#ROUGE_1"></a>
    
    

                </section>
                <section id="ROUGE_2">
                    <div class="attr variable">
            <span class="name">ROUGE_2</span>        =
<span class="default_value">&#39;rouge2&#39;</span>

        
    </div>
    <a class="headerlink" href="#ROUGE_2"></a>
    
    

                </section>
                <section id="ROUGE_L">
                    <div class="attr variable">
            <span class="name">ROUGE_L</span>        =
<span class="default_value">&#39;rougeL&#39;</span>

        
    </div>
    <a class="headerlink" href="#ROUGE_L"></a>
    
    

                </section>
                <section id="ROUGE_TYPES">
                    <div class="attr variable">
            <span class="name">ROUGE_TYPES</span>        =
<span class="default_value">[&#39;rouge1&#39;, &#39;rouge2&#39;, &#39;rougeL&#39;]</span>

        
    </div>
    <a class="headerlink" href="#ROUGE_TYPES"></a>
    
    

                </section>
                <section id="MICROSOFT_DEBERTA_MODEL">
                    <div class="attr variable">
            <span class="name">MICROSOFT_DEBERTA_MODEL</span>        =
<span class="default_value">&#39;microsoft/deberta-xlarge-mnli&#39;</span>

        
    </div>
    <a class="headerlink" href="#MICROSOFT_DEBERTA_MODEL"></a>
    
    

                </section>
                <section id="ROBERTA_MODEL">
                    <div class="attr variable">
            <span class="name">ROBERTA_MODEL</span>        =
<span class="default_value">&#39;roberta-large-mnli&#39;</span>

        
    </div>
    <a class="headerlink" href="#ROBERTA_MODEL"></a>
    
    

                </section>
                <section id="DEFAULT_MODEL_TYPE">
                    <div class="attr variable">
            <span class="name">DEFAULT_MODEL_TYPE</span>        =
<span class="default_value">&#39;microsoft/deberta-xlarge-mnli&#39;</span>

        
    </div>
    <a class="headerlink" href="#DEFAULT_MODEL_TYPE"></a>
    
    

                </section>
                <section id="MODEL_TYPES_SUPPORTED">
                    <div class="attr variable">
            <span class="name">MODEL_TYPES_SUPPORTED</span>        =
<span class="default_value">[&#39;microsoft/deberta-xlarge-mnli&#39;, &#39;roberta-large-mnli&#39;]</span>

        
    </div>
    <a class="headerlink" href="#MODEL_TYPES_SUPPORTED"></a>
    
    

                </section>
                <section id="logger">
                    <div class="attr variable">
            <span class="name">logger</span>        =
<span class="default_value">&lt;Logger <a href="">src.amazon_fmeval.eval_algorithms.summarization_accuracy</a> (WARNING)&gt;</span>

        
    </div>
    <a class="headerlink" href="#logger"></a>
    
    

                </section>
                <section id="SummarizationAccuracyConfig">
                            <input id="SummarizationAccuracyConfig-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
                    <div class="decorator">@dataclass(frozen=True)</div>

    <span class="def">class</span>
    <span class="name">SummarizationAccuracyConfig</span><wbr>(<span class="base">amazon_fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmConfig</span>):

                <label class="view-source-button" for="SummarizationAccuracyConfig-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SummarizationAccuracyConfig"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SummarizationAccuracyConfig-67"><a href="#SummarizationAccuracyConfig-67"><span class="linenos">67</span></a><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="SummarizationAccuracyConfig-68"><a href="#SummarizationAccuracyConfig-68"><span class="linenos">68</span></a><span class="k">class</span> <span class="nc">SummarizationAccuracyConfig</span><span class="p">(</span><span class="n">EvalAlgorithmConfig</span><span class="p">):</span>
</span><span id="SummarizationAccuracyConfig-69"><a href="#SummarizationAccuracyConfig-69"><span class="linenos">69</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="SummarizationAccuracyConfig-70"><a href="#SummarizationAccuracyConfig-70"><span class="linenos">70</span></a><span class="sd">    Configuration for the summarization accuracy eval algorithm</span>
</span><span id="SummarizationAccuracyConfig-71"><a href="#SummarizationAccuracyConfig-71"><span class="linenos">71</span></a>
</span><span id="SummarizationAccuracyConfig-72"><a href="#SummarizationAccuracyConfig-72"><span class="linenos">72</span></a><span class="sd">    :param rouge_type: Type of rouge metric in eval results</span>
</span><span id="SummarizationAccuracyConfig-73"><a href="#SummarizationAccuracyConfig-73"><span class="linenos">73</span></a><span class="sd">    :param use_stemmer_for_rouge: bool value to set using stemmer for rouge metric</span>
</span><span id="SummarizationAccuracyConfig-74"><a href="#SummarizationAccuracyConfig-74"><span class="linenos">74</span></a><span class="sd">    :param model_type_for_bertscore: model to use for bert score</span>
</span><span id="SummarizationAccuracyConfig-75"><a href="#SummarizationAccuracyConfig-75"><span class="linenos">75</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="SummarizationAccuracyConfig-76"><a href="#SummarizationAccuracyConfig-76"><span class="linenos">76</span></a>
</span><span id="SummarizationAccuracyConfig-77"><a href="#SummarizationAccuracyConfig-77"><span class="linenos">77</span></a>    <span class="n">rouge_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">ROUGE_2</span>
</span><span id="SummarizationAccuracyConfig-78"><a href="#SummarizationAccuracyConfig-78"><span class="linenos">78</span></a>    <span class="n">use_stemmer_for_rouge</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="SummarizationAccuracyConfig-79"><a href="#SummarizationAccuracyConfig-79"><span class="linenos">79</span></a>    <span class="n">model_type_for_bertscore</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">DEFAULT_MODEL_TYPE</span>
</span><span id="SummarizationAccuracyConfig-80"><a href="#SummarizationAccuracyConfig-80"><span class="linenos">80</span></a>
</span><span id="SummarizationAccuracyConfig-81"><a href="#SummarizationAccuracyConfig-81"><span class="linenos">81</span></a>    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="SummarizationAccuracyConfig-82"><a href="#SummarizationAccuracyConfig-82"><span class="linenos">82</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">rouge_type</span> <span class="ow">in</span> <span class="n">ROUGE_TYPES</span><span class="p">:</span>
</span><span id="SummarizationAccuracyConfig-83"><a href="#SummarizationAccuracyConfig-83"><span class="linenos">83</span></a>            <span class="k">raise</span> <span class="n">EvalAlgorithmClientError</span><span class="p">(</span>
</span><span id="SummarizationAccuracyConfig-84"><a href="#SummarizationAccuracyConfig-84"><span class="linenos">84</span></a>                <span class="sa">f</span><span class="s2">&quot;Invalid rouge_type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rouge_type</span><span class="si">}</span><span class="s2"> requested in SummarizationAccuracyConfig, &quot;</span>
</span><span id="SummarizationAccuracyConfig-85"><a href="#SummarizationAccuracyConfig-85"><span class="linenos">85</span></a>                <span class="sa">f</span><span class="s2">&quot;please choose from acceptable values: </span><span class="si">{</span><span class="n">ROUGE_TYPES</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="SummarizationAccuracyConfig-86"><a href="#SummarizationAccuracyConfig-86"><span class="linenos">86</span></a>            <span class="p">)</span>
</span><span id="SummarizationAccuracyConfig-87"><a href="#SummarizationAccuracyConfig-87"><span class="linenos">87</span></a>
</span><span id="SummarizationAccuracyConfig-88"><a href="#SummarizationAccuracyConfig-88"><span class="linenos">88</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_type_for_bertscore</span> <span class="ow">in</span> <span class="n">MODEL_TYPES_SUPPORTED</span><span class="p">:</span>
</span><span id="SummarizationAccuracyConfig-89"><a href="#SummarizationAccuracyConfig-89"><span class="linenos">89</span></a>            <span class="k">raise</span> <span class="n">EvalAlgorithmClientError</span><span class="p">(</span>
</span><span id="SummarizationAccuracyConfig-90"><a href="#SummarizationAccuracyConfig-90"><span class="linenos">90</span></a>                <span class="sa">f</span><span class="s2">&quot;Invalid model_type_for_bertscore: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_type_for_bertscore</span><span class="si">}</span><span class="s2"> requested in &quot;</span>
</span><span id="SummarizationAccuracyConfig-91"><a href="#SummarizationAccuracyConfig-91"><span class="linenos">91</span></a>                <span class="sa">f</span><span class="s2">&quot;SummarizationAccuracyConfig, please choose from acceptable values: </span><span class="si">{</span><span class="n">MODEL_TYPES_SUPPORTED</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="SummarizationAccuracyConfig-92"><a href="#SummarizationAccuracyConfig-92"><span class="linenos">92</span></a>            <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Configuration for the summarization accuracy eval algorithm</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>rouge_type</strong>:  Type of rouge metric in eval results</li>
<li><strong>use_stemmer_for_rouge</strong>:  bool value to set using stemmer for rouge metric</li>
<li><strong>model_type_for_bertscore</strong>:  model to use for bert score</li>
</ul>
</div>


                            <div id="SummarizationAccuracyConfig.__init__" class="classattr">
                                <div class="attr function">
            
        <span class="name">SummarizationAccuracyConfig</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">rouge_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;rouge2&#39;</span>,</span><span class="param">	<span class="n">use_stemmer_for_rouge</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">model_type_for_bertscore</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;microsoft/deberta-xlarge-mnli&#39;</span></span>)</span>

        
    </div>
    <a class="headerlink" href="#SummarizationAccuracyConfig.__init__"></a>
    
    

                            </div>
                            <div id="SummarizationAccuracyConfig.rouge_type" class="classattr">
                                <div class="attr variable">
            <span class="name">rouge_type</span><span class="annotation">: str</span>        =
<span class="default_value">&#39;rouge2&#39;</span>

        
    </div>
    <a class="headerlink" href="#SummarizationAccuracyConfig.rouge_type"></a>
    
    

                            </div>
                            <div id="SummarizationAccuracyConfig.use_stemmer_for_rouge" class="classattr">
                                <div class="attr variable">
            <span class="name">use_stemmer_for_rouge</span><span class="annotation">: bool</span>        =
<span class="default_value">True</span>

        
    </div>
    <a class="headerlink" href="#SummarizationAccuracyConfig.use_stemmer_for_rouge"></a>
    
    

                            </div>
                            <div id="SummarizationAccuracyConfig.model_type_for_bertscore" class="classattr">
                                <div class="attr variable">
            <span class="name">model_type_for_bertscore</span><span class="annotation">: str</span>        =
<span class="default_value">&#39;microsoft/deberta-xlarge-mnli&#39;</span>

        
    </div>
    <a class="headerlink" href="#SummarizationAccuracyConfig.model_type_for_bertscore"></a>
    
    

                            </div>
                </section>
                <section id="SummarizationAccuracy">
                            <input id="SummarizationAccuracy-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">SummarizationAccuracy</span><wbr>(<span class="base">amazon_fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface</span>):

                <label class="view-source-button" for="SummarizationAccuracy-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SummarizationAccuracy"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SummarizationAccuracy-95"><a href="#SummarizationAccuracy-95"><span class="linenos"> 95</span></a><span class="k">class</span> <span class="nc">SummarizationAccuracy</span><span class="p">(</span><span class="n">EvalAlgorithmInterface</span><span class="p">):</span>
</span><span id="SummarizationAccuracy-96"><a href="#SummarizationAccuracy-96"><span class="linenos"> 96</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="SummarizationAccuracy-97"><a href="#SummarizationAccuracy-97"><span class="linenos"> 97</span></a><span class="sd">    Summarization Accuracy Eval algorithm</span>
</span><span id="SummarizationAccuracy-98"><a href="#SummarizationAccuracy-98"><span class="linenos"> 98</span></a>
</span><span id="SummarizationAccuracy-99"><a href="#SummarizationAccuracy-99"><span class="linenos"> 99</span></a><span class="sd">    The aim of this eval algo is to evaluate how well a model can summarise text.</span>
</span><span id="SummarizationAccuracy-100"><a href="#SummarizationAccuracy-100"><span class="linenos">100</span></a><span class="sd">    The algo uses a reference summary to compare the output generated by the model and a series</span>
</span><span id="SummarizationAccuracy-101"><a href="#SummarizationAccuracy-101"><span class="linenos">101</span></a><span class="sd">    of quality metrics based on overlapping between words (ROUGE and METEOR) and similarity scores (bert scores)</span>
</span><span id="SummarizationAccuracy-102"><a href="#SummarizationAccuracy-102"><span class="linenos">102</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="SummarizationAccuracy-103"><a href="#SummarizationAccuracy-103"><span class="linenos">103</span></a>
</span><span id="SummarizationAccuracy-104"><a href="#SummarizationAccuracy-104"><span class="linenos">104</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eval_algorithm_config</span><span class="p">:</span> <span class="n">SummarizationAccuracyConfig</span> <span class="o">=</span> <span class="n">SummarizationAccuracyConfig</span><span class="p">()):</span>
</span><span id="SummarizationAccuracy-105"><a href="#SummarizationAccuracy-105"><span class="linenos">105</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Default constructor</span>
</span><span id="SummarizationAccuracy-106"><a href="#SummarizationAccuracy-106"><span class="linenos">106</span></a>
</span><span id="SummarizationAccuracy-107"><a href="#SummarizationAccuracy-107"><span class="linenos">107</span></a><span class="sd">        :param eval_algorithm_config: Summarization Accuracy eval algorithm config.</span>
</span><span id="SummarizationAccuracy-108"><a href="#SummarizationAccuracy-108"><span class="linenos">108</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="SummarizationAccuracy-109"><a href="#SummarizationAccuracy-109"><span class="linenos">109</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">eval_algorithm_config</span><span class="p">)</span>
</span><span id="SummarizationAccuracy-110"><a href="#SummarizationAccuracy-110"><span class="linenos">110</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">eval_name</span> <span class="o">=</span> <span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">SUMMARIZATION_ACCURACY</span><span class="o">.</span><span class="n">value</span>
</span><span id="SummarizationAccuracy-111"><a href="#SummarizationAccuracy-111"><span class="linenos">111</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_eval_algorithm_config</span> <span class="o">=</span> <span class="n">eval_algorithm_config</span>
</span><span id="SummarizationAccuracy-112"><a href="#SummarizationAccuracy-112"><span class="linenos">112</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_load_eval_helpers</span><span class="p">()</span>
</span><span id="SummarizationAccuracy-113"><a href="#SummarizationAccuracy-113"><span class="linenos">113</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_score_eval_func_mapping</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="SummarizationAccuracy-114"><a href="#SummarizationAccuracy-114"><span class="linenos">114</span></a>            <span class="n">METEOR_SCORE</span><span class="p">:</span> <span class="n">get_meteor_score</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-115"><a href="#SummarizationAccuracy-115"><span class="linenos">115</span></a>            <span class="n">ROUGE_SCORE</span><span class="p">:</span> <span class="n">get_rouge_score</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-116"><a href="#SummarizationAccuracy-116"><span class="linenos">116</span></a>            <span class="n">BERT_SCORE</span><span class="p">:</span> <span class="n">get_bert_score</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-117"><a href="#SummarizationAccuracy-117"><span class="linenos">117</span></a>        <span class="p">}</span>
</span><span id="SummarizationAccuracy-118"><a href="#SummarizationAccuracy-118"><span class="linenos">118</span></a>
</span><span id="SummarizationAccuracy-119"><a href="#SummarizationAccuracy-119"><span class="linenos">119</span></a>    <span class="k">def</span> <span class="nf">_load_eval_helpers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="SummarizationAccuracy-120"><a href="#SummarizationAccuracy-120"><span class="linenos">120</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="SummarizationAccuracy-121"><a href="#SummarizationAccuracy-121"><span class="linenos">121</span></a><span class="sd">        Method to download required helpers for eval_algo in constructor call</span>
</span><span id="SummarizationAccuracy-122"><a href="#SummarizationAccuracy-122"><span class="linenos">122</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="SummarizationAccuracy-123"><a href="#SummarizationAccuracy-123"><span class="linenos">123</span></a>        <span class="c1"># load helper modules for meteor</span>
</span><span id="SummarizationAccuracy-124"><a href="#SummarizationAccuracy-124"><span class="linenos">124</span></a>        <span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;wordnet&quot;</span><span class="p">)</span>
</span><span id="SummarizationAccuracy-125"><a href="#SummarizationAccuracy-125"><span class="linenos">125</span></a>        <span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;punkt&quot;</span><span class="p">)</span>
</span><span id="SummarizationAccuracy-126"><a href="#SummarizationAccuracy-126"><span class="linenos">126</span></a>        <span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;omw-1.4&quot;</span><span class="p">)</span>
</span><span id="SummarizationAccuracy-127"><a href="#SummarizationAccuracy-127"><span class="linenos">127</span></a>
</span><span id="SummarizationAccuracy-128"><a href="#SummarizationAccuracy-128"><span class="linenos">128</span></a>        <span class="c1"># load HelperMode for bertscore</span>
</span><span id="SummarizationAccuracy-129"><a href="#SummarizationAccuracy-129"><span class="linenos">129</span></a>        <span class="n">BertscoreHelperModel</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_eval_algorithm_config</span><span class="o">.</span><span class="n">model_type_for_bertscore</span><span class="p">)</span>
</span><span id="SummarizationAccuracy-130"><a href="#SummarizationAccuracy-130"><span class="linenos">130</span></a>
</span><span id="SummarizationAccuracy-131"><a href="#SummarizationAccuracy-131"><span class="linenos">131</span></a>    <span class="k">def</span> <span class="nf">evaluate_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">EvalScore</span><span class="p">]:</span>  <span class="c1"># type: ignore[override]</span>
</span><span id="SummarizationAccuracy-132"><a href="#SummarizationAccuracy-132"><span class="linenos">132</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="SummarizationAccuracy-133"><a href="#SummarizationAccuracy-133"><span class="linenos">133</span></a><span class="sd">        Summarization Accuracy evaluate sample.</span>
</span><span id="SummarizationAccuracy-134"><a href="#SummarizationAccuracy-134"><span class="linenos">134</span></a>
</span><span id="SummarizationAccuracy-135"><a href="#SummarizationAccuracy-135"><span class="linenos">135</span></a><span class="sd">        :param target_output: The expected responses from the model</span>
</span><span id="SummarizationAccuracy-136"><a href="#SummarizationAccuracy-136"><span class="linenos">136</span></a><span class="sd">        :param model_output: The output of a model that we want to evaluate.</span>
</span><span id="SummarizationAccuracy-137"><a href="#SummarizationAccuracy-137"><span class="linenos">137</span></a><span class="sd">        :return: list of EvalScore objects</span>
</span><span id="SummarizationAccuracy-138"><a href="#SummarizationAccuracy-138"><span class="linenos">138</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="SummarizationAccuracy-139"><a href="#SummarizationAccuracy-139"><span class="linenos">139</span></a>        <span class="k">if</span> <span class="n">target_output</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="SummarizationAccuracy-140"><a href="#SummarizationAccuracy-140"><span class="linenos">140</span></a>            <span class="k">raise</span> <span class="n">EvalAlgorithmClientError</span><span class="p">(</span>
</span><span id="SummarizationAccuracy-141"><a href="#SummarizationAccuracy-141"><span class="linenos">141</span></a>                <span class="s2">&quot;Missing required input: target_output, for Summarization Accuracy evaluate_sample&quot;</span>
</span><span id="SummarizationAccuracy-142"><a href="#SummarizationAccuracy-142"><span class="linenos">142</span></a>            <span class="p">)</span>
</span><span id="SummarizationAccuracy-143"><a href="#SummarizationAccuracy-143"><span class="linenos">143</span></a>        <span class="k">if</span> <span class="n">model_output</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="SummarizationAccuracy-144"><a href="#SummarizationAccuracy-144"><span class="linenos">144</span></a>            <span class="k">raise</span> <span class="n">EvalAlgorithmClientError</span><span class="p">(</span>
</span><span id="SummarizationAccuracy-145"><a href="#SummarizationAccuracy-145"><span class="linenos">145</span></a>                <span class="s2">&quot;Missing required input: model_output, for Summarization Accuracy evaluate_sample&quot;</span>
</span><span id="SummarizationAccuracy-146"><a href="#SummarizationAccuracy-146"><span class="linenos">146</span></a>            <span class="p">)</span>
</span><span id="SummarizationAccuracy-147"><a href="#SummarizationAccuracy-147"><span class="linenos">147</span></a>
</span><span id="SummarizationAccuracy-148"><a href="#SummarizationAccuracy-148"><span class="linenos">148</span></a>        <span class="k">return</span> <span class="p">[</span>
</span><span id="SummarizationAccuracy-149"><a href="#SummarizationAccuracy-149"><span class="linenos">149</span></a>            <span class="n">EvalScore</span><span class="p">(</span>
</span><span id="SummarizationAccuracy-150"><a href="#SummarizationAccuracy-150"><span class="linenos">150</span></a>                <span class="n">name</span><span class="o">=</span><span class="n">eval_score</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-151"><a href="#SummarizationAccuracy-151"><span class="linenos">151</span></a>                <span class="n">value</span><span class="o">=</span><span class="n">eval_fn</span><span class="p">(</span>
</span><span id="SummarizationAccuracy-152"><a href="#SummarizationAccuracy-152"><span class="linenos">152</span></a>                    <span class="n">target_output</span><span class="o">=</span><span class="n">target_output</span><span class="p">,</span> <span class="n">model_output</span><span class="o">=</span><span class="n">model_output</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_eval_algorithm_config</span>
</span><span id="SummarizationAccuracy-153"><a href="#SummarizationAccuracy-153"><span class="linenos">153</span></a>                <span class="p">),</span>
</span><span id="SummarizationAccuracy-154"><a href="#SummarizationAccuracy-154"><span class="linenos">154</span></a>            <span class="p">)</span>
</span><span id="SummarizationAccuracy-155"><a href="#SummarizationAccuracy-155"><span class="linenos">155</span></a>            <span class="k">for</span> <span class="n">eval_score</span><span class="p">,</span> <span class="n">eval_fn</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_score_eval_func_mapping</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
</span><span id="SummarizationAccuracy-156"><a href="#SummarizationAccuracy-156"><span class="linenos">156</span></a>        <span class="p">]</span>
</span><span id="SummarizationAccuracy-157"><a href="#SummarizationAccuracy-157"><span class="linenos">157</span></a>
</span><span id="SummarizationAccuracy-158"><a href="#SummarizationAccuracy-158"><span class="linenos">158</span></a>    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
</span><span id="SummarizationAccuracy-159"><a href="#SummarizationAccuracy-159"><span class="linenos">159</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-160"><a href="#SummarizationAccuracy-160"><span class="linenos">160</span></a>        <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ModelRunner</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-161"><a href="#SummarizationAccuracy-161"><span class="linenos">161</span></a>        <span class="n">dataset_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DataConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-162"><a href="#SummarizationAccuracy-162"><span class="linenos">162</span></a>        <span class="n">prompt_template</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-163"><a href="#SummarizationAccuracy-163"><span class="linenos">163</span></a>        <span class="n">save</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-164"><a href="#SummarizationAccuracy-164"><span class="linenos">164</span></a>        <span class="n">num_records</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-165"><a href="#SummarizationAccuracy-165"><span class="linenos">165</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">EvalOutput</span><span class="p">]:</span>
</span><span id="SummarizationAccuracy-166"><a href="#SummarizationAccuracy-166"><span class="linenos">166</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="SummarizationAccuracy-167"><a href="#SummarizationAccuracy-167"><span class="linenos">167</span></a><span class="sd">        Summarization Accuracy evaluate</span>
</span><span id="SummarizationAccuracy-168"><a href="#SummarizationAccuracy-168"><span class="linenos">168</span></a>
</span><span id="SummarizationAccuracy-169"><a href="#SummarizationAccuracy-169"><span class="linenos">169</span></a><span class="sd">        :param model: An instance of ModelRunner which is the model under evaluation</span>
</span><span id="SummarizationAccuracy-170"><a href="#SummarizationAccuracy-170"><span class="linenos">170</span></a><span class="sd">        :param dataset_config: Configures the single dataset used for evaluation. If not provided,</span>
</span><span id="SummarizationAccuracy-171"><a href="#SummarizationAccuracy-171"><span class="linenos">171</span></a><span class="sd">            evaluation will use all of it&#39;s supported built-in datasets</span>
</span><span id="SummarizationAccuracy-172"><a href="#SummarizationAccuracy-172"><span class="linenos">172</span></a><span class="sd">        :param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults</span>
</span><span id="SummarizationAccuracy-173"><a href="#SummarizationAccuracy-173"><span class="linenos">173</span></a><span class="sd">            will be used.</span>
</span><span id="SummarizationAccuracy-174"><a href="#SummarizationAccuracy-174"><span class="linenos">174</span></a><span class="sd">        :param save: If set to true, prompt responses and scores will be saved to file. The output is written to</span>
</span><span id="SummarizationAccuracy-175"><a href="#SummarizationAccuracy-175"><span class="linenos">175</span></a><span class="sd">                     EvalAlgorithmInterface.EVAL_RESULTS_PATH</span>
</span><span id="SummarizationAccuracy-176"><a href="#SummarizationAccuracy-176"><span class="linenos">176</span></a><span class="sd">        :param num_records: The number of records to be sampled randomly from the input dataset to perform the</span>
</span><span id="SummarizationAccuracy-177"><a href="#SummarizationAccuracy-177"><span class="linenos">177</span></a><span class="sd">                            evaluation</span>
</span><span id="SummarizationAccuracy-178"><a href="#SummarizationAccuracy-178"><span class="linenos">178</span></a>
</span><span id="SummarizationAccuracy-179"><a href="#SummarizationAccuracy-179"><span class="linenos">179</span></a><span class="sd">        :return: List of EvalOutput objects.</span>
</span><span id="SummarizationAccuracy-180"><a href="#SummarizationAccuracy-180"><span class="linenos">180</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="SummarizationAccuracy-181"><a href="#SummarizationAccuracy-181"><span class="linenos">181</span></a>        <span class="k">if</span> <span class="n">dataset_config</span><span class="p">:</span>
</span><span id="SummarizationAccuracy-182"><a href="#SummarizationAccuracy-182"><span class="linenos">182</span></a>            <span class="n">dataset_configs</span> <span class="o">=</span> <span class="p">[</span><span class="n">dataset_config</span><span class="p">]</span>
</span><span id="SummarizationAccuracy-183"><a href="#SummarizationAccuracy-183"><span class="linenos">183</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="SummarizationAccuracy-184"><a href="#SummarizationAccuracy-184"><span class="linenos">184</span></a>            <span class="n">dataset_configs</span> <span class="o">=</span> <span class="p">[</span><span class="n">DATASET_CONFIGS</span><span class="p">[</span><span class="n">dataset_name</span><span class="p">]</span> <span class="k">for</span> <span class="n">dataset_name</span> <span class="ow">in</span> <span class="n">EVAL_DATASETS</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_name</span><span class="p">]]</span>
</span><span id="SummarizationAccuracy-185"><a href="#SummarizationAccuracy-185"><span class="linenos">185</span></a>
</span><span id="SummarizationAccuracy-186"><a href="#SummarizationAccuracy-186"><span class="linenos">186</span></a>        <span class="n">eval_outputs</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="SummarizationAccuracy-187"><a href="#SummarizationAccuracy-187"><span class="linenos">187</span></a>        <span class="k">for</span> <span class="n">dataset_config</span> <span class="ow">in</span> <span class="n">dataset_configs</span><span class="p">:</span>
</span><span id="SummarizationAccuracy-188"><a href="#SummarizationAccuracy-188"><span class="linenos">188</span></a>            <span class="n">dataset</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">dataset_config</span><span class="p">,</span> <span class="n">num_records</span><span class="p">)</span>
</span><span id="SummarizationAccuracy-189"><a href="#SummarizationAccuracy-189"><span class="linenos">189</span></a>            <span class="n">validate_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">TARGET_OUTPUT_COLUMN_NAME</span><span class="p">,</span> <span class="n">MODEL_INPUT_COLUMN_NAME</span><span class="p">])</span>
</span><span id="SummarizationAccuracy-190"><a href="#SummarizationAccuracy-190"><span class="linenos">190</span></a>            <span class="n">dataset_prompt_template</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="SummarizationAccuracy-191"><a href="#SummarizationAccuracy-191"><span class="linenos">191</span></a>            <span class="k">if</span> <span class="n">MODEL_OUTPUT_COLUMN_NAME</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">columns</span><span class="p">():</span>
</span><span id="SummarizationAccuracy-192"><a href="#SummarizationAccuracy-192"><span class="linenos">192</span></a>                <span class="n">util</span><span class="o">.</span><span class="n">require</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;No ModelRunner provided. ModelRunner is required for inference on model_inputs&quot;</span><span class="p">)</span>
</span><span id="SummarizationAccuracy-193"><a href="#SummarizationAccuracy-193"><span class="linenos">193</span></a>                <span class="n">dataset_prompt_template</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="SummarizationAccuracy-194"><a href="#SummarizationAccuracy-194"><span class="linenos">194</span></a>                    <span class="n">get_default_prompt_template</span><span class="p">(</span><span class="n">dataset_config</span><span class="o">.</span><span class="n">dataset_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">prompt_template</span> <span class="k">else</span> <span class="n">prompt_template</span>
</span><span id="SummarizationAccuracy-195"><a href="#SummarizationAccuracy-195"><span class="linenos">195</span></a>                <span class="p">)</span>
</span><span id="SummarizationAccuracy-196"><a href="#SummarizationAccuracy-196"><span class="linenos">196</span></a>                <span class="n">dataset</span> <span class="o">=</span> <span class="n">generate_prompt_column_for_dataset</span><span class="p">(</span>
</span><span id="SummarizationAccuracy-197"><a href="#SummarizationAccuracy-197"><span class="linenos">197</span></a>                    <span class="n">dataset_prompt_template</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">MODEL_INPUT_COLUMN_NAME</span><span class="p">,</span> <span class="n">PROMPT_COLUMN_NAME</span>
</span><span id="SummarizationAccuracy-198"><a href="#SummarizationAccuracy-198"><span class="linenos">198</span></a>                <span class="p">)</span>
</span><span id="SummarizationAccuracy-199"><a href="#SummarizationAccuracy-199"><span class="linenos">199</span></a>                <span class="k">assert</span> <span class="n">model</span>  <span class="c1"># to satisfy mypy</span>
</span><span id="SummarizationAccuracy-200"><a href="#SummarizationAccuracy-200"><span class="linenos">200</span></a>                <span class="n">dataset</span> <span class="o">=</span> <span class="n">generate_model_predict_response_for_dataset</span><span class="p">(</span>
</span><span id="SummarizationAccuracy-201"><a href="#SummarizationAccuracy-201"><span class="linenos">201</span></a>                    <span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">PROMPT_COLUMN_NAME</span><span class="p">,</span> <span class="n">MODEL_OUTPUT_COLUMN_NAME</span>
</span><span id="SummarizationAccuracy-202"><a href="#SummarizationAccuracy-202"><span class="linenos">202</span></a>                <span class="p">)</span>
</span><span id="SummarizationAccuracy-203"><a href="#SummarizationAccuracy-203"><span class="linenos">203</span></a>            <span class="k">with</span> <span class="n">timed_block</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Computing score and aggregation on dataset </span><span class="si">{</span><span class="n">dataset_config</span><span class="o">.</span><span class="n">dataset_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">logger</span><span class="p">):</span>
</span><span id="SummarizationAccuracy-204"><a href="#SummarizationAccuracy-204"><span class="linenos">204</span></a>                <span class="k">for</span> <span class="n">eval_score</span><span class="p">,</span> <span class="n">eval_func</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_score_eval_func_mapping</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="SummarizationAccuracy-205"><a href="#SummarizationAccuracy-205"><span class="linenos">205</span></a>                    <span class="n">dataset</span> <span class="o">=</span> <span class="n">add_score_to_dataset</span><span class="p">(</span>
</span><span id="SummarizationAccuracy-206"><a href="#SummarizationAccuracy-206"><span class="linenos">206</span></a>                        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-207"><a href="#SummarizationAccuracy-207"><span class="linenos">207</span></a>                        <span class="n">eval_func</span><span class="o">=</span><span class="n">eval_func</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-208"><a href="#SummarizationAccuracy-208"><span class="linenos">208</span></a>                        <span class="n">score_column_name</span><span class="o">=</span><span class="n">eval_score</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-209"><a href="#SummarizationAccuracy-209"><span class="linenos">209</span></a>                        <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_eval_algorithm_config</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-210"><a href="#SummarizationAccuracy-210"><span class="linenos">210</span></a>                    <span class="p">)</span>
</span><span id="SummarizationAccuracy-211"><a href="#SummarizationAccuracy-211"><span class="linenos">211</span></a>
</span><span id="SummarizationAccuracy-212"><a href="#SummarizationAccuracy-212"><span class="linenos">212</span></a>                <span class="n">dataset_scores</span><span class="p">,</span> <span class="n">category_scores</span> <span class="o">=</span> <span class="n">aggregate_evaluation_scores</span><span class="p">(</span>
</span><span id="SummarizationAccuracy-213"><a href="#SummarizationAccuracy-213"><span class="linenos">213</span></a>                    <span class="n">dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">METEOR_SCORE</span><span class="p">,</span> <span class="n">ROUGE_SCORE</span><span class="p">,</span> <span class="n">BERT_SCORE</span><span class="p">],</span> <span class="n">agg_method</span><span class="o">=</span><span class="n">MEAN</span>
</span><span id="SummarizationAccuracy-214"><a href="#SummarizationAccuracy-214"><span class="linenos">214</span></a>                <span class="p">)</span>
</span><span id="SummarizationAccuracy-215"><a href="#SummarizationAccuracy-215"><span class="linenos">215</span></a>                <span class="n">eval_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="SummarizationAccuracy-216"><a href="#SummarizationAccuracy-216"><span class="linenos">216</span></a>                    <span class="n">EvalOutput</span><span class="p">(</span>
</span><span id="SummarizationAccuracy-217"><a href="#SummarizationAccuracy-217"><span class="linenos">217</span></a>                        <span class="n">eval_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_name</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-218"><a href="#SummarizationAccuracy-218"><span class="linenos">218</span></a>                        <span class="n">dataset_name</span><span class="o">=</span><span class="n">dataset_config</span><span class="o">.</span><span class="n">dataset_name</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-219"><a href="#SummarizationAccuracy-219"><span class="linenos">219</span></a>                        <span class="n">prompt_template</span><span class="o">=</span><span class="n">dataset_prompt_template</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-220"><a href="#SummarizationAccuracy-220"><span class="linenos">220</span></a>                        <span class="n">dataset_scores</span><span class="o">=</span><span class="n">dataset_scores</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-221"><a href="#SummarizationAccuracy-221"><span class="linenos">221</span></a>                        <span class="n">category_scores</span><span class="o">=</span><span class="n">category_scores</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-222"><a href="#SummarizationAccuracy-222"><span class="linenos">222</span></a>                        <span class="n">output_path</span><span class="o">=</span><span class="n">generate_output_dataset_path</span><span class="p">(</span>
</span><span id="SummarizationAccuracy-223"><a href="#SummarizationAccuracy-223"><span class="linenos">223</span></a>                            <span class="n">path_to_parent_dir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_eval_results_path</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-224"><a href="#SummarizationAccuracy-224"><span class="linenos">224</span></a>                            <span class="n">eval_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_name</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-225"><a href="#SummarizationAccuracy-225"><span class="linenos">225</span></a>                            <span class="n">dataset_name</span><span class="o">=</span><span class="n">dataset_config</span><span class="o">.</span><span class="n">dataset_name</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-226"><a href="#SummarizationAccuracy-226"><span class="linenos">226</span></a>                        <span class="p">),</span>
</span><span id="SummarizationAccuracy-227"><a href="#SummarizationAccuracy-227"><span class="linenos">227</span></a>                    <span class="p">)</span>
</span><span id="SummarizationAccuracy-228"><a href="#SummarizationAccuracy-228"><span class="linenos">228</span></a>                <span class="p">)</span>
</span><span id="SummarizationAccuracy-229"><a href="#SummarizationAccuracy-229"><span class="linenos">229</span></a>            <span class="k">if</span> <span class="n">save</span><span class="p">:</span>
</span><span id="SummarizationAccuracy-230"><a href="#SummarizationAccuracy-230"><span class="linenos">230</span></a>                <span class="n">save_dataset</span><span class="p">(</span>
</span><span id="SummarizationAccuracy-231"><a href="#SummarizationAccuracy-231"><span class="linenos">231</span></a>                    <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-232"><a href="#SummarizationAccuracy-232"><span class="linenos">232</span></a>                    <span class="n">score_names</span><span class="o">=</span><span class="p">[</span><span class="n">METEOR_SCORE</span><span class="p">,</span> <span class="n">ROUGE_SCORE</span><span class="p">,</span> <span class="n">BERT_SCORE</span><span class="p">],</span>
</span><span id="SummarizationAccuracy-233"><a href="#SummarizationAccuracy-233"><span class="linenos">233</span></a>                    <span class="n">path</span><span class="o">=</span><span class="n">generate_output_dataset_path</span><span class="p">(</span>
</span><span id="SummarizationAccuracy-234"><a href="#SummarizationAccuracy-234"><span class="linenos">234</span></a>                        <span class="n">path_to_parent_dir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_eval_results_path</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-235"><a href="#SummarizationAccuracy-235"><span class="linenos">235</span></a>                        <span class="n">eval_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_name</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-236"><a href="#SummarizationAccuracy-236"><span class="linenos">236</span></a>                        <span class="n">dataset_name</span><span class="o">=</span><span class="n">dataset_config</span><span class="o">.</span><span class="n">dataset_name</span><span class="p">,</span>
</span><span id="SummarizationAccuracy-237"><a href="#SummarizationAccuracy-237"><span class="linenos">237</span></a>                    <span class="p">),</span>
</span><span id="SummarizationAccuracy-238"><a href="#SummarizationAccuracy-238"><span class="linenos">238</span></a>                <span class="p">)</span>
</span><span id="SummarizationAccuracy-239"><a href="#SummarizationAccuracy-239"><span class="linenos">239</span></a>
</span><span id="SummarizationAccuracy-240"><a href="#SummarizationAccuracy-240"><span class="linenos">240</span></a>        <span class="k">return</span> <span class="n">eval_outputs</span>
</span></pre></div>


            <div class="docstring"><p>Summarization Accuracy Eval algorithm</p>

<p>The aim of this eval algo is to evaluate how well a model can summarise text.
The algo uses a reference summary to compare the output generated by the model and a series
of quality metrics based on overlapping between words (ROUGE and METEOR) and similarity scores (bert scores)</p>
</div>


                            <div id="SummarizationAccuracy.__init__" class="classattr">
                                        <input id="SummarizationAccuracy.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">SummarizationAccuracy</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">eval_algorithm_config</span><span class="p">:</span> <span class="n"><a href="#SummarizationAccuracyConfig">SummarizationAccuracyConfig</a></span> <span class="o">=</span> <span class="n">SummarizationAccuracyConfig</span><span class="p">(</span><span class="n">rouge_type</span><span class="o">=</span><span class="s1">&#39;rouge2&#39;</span><span class="p">,</span> <span class="n">use_stemmer_for_rouge</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">model_type_for_bertscore</span><span class="o">=</span><span class="s1">&#39;microsoft/deberta-xlarge-mnli&#39;</span><span class="p">)</span></span>)</span>

                <label class="view-source-button" for="SummarizationAccuracy.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SummarizationAccuracy.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SummarizationAccuracy.__init__-104"><a href="#SummarizationAccuracy.__init__-104"><span class="linenos">104</span></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eval_algorithm_config</span><span class="p">:</span> <span class="n">SummarizationAccuracyConfig</span> <span class="o">=</span> <span class="n">SummarizationAccuracyConfig</span><span class="p">()):</span>
</span><span id="SummarizationAccuracy.__init__-105"><a href="#SummarizationAccuracy.__init__-105"><span class="linenos">105</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Default constructor</span>
</span><span id="SummarizationAccuracy.__init__-106"><a href="#SummarizationAccuracy.__init__-106"><span class="linenos">106</span></a>
</span><span id="SummarizationAccuracy.__init__-107"><a href="#SummarizationAccuracy.__init__-107"><span class="linenos">107</span></a><span class="sd">        :param eval_algorithm_config: Summarization Accuracy eval algorithm config.</span>
</span><span id="SummarizationAccuracy.__init__-108"><a href="#SummarizationAccuracy.__init__-108"><span class="linenos">108</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="SummarizationAccuracy.__init__-109"><a href="#SummarizationAccuracy.__init__-109"><span class="linenos">109</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">eval_algorithm_config</span><span class="p">)</span>
</span><span id="SummarizationAccuracy.__init__-110"><a href="#SummarizationAccuracy.__init__-110"><span class="linenos">110</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">eval_name</span> <span class="o">=</span> <span class="n">EvalAlgorithm</span><span class="o">.</span><span class="n">SUMMARIZATION_ACCURACY</span><span class="o">.</span><span class="n">value</span>
</span><span id="SummarizationAccuracy.__init__-111"><a href="#SummarizationAccuracy.__init__-111"><span class="linenos">111</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_eval_algorithm_config</span> <span class="o">=</span> <span class="n">eval_algorithm_config</span>
</span><span id="SummarizationAccuracy.__init__-112"><a href="#SummarizationAccuracy.__init__-112"><span class="linenos">112</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_load_eval_helpers</span><span class="p">()</span>
</span><span id="SummarizationAccuracy.__init__-113"><a href="#SummarizationAccuracy.__init__-113"><span class="linenos">113</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_score_eval_func_mapping</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="SummarizationAccuracy.__init__-114"><a href="#SummarizationAccuracy.__init__-114"><span class="linenos">114</span></a>            <span class="n">METEOR_SCORE</span><span class="p">:</span> <span class="n">get_meteor_score</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.__init__-115"><a href="#SummarizationAccuracy.__init__-115"><span class="linenos">115</span></a>            <span class="n">ROUGE_SCORE</span><span class="p">:</span> <span class="n">get_rouge_score</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.__init__-116"><a href="#SummarizationAccuracy.__init__-116"><span class="linenos">116</span></a>            <span class="n">BERT_SCORE</span><span class="p">:</span> <span class="n">get_bert_score</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.__init__-117"><a href="#SummarizationAccuracy.__init__-117"><span class="linenos">117</span></a>        <span class="p">}</span>
</span></pre></div>


            <div class="docstring"><p>Default constructor</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>eval_algorithm_config</strong>:  Summarization Accuracy eval algorithm config.</li>
</ul>
</div>


                            </div>
                            <div id="SummarizationAccuracy.eval_name" class="classattr">
                                <div class="attr variable">
            <span class="name">eval_name</span>

        
    </div>
    <a class="headerlink" href="#SummarizationAccuracy.eval_name"></a>
    
    

                            </div>
                            <div id="SummarizationAccuracy.evaluate_sample" class="classattr">
                                        <input id="SummarizationAccuracy.evaluate_sample-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">evaluate_sample</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">target_output</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">model_output</span><span class="p">:</span> <span class="nb">str</span></span><span class="return-annotation">) -> <span class="n">List</span><span class="p">[</span><span class="n">amazon_fmeval</span><span class="o">.</span><span class="n">eval_algorithms</span><span class="o">.</span><span class="n">EvalScore</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="SummarizationAccuracy.evaluate_sample-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SummarizationAccuracy.evaluate_sample"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SummarizationAccuracy.evaluate_sample-131"><a href="#SummarizationAccuracy.evaluate_sample-131"><span class="linenos">131</span></a>    <span class="k">def</span> <span class="nf">evaluate_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">EvalScore</span><span class="p">]:</span>  <span class="c1"># type: ignore[override]</span>
</span><span id="SummarizationAccuracy.evaluate_sample-132"><a href="#SummarizationAccuracy.evaluate_sample-132"><span class="linenos">132</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="SummarizationAccuracy.evaluate_sample-133"><a href="#SummarizationAccuracy.evaluate_sample-133"><span class="linenos">133</span></a><span class="sd">        Summarization Accuracy evaluate sample.</span>
</span><span id="SummarizationAccuracy.evaluate_sample-134"><a href="#SummarizationAccuracy.evaluate_sample-134"><span class="linenos">134</span></a>
</span><span id="SummarizationAccuracy.evaluate_sample-135"><a href="#SummarizationAccuracy.evaluate_sample-135"><span class="linenos">135</span></a><span class="sd">        :param target_output: The expected responses from the model</span>
</span><span id="SummarizationAccuracy.evaluate_sample-136"><a href="#SummarizationAccuracy.evaluate_sample-136"><span class="linenos">136</span></a><span class="sd">        :param model_output: The output of a model that we want to evaluate.</span>
</span><span id="SummarizationAccuracy.evaluate_sample-137"><a href="#SummarizationAccuracy.evaluate_sample-137"><span class="linenos">137</span></a><span class="sd">        :return: list of EvalScore objects</span>
</span><span id="SummarizationAccuracy.evaluate_sample-138"><a href="#SummarizationAccuracy.evaluate_sample-138"><span class="linenos">138</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="SummarizationAccuracy.evaluate_sample-139"><a href="#SummarizationAccuracy.evaluate_sample-139"><span class="linenos">139</span></a>        <span class="k">if</span> <span class="n">target_output</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="SummarizationAccuracy.evaluate_sample-140"><a href="#SummarizationAccuracy.evaluate_sample-140"><span class="linenos">140</span></a>            <span class="k">raise</span> <span class="n">EvalAlgorithmClientError</span><span class="p">(</span>
</span><span id="SummarizationAccuracy.evaluate_sample-141"><a href="#SummarizationAccuracy.evaluate_sample-141"><span class="linenos">141</span></a>                <span class="s2">&quot;Missing required input: target_output, for Summarization Accuracy evaluate_sample&quot;</span>
</span><span id="SummarizationAccuracy.evaluate_sample-142"><a href="#SummarizationAccuracy.evaluate_sample-142"><span class="linenos">142</span></a>            <span class="p">)</span>
</span><span id="SummarizationAccuracy.evaluate_sample-143"><a href="#SummarizationAccuracy.evaluate_sample-143"><span class="linenos">143</span></a>        <span class="k">if</span> <span class="n">model_output</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="SummarizationAccuracy.evaluate_sample-144"><a href="#SummarizationAccuracy.evaluate_sample-144"><span class="linenos">144</span></a>            <span class="k">raise</span> <span class="n">EvalAlgorithmClientError</span><span class="p">(</span>
</span><span id="SummarizationAccuracy.evaluate_sample-145"><a href="#SummarizationAccuracy.evaluate_sample-145"><span class="linenos">145</span></a>                <span class="s2">&quot;Missing required input: model_output, for Summarization Accuracy evaluate_sample&quot;</span>
</span><span id="SummarizationAccuracy.evaluate_sample-146"><a href="#SummarizationAccuracy.evaluate_sample-146"><span class="linenos">146</span></a>            <span class="p">)</span>
</span><span id="SummarizationAccuracy.evaluate_sample-147"><a href="#SummarizationAccuracy.evaluate_sample-147"><span class="linenos">147</span></a>
</span><span id="SummarizationAccuracy.evaluate_sample-148"><a href="#SummarizationAccuracy.evaluate_sample-148"><span class="linenos">148</span></a>        <span class="k">return</span> <span class="p">[</span>
</span><span id="SummarizationAccuracy.evaluate_sample-149"><a href="#SummarizationAccuracy.evaluate_sample-149"><span class="linenos">149</span></a>            <span class="n">EvalScore</span><span class="p">(</span>
</span><span id="SummarizationAccuracy.evaluate_sample-150"><a href="#SummarizationAccuracy.evaluate_sample-150"><span class="linenos">150</span></a>                <span class="n">name</span><span class="o">=</span><span class="n">eval_score</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate_sample-151"><a href="#SummarizationAccuracy.evaluate_sample-151"><span class="linenos">151</span></a>                <span class="n">value</span><span class="o">=</span><span class="n">eval_fn</span><span class="p">(</span>
</span><span id="SummarizationAccuracy.evaluate_sample-152"><a href="#SummarizationAccuracy.evaluate_sample-152"><span class="linenos">152</span></a>                    <span class="n">target_output</span><span class="o">=</span><span class="n">target_output</span><span class="p">,</span> <span class="n">model_output</span><span class="o">=</span><span class="n">model_output</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_eval_algorithm_config</span>
</span><span id="SummarizationAccuracy.evaluate_sample-153"><a href="#SummarizationAccuracy.evaluate_sample-153"><span class="linenos">153</span></a>                <span class="p">),</span>
</span><span id="SummarizationAccuracy.evaluate_sample-154"><a href="#SummarizationAccuracy.evaluate_sample-154"><span class="linenos">154</span></a>            <span class="p">)</span>
</span><span id="SummarizationAccuracy.evaluate_sample-155"><a href="#SummarizationAccuracy.evaluate_sample-155"><span class="linenos">155</span></a>            <span class="k">for</span> <span class="n">eval_score</span><span class="p">,</span> <span class="n">eval_fn</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_score_eval_func_mapping</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
</span><span id="SummarizationAccuracy.evaluate_sample-156"><a href="#SummarizationAccuracy.evaluate_sample-156"><span class="linenos">156</span></a>        <span class="p">]</span>
</span></pre></div>


            <div class="docstring"><p>Summarization Accuracy evaluate sample.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>target_output</strong>:  The expected responses from the model</li>
<li><strong>model_output</strong>:  The output of a model that we want to evaluate.</li>
</ul>

<h6 id="returns">Returns</h6>

<blockquote>
  <p>list of EvalScore objects</p>
</blockquote>
</div>


                            </div>
                            <div id="SummarizationAccuracy.evaluate" class="classattr">
                                        <input id="SummarizationAccuracy.evaluate-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">evaluate</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">amazon_fmeval</span><span class="o">.</span><span class="n">model_runners</span><span class="o">.</span><span class="n">model_runner</span><span class="o">.</span><span class="n">ModelRunner</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">dataset_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">amazon_fmeval</span><span class="o">.</span><span class="n">data_loaders</span><span class="o">.</span><span class="n">data_config</span><span class="o">.</span><span class="n">DataConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">prompt_template</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">save</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">num_records</span><span class="o">=</span><span class="mi">100</span></span><span class="return-annotation">) -> <span class="n">List</span><span class="p">[</span><span class="n">amazon_fmeval</span><span class="o">.</span><span class="n">eval_algorithms</span><span class="o">.</span><span class="n">EvalOutput</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="SummarizationAccuracy.evaluate-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SummarizationAccuracy.evaluate"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SummarizationAccuracy.evaluate-158"><a href="#SummarizationAccuracy.evaluate-158"><span class="linenos">158</span></a>    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
</span><span id="SummarizationAccuracy.evaluate-159"><a href="#SummarizationAccuracy.evaluate-159"><span class="linenos">159</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-160"><a href="#SummarizationAccuracy.evaluate-160"><span class="linenos">160</span></a>        <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ModelRunner</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-161"><a href="#SummarizationAccuracy.evaluate-161"><span class="linenos">161</span></a>        <span class="n">dataset_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DataConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-162"><a href="#SummarizationAccuracy.evaluate-162"><span class="linenos">162</span></a>        <span class="n">prompt_template</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-163"><a href="#SummarizationAccuracy.evaluate-163"><span class="linenos">163</span></a>        <span class="n">save</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-164"><a href="#SummarizationAccuracy.evaluate-164"><span class="linenos">164</span></a>        <span class="n">num_records</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-165"><a href="#SummarizationAccuracy.evaluate-165"><span class="linenos">165</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">EvalOutput</span><span class="p">]:</span>
</span><span id="SummarizationAccuracy.evaluate-166"><a href="#SummarizationAccuracy.evaluate-166"><span class="linenos">166</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="SummarizationAccuracy.evaluate-167"><a href="#SummarizationAccuracy.evaluate-167"><span class="linenos">167</span></a><span class="sd">        Summarization Accuracy evaluate</span>
</span><span id="SummarizationAccuracy.evaluate-168"><a href="#SummarizationAccuracy.evaluate-168"><span class="linenos">168</span></a>
</span><span id="SummarizationAccuracy.evaluate-169"><a href="#SummarizationAccuracy.evaluate-169"><span class="linenos">169</span></a><span class="sd">        :param model: An instance of ModelRunner which is the model under evaluation</span>
</span><span id="SummarizationAccuracy.evaluate-170"><a href="#SummarizationAccuracy.evaluate-170"><span class="linenos">170</span></a><span class="sd">        :param dataset_config: Configures the single dataset used for evaluation. If not provided,</span>
</span><span id="SummarizationAccuracy.evaluate-171"><a href="#SummarizationAccuracy.evaluate-171"><span class="linenos">171</span></a><span class="sd">            evaluation will use all of it&#39;s supported built-in datasets</span>
</span><span id="SummarizationAccuracy.evaluate-172"><a href="#SummarizationAccuracy.evaluate-172"><span class="linenos">172</span></a><span class="sd">        :param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults</span>
</span><span id="SummarizationAccuracy.evaluate-173"><a href="#SummarizationAccuracy.evaluate-173"><span class="linenos">173</span></a><span class="sd">            will be used.</span>
</span><span id="SummarizationAccuracy.evaluate-174"><a href="#SummarizationAccuracy.evaluate-174"><span class="linenos">174</span></a><span class="sd">        :param save: If set to true, prompt responses and scores will be saved to file. The output is written to</span>
</span><span id="SummarizationAccuracy.evaluate-175"><a href="#SummarizationAccuracy.evaluate-175"><span class="linenos">175</span></a><span class="sd">                     EvalAlgorithmInterface.EVAL_RESULTS_PATH</span>
</span><span id="SummarizationAccuracy.evaluate-176"><a href="#SummarizationAccuracy.evaluate-176"><span class="linenos">176</span></a><span class="sd">        :param num_records: The number of records to be sampled randomly from the input dataset to perform the</span>
</span><span id="SummarizationAccuracy.evaluate-177"><a href="#SummarizationAccuracy.evaluate-177"><span class="linenos">177</span></a><span class="sd">                            evaluation</span>
</span><span id="SummarizationAccuracy.evaluate-178"><a href="#SummarizationAccuracy.evaluate-178"><span class="linenos">178</span></a>
</span><span id="SummarizationAccuracy.evaluate-179"><a href="#SummarizationAccuracy.evaluate-179"><span class="linenos">179</span></a><span class="sd">        :return: List of EvalOutput objects.</span>
</span><span id="SummarizationAccuracy.evaluate-180"><a href="#SummarizationAccuracy.evaluate-180"><span class="linenos">180</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="SummarizationAccuracy.evaluate-181"><a href="#SummarizationAccuracy.evaluate-181"><span class="linenos">181</span></a>        <span class="k">if</span> <span class="n">dataset_config</span><span class="p">:</span>
</span><span id="SummarizationAccuracy.evaluate-182"><a href="#SummarizationAccuracy.evaluate-182"><span class="linenos">182</span></a>            <span class="n">dataset_configs</span> <span class="o">=</span> <span class="p">[</span><span class="n">dataset_config</span><span class="p">]</span>
</span><span id="SummarizationAccuracy.evaluate-183"><a href="#SummarizationAccuracy.evaluate-183"><span class="linenos">183</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="SummarizationAccuracy.evaluate-184"><a href="#SummarizationAccuracy.evaluate-184"><span class="linenos">184</span></a>            <span class="n">dataset_configs</span> <span class="o">=</span> <span class="p">[</span><span class="n">DATASET_CONFIGS</span><span class="p">[</span><span class="n">dataset_name</span><span class="p">]</span> <span class="k">for</span> <span class="n">dataset_name</span> <span class="ow">in</span> <span class="n">EVAL_DATASETS</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_name</span><span class="p">]]</span>
</span><span id="SummarizationAccuracy.evaluate-185"><a href="#SummarizationAccuracy.evaluate-185"><span class="linenos">185</span></a>
</span><span id="SummarizationAccuracy.evaluate-186"><a href="#SummarizationAccuracy.evaluate-186"><span class="linenos">186</span></a>        <span class="n">eval_outputs</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="SummarizationAccuracy.evaluate-187"><a href="#SummarizationAccuracy.evaluate-187"><span class="linenos">187</span></a>        <span class="k">for</span> <span class="n">dataset_config</span> <span class="ow">in</span> <span class="n">dataset_configs</span><span class="p">:</span>
</span><span id="SummarizationAccuracy.evaluate-188"><a href="#SummarizationAccuracy.evaluate-188"><span class="linenos">188</span></a>            <span class="n">dataset</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">dataset_config</span><span class="p">,</span> <span class="n">num_records</span><span class="p">)</span>
</span><span id="SummarizationAccuracy.evaluate-189"><a href="#SummarizationAccuracy.evaluate-189"><span class="linenos">189</span></a>            <span class="n">validate_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">TARGET_OUTPUT_COLUMN_NAME</span><span class="p">,</span> <span class="n">MODEL_INPUT_COLUMN_NAME</span><span class="p">])</span>
</span><span id="SummarizationAccuracy.evaluate-190"><a href="#SummarizationAccuracy.evaluate-190"><span class="linenos">190</span></a>            <span class="n">dataset_prompt_template</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="SummarizationAccuracy.evaluate-191"><a href="#SummarizationAccuracy.evaluate-191"><span class="linenos">191</span></a>            <span class="k">if</span> <span class="n">MODEL_OUTPUT_COLUMN_NAME</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">columns</span><span class="p">():</span>
</span><span id="SummarizationAccuracy.evaluate-192"><a href="#SummarizationAccuracy.evaluate-192"><span class="linenos">192</span></a>                <span class="n">util</span><span class="o">.</span><span class="n">require</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;No ModelRunner provided. ModelRunner is required for inference on model_inputs&quot;</span><span class="p">)</span>
</span><span id="SummarizationAccuracy.evaluate-193"><a href="#SummarizationAccuracy.evaluate-193"><span class="linenos">193</span></a>                <span class="n">dataset_prompt_template</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="SummarizationAccuracy.evaluate-194"><a href="#SummarizationAccuracy.evaluate-194"><span class="linenos">194</span></a>                    <span class="n">get_default_prompt_template</span><span class="p">(</span><span class="n">dataset_config</span><span class="o">.</span><span class="n">dataset_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">prompt_template</span> <span class="k">else</span> <span class="n">prompt_template</span>
</span><span id="SummarizationAccuracy.evaluate-195"><a href="#SummarizationAccuracy.evaluate-195"><span class="linenos">195</span></a>                <span class="p">)</span>
</span><span id="SummarizationAccuracy.evaluate-196"><a href="#SummarizationAccuracy.evaluate-196"><span class="linenos">196</span></a>                <span class="n">dataset</span> <span class="o">=</span> <span class="n">generate_prompt_column_for_dataset</span><span class="p">(</span>
</span><span id="SummarizationAccuracy.evaluate-197"><a href="#SummarizationAccuracy.evaluate-197"><span class="linenos">197</span></a>                    <span class="n">dataset_prompt_template</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">MODEL_INPUT_COLUMN_NAME</span><span class="p">,</span> <span class="n">PROMPT_COLUMN_NAME</span>
</span><span id="SummarizationAccuracy.evaluate-198"><a href="#SummarizationAccuracy.evaluate-198"><span class="linenos">198</span></a>                <span class="p">)</span>
</span><span id="SummarizationAccuracy.evaluate-199"><a href="#SummarizationAccuracy.evaluate-199"><span class="linenos">199</span></a>                <span class="k">assert</span> <span class="n">model</span>  <span class="c1"># to satisfy mypy</span>
</span><span id="SummarizationAccuracy.evaluate-200"><a href="#SummarizationAccuracy.evaluate-200"><span class="linenos">200</span></a>                <span class="n">dataset</span> <span class="o">=</span> <span class="n">generate_model_predict_response_for_dataset</span><span class="p">(</span>
</span><span id="SummarizationAccuracy.evaluate-201"><a href="#SummarizationAccuracy.evaluate-201"><span class="linenos">201</span></a>                    <span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">PROMPT_COLUMN_NAME</span><span class="p">,</span> <span class="n">MODEL_OUTPUT_COLUMN_NAME</span>
</span><span id="SummarizationAccuracy.evaluate-202"><a href="#SummarizationAccuracy.evaluate-202"><span class="linenos">202</span></a>                <span class="p">)</span>
</span><span id="SummarizationAccuracy.evaluate-203"><a href="#SummarizationAccuracy.evaluate-203"><span class="linenos">203</span></a>            <span class="k">with</span> <span class="n">timed_block</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Computing score and aggregation on dataset </span><span class="si">{</span><span class="n">dataset_config</span><span class="o">.</span><span class="n">dataset_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">logger</span><span class="p">):</span>
</span><span id="SummarizationAccuracy.evaluate-204"><a href="#SummarizationAccuracy.evaluate-204"><span class="linenos">204</span></a>                <span class="k">for</span> <span class="n">eval_score</span><span class="p">,</span> <span class="n">eval_func</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_score_eval_func_mapping</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="SummarizationAccuracy.evaluate-205"><a href="#SummarizationAccuracy.evaluate-205"><span class="linenos">205</span></a>                    <span class="n">dataset</span> <span class="o">=</span> <span class="n">add_score_to_dataset</span><span class="p">(</span>
</span><span id="SummarizationAccuracy.evaluate-206"><a href="#SummarizationAccuracy.evaluate-206"><span class="linenos">206</span></a>                        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-207"><a href="#SummarizationAccuracy.evaluate-207"><span class="linenos">207</span></a>                        <span class="n">eval_func</span><span class="o">=</span><span class="n">eval_func</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-208"><a href="#SummarizationAccuracy.evaluate-208"><span class="linenos">208</span></a>                        <span class="n">score_column_name</span><span class="o">=</span><span class="n">eval_score</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-209"><a href="#SummarizationAccuracy.evaluate-209"><span class="linenos">209</span></a>                        <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_eval_algorithm_config</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-210"><a href="#SummarizationAccuracy.evaluate-210"><span class="linenos">210</span></a>                    <span class="p">)</span>
</span><span id="SummarizationAccuracy.evaluate-211"><a href="#SummarizationAccuracy.evaluate-211"><span class="linenos">211</span></a>
</span><span id="SummarizationAccuracy.evaluate-212"><a href="#SummarizationAccuracy.evaluate-212"><span class="linenos">212</span></a>                <span class="n">dataset_scores</span><span class="p">,</span> <span class="n">category_scores</span> <span class="o">=</span> <span class="n">aggregate_evaluation_scores</span><span class="p">(</span>
</span><span id="SummarizationAccuracy.evaluate-213"><a href="#SummarizationAccuracy.evaluate-213"><span class="linenos">213</span></a>                    <span class="n">dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">METEOR_SCORE</span><span class="p">,</span> <span class="n">ROUGE_SCORE</span><span class="p">,</span> <span class="n">BERT_SCORE</span><span class="p">],</span> <span class="n">agg_method</span><span class="o">=</span><span class="n">MEAN</span>
</span><span id="SummarizationAccuracy.evaluate-214"><a href="#SummarizationAccuracy.evaluate-214"><span class="linenos">214</span></a>                <span class="p">)</span>
</span><span id="SummarizationAccuracy.evaluate-215"><a href="#SummarizationAccuracy.evaluate-215"><span class="linenos">215</span></a>                <span class="n">eval_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="SummarizationAccuracy.evaluate-216"><a href="#SummarizationAccuracy.evaluate-216"><span class="linenos">216</span></a>                    <span class="n">EvalOutput</span><span class="p">(</span>
</span><span id="SummarizationAccuracy.evaluate-217"><a href="#SummarizationAccuracy.evaluate-217"><span class="linenos">217</span></a>                        <span class="n">eval_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_name</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-218"><a href="#SummarizationAccuracy.evaluate-218"><span class="linenos">218</span></a>                        <span class="n">dataset_name</span><span class="o">=</span><span class="n">dataset_config</span><span class="o">.</span><span class="n">dataset_name</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-219"><a href="#SummarizationAccuracy.evaluate-219"><span class="linenos">219</span></a>                        <span class="n">prompt_template</span><span class="o">=</span><span class="n">dataset_prompt_template</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-220"><a href="#SummarizationAccuracy.evaluate-220"><span class="linenos">220</span></a>                        <span class="n">dataset_scores</span><span class="o">=</span><span class="n">dataset_scores</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-221"><a href="#SummarizationAccuracy.evaluate-221"><span class="linenos">221</span></a>                        <span class="n">category_scores</span><span class="o">=</span><span class="n">category_scores</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-222"><a href="#SummarizationAccuracy.evaluate-222"><span class="linenos">222</span></a>                        <span class="n">output_path</span><span class="o">=</span><span class="n">generate_output_dataset_path</span><span class="p">(</span>
</span><span id="SummarizationAccuracy.evaluate-223"><a href="#SummarizationAccuracy.evaluate-223"><span class="linenos">223</span></a>                            <span class="n">path_to_parent_dir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_eval_results_path</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-224"><a href="#SummarizationAccuracy.evaluate-224"><span class="linenos">224</span></a>                            <span class="n">eval_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_name</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-225"><a href="#SummarizationAccuracy.evaluate-225"><span class="linenos">225</span></a>                            <span class="n">dataset_name</span><span class="o">=</span><span class="n">dataset_config</span><span class="o">.</span><span class="n">dataset_name</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-226"><a href="#SummarizationAccuracy.evaluate-226"><span class="linenos">226</span></a>                        <span class="p">),</span>
</span><span id="SummarizationAccuracy.evaluate-227"><a href="#SummarizationAccuracy.evaluate-227"><span class="linenos">227</span></a>                    <span class="p">)</span>
</span><span id="SummarizationAccuracy.evaluate-228"><a href="#SummarizationAccuracy.evaluate-228"><span class="linenos">228</span></a>                <span class="p">)</span>
</span><span id="SummarizationAccuracy.evaluate-229"><a href="#SummarizationAccuracy.evaluate-229"><span class="linenos">229</span></a>            <span class="k">if</span> <span class="n">save</span><span class="p">:</span>
</span><span id="SummarizationAccuracy.evaluate-230"><a href="#SummarizationAccuracy.evaluate-230"><span class="linenos">230</span></a>                <span class="n">save_dataset</span><span class="p">(</span>
</span><span id="SummarizationAccuracy.evaluate-231"><a href="#SummarizationAccuracy.evaluate-231"><span class="linenos">231</span></a>                    <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-232"><a href="#SummarizationAccuracy.evaluate-232"><span class="linenos">232</span></a>                    <span class="n">score_names</span><span class="o">=</span><span class="p">[</span><span class="n">METEOR_SCORE</span><span class="p">,</span> <span class="n">ROUGE_SCORE</span><span class="p">,</span> <span class="n">BERT_SCORE</span><span class="p">],</span>
</span><span id="SummarizationAccuracy.evaluate-233"><a href="#SummarizationAccuracy.evaluate-233"><span class="linenos">233</span></a>                    <span class="n">path</span><span class="o">=</span><span class="n">generate_output_dataset_path</span><span class="p">(</span>
</span><span id="SummarizationAccuracy.evaluate-234"><a href="#SummarizationAccuracy.evaluate-234"><span class="linenos">234</span></a>                        <span class="n">path_to_parent_dir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_eval_results_path</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-235"><a href="#SummarizationAccuracy.evaluate-235"><span class="linenos">235</span></a>                        <span class="n">eval_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_name</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-236"><a href="#SummarizationAccuracy.evaluate-236"><span class="linenos">236</span></a>                        <span class="n">dataset_name</span><span class="o">=</span><span class="n">dataset_config</span><span class="o">.</span><span class="n">dataset_name</span><span class="p">,</span>
</span><span id="SummarizationAccuracy.evaluate-237"><a href="#SummarizationAccuracy.evaluate-237"><span class="linenos">237</span></a>                    <span class="p">),</span>
</span><span id="SummarizationAccuracy.evaluate-238"><a href="#SummarizationAccuracy.evaluate-238"><span class="linenos">238</span></a>                <span class="p">)</span>
</span><span id="SummarizationAccuracy.evaluate-239"><a href="#SummarizationAccuracy.evaluate-239"><span class="linenos">239</span></a>
</span><span id="SummarizationAccuracy.evaluate-240"><a href="#SummarizationAccuracy.evaluate-240"><span class="linenos">240</span></a>        <span class="k">return</span> <span class="n">eval_outputs</span>
</span></pre></div>


            <div class="docstring"><p>Summarization Accuracy evaluate</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>model</strong>:  An instance of ModelRunner which is the model under evaluation</li>
<li><strong>dataset_config</strong>:  Configures the single dataset used for evaluation. If not provided,
evaluation will use all of it's supported built-in datasets</li>
<li><strong>prompt_template</strong>:  A template which can be used to generate prompts, optional, if not provided defaults
will be used.</li>
<li><strong>save</strong>:  If set to true, prompt responses and scores will be saved to file. The output is written to
EvalAlgorithmInterface.EVAL_RESULTS_PATH</li>
<li><strong>num_records</strong>:  The number of records to be sampled randomly from the input dataset to perform the
evaluation</li>
</ul>

<h6 id="returns">Returns</h6>

<blockquote>
  <p>List of EvalOutput objects.</p>
</blockquote>
</div>


                            </div>
                </section>
                <section id="get_meteor_score">
                            <input id="get_meteor_score-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_meteor_score</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">target_output</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">model_output</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">config</span><span class="p">:</span> <span class="n"><a href="#SummarizationAccuracyConfig">SummarizationAccuracyConfig</a></span></span><span class="return-annotation">) -> <span class="nb">float</span>:</span></span>

                <label class="view-source-button" for="get_meteor_score-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#get_meteor_score"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="get_meteor_score-243"><a href="#get_meteor_score-243"><span class="linenos">243</span></a><span class="k">def</span> <span class="nf">get_meteor_score</span><span class="p">(</span><span class="n">target_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SummarizationAccuracyConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
</span><span id="get_meteor_score-244"><a href="#get_meteor_score-244"><span class="linenos">244</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="get_meteor_score-245"><a href="#get_meteor_score-245"><span class="linenos">245</span></a><span class="sd">    METEOR, an automatic metric for machine translation evaluation</span>
</span><span id="get_meteor_score-246"><a href="#get_meteor_score-246"><span class="linenos">246</span></a><span class="sd">    that is based on a generalized concept of unigram matching between the</span>
</span><span id="get_meteor_score-247"><a href="#get_meteor_score-247"><span class="linenos">247</span></a><span class="sd">    machine-produced translation and human-produced reference translations.</span>
</span><span id="get_meteor_score-248"><a href="#get_meteor_score-248"><span class="linenos">248</span></a><span class="sd">    Unigrams can be matched based on their surface forms, stemmed forms,</span>
</span><span id="get_meteor_score-249"><a href="#get_meteor_score-249"><span class="linenos">249</span></a><span class="sd">    and meanings; furthermore, METEOR can be easily extended to include more</span>
</span><span id="get_meteor_score-250"><a href="#get_meteor_score-250"><span class="linenos">250</span></a><span class="sd">    advanced matching strategies. Once all generalized unigram matches</span>
</span><span id="get_meteor_score-251"><a href="#get_meteor_score-251"><span class="linenos">251</span></a><span class="sd">    between the two strings have been found, METEOR computes a score for</span>
</span><span id="get_meteor_score-252"><a href="#get_meteor_score-252"><span class="linenos">252</span></a><span class="sd">    this matching using a combination of unigram-precision, unigram-recall, and</span>
</span><span id="get_meteor_score-253"><a href="#get_meteor_score-253"><span class="linenos">253</span></a><span class="sd">    a measure of fragmentation that is designed to directly capture how</span>
</span><span id="get_meteor_score-254"><a href="#get_meteor_score-254"><span class="linenos">254</span></a><span class="sd">    well-ordered the matched words in the machine translation are in relation</span>
</span><span id="get_meteor_score-255"><a href="#get_meteor_score-255"><span class="linenos">255</span></a><span class="sd">    to the reference.</span>
</span><span id="get_meteor_score-256"><a href="#get_meteor_score-256"><span class="linenos">256</span></a>
</span><span id="get_meteor_score-257"><a href="#get_meteor_score-257"><span class="linenos">257</span></a><span class="sd">    METEOR gets an R correlation value of 0.347 with human evaluation on the Arabic</span>
</span><span id="get_meteor_score-258"><a href="#get_meteor_score-258"><span class="linenos">258</span></a><span class="sd">    data and 0.331 on the Chinese data. This is shown to be an improvement on</span>
</span><span id="get_meteor_score-259"><a href="#get_meteor_score-259"><span class="linenos">259</span></a><span class="sd">    using simply unigram-precision, unigram-recall and their harmonic F1</span>
</span><span id="get_meteor_score-260"><a href="#get_meteor_score-260"><span class="linenos">260</span></a><span class="sd">    combination.</span>
</span><span id="get_meteor_score-261"><a href="#get_meteor_score-261"><span class="linenos">261</span></a>
</span><span id="get_meteor_score-262"><a href="#get_meteor_score-262"><span class="linenos">262</span></a><span class="sd">    :param target_output: The expected responses from the model</span>
</span><span id="get_meteor_score-263"><a href="#get_meteor_score-263"><span class="linenos">263</span></a><span class="sd">    :param model_output: The output of a model that we want to evaluate.</span>
</span><span id="get_meteor_score-264"><a href="#get_meteor_score-264"><span class="linenos">264</span></a><span class="sd">    :returns: meteor score</span>
</span><span id="get_meteor_score-265"><a href="#get_meteor_score-265"><span class="linenos">265</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="get_meteor_score-266"><a href="#get_meteor_score-266"><span class="linenos">266</span></a>    <span class="k">return</span> <span class="n">meteor_score</span><span class="o">.</span><span class="n">single_meteor_score</span><span class="p">(</span>
</span><span id="get_meteor_score-267"><a href="#get_meteor_score-267"><span class="linenos">267</span></a>        <span class="n">reference</span><span class="o">=</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">target_output</span><span class="p">),</span> <span class="n">hypothesis</span><span class="o">=</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">model_output</span><span class="p">)</span>
</span><span id="get_meteor_score-268"><a href="#get_meteor_score-268"><span class="linenos">268</span></a>    <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>METEOR, an automatic metric for machine translation evaluation
that is based on a generalized concept of unigram matching between the
machine-produced translation and human-produced reference translations.
Unigrams can be matched based on their surface forms, stemmed forms,
and meanings; furthermore, METEOR can be easily extended to include more
advanced matching strategies. Once all generalized unigram matches
between the two strings have been found, METEOR computes a score for
this matching using a combination of unigram-precision, unigram-recall, and
a measure of fragmentation that is designed to directly capture how
well-ordered the matched words in the machine translation are in relation
to the reference.</p>

<p>METEOR gets an R correlation value of 0.347 with human evaluation on the Arabic
data and 0.331 on the Chinese data. This is shown to be an improvement on
using simply unigram-precision, unigram-recall and their harmonic F1
combination.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>target_output</strong>:  The expected responses from the model</li>
<li><strong>model_output</strong>:  The output of a model that we want to evaluate.
:returns: meteor score</li>
</ul>
</div>


                </section>
                <section id="get_rouge_score">
                            <input id="get_rouge_score-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_rouge_score</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">target_output</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">model_output</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">config</span><span class="p">:</span> <span class="n"><a href="#SummarizationAccuracyConfig">SummarizationAccuracyConfig</a></span></span><span class="return-annotation">) -> <span class="nb">float</span>:</span></span>

                <label class="view-source-button" for="get_rouge_score-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#get_rouge_score"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="get_rouge_score-271"><a href="#get_rouge_score-271"><span class="linenos">271</span></a><span class="k">def</span> <span class="nf">get_rouge_score</span><span class="p">(</span><span class="n">target_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SummarizationAccuracyConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
</span><span id="get_rouge_score-272"><a href="#get_rouge_score-272"><span class="linenos">272</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="get_rouge_score-273"><a href="#get_rouge_score-273"><span class="linenos">273</span></a><span class="sd">    The ROUGE-N, where N=[1,2,L], score is a standard metric for summarization quality.</span>
</span><span id="get_rouge_score-274"><a href="#get_rouge_score-274"><span class="linenos">274</span></a><span class="sd">    It computes the word overlap between the reference and model summary. Given that this metric is based on simple</span>
</span><span id="get_rouge_score-275"><a href="#get_rouge_score-275"><span class="linenos">275</span></a><span class="sd">    word overlap statistics, it works best for extractive summaries.</span>
</span><span id="get_rouge_score-276"><a href="#get_rouge_score-276"><span class="linenos">276</span></a><span class="sd">    Note that if we rephrase the summary without changing its meaning the ROUGE-N score will drop.</span>
</span><span id="get_rouge_score-277"><a href="#get_rouge_score-277"><span class="linenos">277</span></a>
</span><span id="get_rouge_score-278"><a href="#get_rouge_score-278"><span class="linenos">278</span></a><span class="sd">    Reference: https://huggingface.co/spaces/evaluate-metric/rouge</span>
</span><span id="get_rouge_score-279"><a href="#get_rouge_score-279"><span class="linenos">279</span></a>
</span><span id="get_rouge_score-280"><a href="#get_rouge_score-280"><span class="linenos">280</span></a><span class="sd">    :param target_output: The expected responses from the model</span>
</span><span id="get_rouge_score-281"><a href="#get_rouge_score-281"><span class="linenos">281</span></a><span class="sd">    :param model_output: The output of a model that we want to evaluate.</span>
</span><span id="get_rouge_score-282"><a href="#get_rouge_score-282"><span class="linenos">282</span></a><span class="sd">    :param config: Eval algo config</span>
</span><span id="get_rouge_score-283"><a href="#get_rouge_score-283"><span class="linenos">283</span></a><span class="sd">    :returns: rouge score: boolean indicating using stemmer for rouge</span>
</span><span id="get_rouge_score-284"><a href="#get_rouge_score-284"><span class="linenos">284</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="get_rouge_score-285"><a href="#get_rouge_score-285"><span class="linenos">285</span></a>    <span class="n">rouge</span> <span class="o">=</span> <span class="n">hf_evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;rouge&quot;</span><span class="p">)</span>
</span><span id="get_rouge_score-286"><a href="#get_rouge_score-286"><span class="linenos">286</span></a>    <span class="k">return</span> <span class="n">rouge</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span>
</span><span id="get_rouge_score-287"><a href="#get_rouge_score-287"><span class="linenos">287</span></a>        <span class="n">predictions</span><span class="o">=</span><span class="p">[</span><span class="n">model_output</span><span class="p">],</span>
</span><span id="get_rouge_score-288"><a href="#get_rouge_score-288"><span class="linenos">288</span></a>        <span class="n">references</span><span class="o">=</span><span class="p">[</span><span class="n">target_output</span><span class="p">],</span>
</span><span id="get_rouge_score-289"><a href="#get_rouge_score-289"><span class="linenos">289</span></a>        <span class="n">use_stemmer</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_stemmer_for_rouge</span><span class="p">,</span>
</span><span id="get_rouge_score-290"><a href="#get_rouge_score-290"><span class="linenos">290</span></a>        <span class="n">rouge_types</span><span class="o">=</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">rouge_type</span><span class="p">],</span>
</span><span id="get_rouge_score-291"><a href="#get_rouge_score-291"><span class="linenos">291</span></a>    <span class="p">)[</span><span class="n">config</span><span class="o">.</span><span class="n">rouge_type</span><span class="p">]</span>
</span></pre></div>


            <div class="docstring"><p>The ROUGE-N, where N=[1,2,L], score is a standard metric for summarization quality.
It computes the word overlap between the reference and model summary. Given that this metric is based on simple
word overlap statistics, it works best for extractive summaries.
Note that if we rephrase the summary without changing its meaning the ROUGE-N score will drop.</p>

<p>Reference: <a href="https://huggingface.co/spaces/evaluate-metric/rouge">https://huggingface.co/spaces/evaluate-metric/rouge</a></p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>target_output</strong>:  The expected responses from the model</li>
<li><strong>model_output</strong>:  The output of a model that we want to evaluate.</li>
<li><strong>config</strong>:  Eval algo config
:returns: rouge score: boolean indicating using stemmer for rouge</li>
</ul>
</div>


                </section>
                <section id="get_bert_score">
                            <input id="get_bert_score-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_bert_score</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">target_output</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">model_output</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">config</span><span class="p">:</span> <span class="n"><a href="#SummarizationAccuracyConfig">SummarizationAccuracyConfig</a></span></span><span class="return-annotation">) -> <span class="nb">float</span>:</span></span>

                <label class="view-source-button" for="get_bert_score-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#get_bert_score"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="get_bert_score-294"><a href="#get_bert_score-294"><span class="linenos">294</span></a><span class="k">def</span> <span class="nf">get_bert_score</span><span class="p">(</span><span class="n">target_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SummarizationAccuracyConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
</span><span id="get_bert_score-295"><a href="#get_bert_score-295"><span class="linenos">295</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="get_bert_score-296"><a href="#get_bert_score-296"><span class="linenos">296</span></a><span class="sd">    BERTscore is a similarity-based metric that compares the embedding of the prediction and target sentences</span>
</span><span id="get_bert_score-297"><a href="#get_bert_score-297"><span class="linenos">297</span></a><span class="sd">    under a (learned) model, typically, from the BERT family.</span>
</span><span id="get_bert_score-298"><a href="#get_bert_score-298"><span class="linenos">298</span></a><span class="sd">    This score may lead to increased flexibility compared to rouge and METEOR in terms of rephrasing since</span>
</span><span id="get_bert_score-299"><a href="#get_bert_score-299"><span class="linenos">299</span></a><span class="sd">    semantically similar sentences are (typically) embedded similarly.</span>
</span><span id="get_bert_score-300"><a href="#get_bert_score-300"><span class="linenos">300</span></a>
</span><span id="get_bert_score-301"><a href="#get_bert_score-301"><span class="linenos">301</span></a><span class="sd">    https://huggingface.co/spaces/evaluate-metric/bertscore</span>
</span><span id="get_bert_score-302"><a href="#get_bert_score-302"><span class="linenos">302</span></a>
</span><span id="get_bert_score-303"><a href="#get_bert_score-303"><span class="linenos">303</span></a><span class="sd">    :param target_output: The expected responses from the model</span>
</span><span id="get_bert_score-304"><a href="#get_bert_score-304"><span class="linenos">304</span></a><span class="sd">    :param model_output: The output of a model that we want to evaluate.</span>
</span><span id="get_bert_score-305"><a href="#get_bert_score-305"><span class="linenos">305</span></a><span class="sd">    :param config: Eval algo config</span>
</span><span id="get_bert_score-306"><a href="#get_bert_score-306"><span class="linenos">306</span></a><span class="sd">    :returns: rouge score: boolean indicating using stemmer for rouge</span>
</span><span id="get_bert_score-307"><a href="#get_bert_score-307"><span class="linenos">307</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="get_bert_score-308"><a href="#get_bert_score-308"><span class="linenos">308</span></a>    <span class="n">bertscore</span> <span class="o">=</span> <span class="n">BertscoreHelperModel</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">model_type_for_bertscore</span><span class="p">)</span>
</span><span id="get_bert_score-309"><a href="#get_bert_score-309"><span class="linenos">309</span></a>    <span class="k">return</span> <span class="n">bertscore</span><span class="o">.</span><span class="n">get_helper_scores</span><span class="p">(</span><span class="n">target_output</span><span class="p">,</span> <span class="n">model_output</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>BERTscore is a similarity-based metric that compares the embedding of the prediction and target sentences
under a (learned) model, typically, from the BERT family.
This score may lead to increased flexibility compared to rouge and METEOR in terms of rephrasing since
semantically similar sentences are (typically) embedded similarly.</p>

<p><a href="https://huggingface.co/spaces/evaluate-metric/bertscore">https://huggingface.co/spaces/evaluate-metric/bertscore</a></p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>target_output</strong>:  The expected responses from the model</li>
<li><strong>model_output</strong>:  The output of a model that we want to evaluate.</li>
<li><strong>config</strong>:  Eval algo config
:returns: rouge score: boolean indicating using stemmer for rouge</li>
</ul>
</div>


                </section>
                <section id="add_score_to_dataset">
                            <input id="add_score_to_dataset-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">add_score_to_dataset</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">dataset</span><span class="p">:</span> <span class="n">datasets</span><span class="o">.</span><span class="n">arrow_dataset</span><span class="o">.</span><span class="n">Dataset</span>,</span><span class="param">	<span class="n">eval_func</span><span class="p">:</span> <span class="n">Callable</span>,</span><span class="param">	<span class="n">score_column_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">config</span><span class="p">:</span> <span class="n"><a href="#SummarizationAccuracyConfig">SummarizationAccuracyConfig</a></span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="add_score_to_dataset-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#add_score_to_dataset"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="add_score_to_dataset-312"><a href="#add_score_to_dataset-312"><span class="linenos">312</span></a><span class="k">def</span> <span class="nf">add_score_to_dataset</span><span class="p">(</span>
</span><span id="add_score_to_dataset-313"><a href="#add_score_to_dataset-313"><span class="linenos">313</span></a>    <span class="n">dataset</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">eval_func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">score_column_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">SummarizationAccuracyConfig</span>
</span><span id="add_score_to_dataset-314"><a href="#add_score_to_dataset-314"><span class="linenos">314</span></a><span class="p">):</span>
</span><span id="add_score_to_dataset-315"><a href="#add_score_to_dataset-315"><span class="linenos">315</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="add_score_to_dataset-316"><a href="#add_score_to_dataset-316"><span class="linenos">316</span></a><span class="sd">    Util method to add a score column to a ray dataset.</span>
</span><span id="add_score_to_dataset-317"><a href="#add_score_to_dataset-317"><span class="linenos">317</span></a>
</span><span id="add_score_to_dataset-318"><a href="#add_score_to_dataset-318"><span class="linenos">318</span></a><span class="sd">    :param dataset: ray Dataset to be used for eval score generation</span>
</span><span id="add_score_to_dataset-319"><a href="#add_score_to_dataset-319"><span class="linenos">319</span></a><span class="sd">    :param eval_func: eval function callable method</span>
</span><span id="add_score_to_dataset-320"><a href="#add_score_to_dataset-320"><span class="linenos">320</span></a><span class="sd">    :param score_column_name: column name for score to be added</span>
</span><span id="add_score_to_dataset-321"><a href="#add_score_to_dataset-321"><span class="linenos">321</span></a><span class="sd">    :param config: Eval algo config</span>
</span><span id="add_score_to_dataset-322"><a href="#add_score_to_dataset-322"><span class="linenos">322</span></a><span class="sd">    :returns: ray Dataset with score column</span>
</span><span id="add_score_to_dataset-323"><a href="#add_score_to_dataset-323"><span class="linenos">323</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="add_score_to_dataset-324"><a href="#add_score_to_dataset-324"><span class="linenos">324</span></a>
</span><span id="add_score_to_dataset-325"><a href="#add_score_to_dataset-325"><span class="linenos">325</span></a>    <span class="k">def</span> <span class="nf">_generate_eval_scores</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">:</span>  <span class="c1"># pragma: no cover</span>
</span><span id="add_score_to_dataset-326"><a href="#add_score_to_dataset-326"><span class="linenos">326</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="add_score_to_dataset-327"><a href="#add_score_to_dataset-327"><span class="linenos">327</span></a><span class="sd">        Map function generating the scores for every input record in input dataset</span>
</span><span id="add_score_to_dataset-328"><a href="#add_score_to_dataset-328"><span class="linenos">328</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="add_score_to_dataset-329"><a href="#add_score_to_dataset-329"><span class="linenos">329</span></a>        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span>
</span><span id="add_score_to_dataset-330"><a href="#add_score_to_dataset-330"><span class="linenos">330</span></a>            <span class="n">data</span><span class="o">=</span><span class="p">[</span>
</span><span id="add_score_to_dataset-331"><a href="#add_score_to_dataset-331"><span class="linenos">331</span></a>                <span class="n">eval_func</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="n">TARGET_OUTPUT_COLUMN_NAME</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="n">MODEL_OUTPUT_COLUMN_NAME</span><span class="p">],</span> <span class="n">config</span><span class="p">)</span>
</span><span id="add_score_to_dataset-332"><a href="#add_score_to_dataset-332"><span class="linenos">332</span></a>                <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">()</span>
</span><span id="add_score_to_dataset-333"><a href="#add_score_to_dataset-333"><span class="linenos">333</span></a>            <span class="p">]</span>
</span><span id="add_score_to_dataset-334"><a href="#add_score_to_dataset-334"><span class="linenos">334</span></a>        <span class="p">)</span>
</span><span id="add_score_to_dataset-335"><a href="#add_score_to_dataset-335"><span class="linenos">335</span></a>
</span><span id="add_score_to_dataset-336"><a href="#add_score_to_dataset-336"><span class="linenos">336</span></a>    <span class="k">return</span> <span class="n">dataset</span><span class="o">.</span><span class="n">add_column</span><span class="p">(</span><span class="n">score_column_name</span><span class="p">,</span> <span class="n">_generate_eval_scores</span><span class="p">)</span><span class="o">.</span><span class="n">materialize</span><span class="p">()</span>
</span></pre></div>


            <div class="docstring"><p>Util method to add a score column to a ray dataset.</p>

<h6 id="parameters">Parameters</h6>

<ul>
<li><strong>dataset</strong>:  ray Dataset to be used for eval score generation</li>
<li><strong>eval_func</strong>:  eval function callable method</li>
<li><strong>score_column_name</strong>:  column name for score to be added</li>
<li><strong>config</strong>:  Eval algo config
:returns: ray Dataset with score column</li>
</ul>
</div>


                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>