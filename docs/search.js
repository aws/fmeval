window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "src.fmeval", "modulename": "src.fmeval", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.constants", "modulename": "src.fmeval.constants", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.constants.EVAL_RESULTS_PATH", "modulename": "src.fmeval.constants", "qualname": "EVAL_RESULTS_PATH", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;EVAL_RESULTS_PATH&#x27;"}, {"fullname": "src.fmeval.constants.DEFAULT_EVAL_RESULTS_PATH", "modulename": "src.fmeval.constants", "qualname": "DEFAULT_EVAL_RESULTS_PATH", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;/tmp/eval_results/&#x27;"}, {"fullname": "src.fmeval.constants.PARALLELIZATION_FACTOR", "modulename": "src.fmeval.constants", "qualname": "PARALLELIZATION_FACTOR", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;PARALLELIZATION_FACTOR&#x27;"}, {"fullname": "src.fmeval.constants.PARTITION_MULTIPLIER", "modulename": "src.fmeval.constants", "qualname": "PARTITION_MULTIPLIER", "kind": "variable", "doc": "<p></p>\n", "default_value": "5"}, {"fullname": "src.fmeval.constants.SAGEMAKER_SERVICE_ENDPOINT_URL", "modulename": "src.fmeval.constants", "qualname": "SAGEMAKER_SERVICE_ENDPOINT_URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;SAGEMAKER_SERVICE_ENDPOINT_URL&#x27;"}, {"fullname": "src.fmeval.constants.SAGEMAKER_RUNTIME_ENDPOINT_URL", "modulename": "src.fmeval.constants", "qualname": "SAGEMAKER_RUNTIME_ENDPOINT_URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;SAGEMAKER_RUNTIME_ENDPOINT_URL&#x27;"}, {"fullname": "src.fmeval.constants.MODEL_INPUT_COLUMN_NAME", "modulename": "src.fmeval.constants", "qualname": "MODEL_INPUT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;model_input&#x27;"}, {"fullname": "src.fmeval.constants.MODEL_OUTPUT_COLUMN_NAME", "modulename": "src.fmeval.constants", "qualname": "MODEL_OUTPUT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;model_output&#x27;"}, {"fullname": "src.fmeval.constants.MODEL_LOG_PROBABILITY_COLUMN_NAME", "modulename": "src.fmeval.constants", "qualname": "MODEL_LOG_PROBABILITY_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;model_log_probability&#x27;"}, {"fullname": "src.fmeval.constants.TARGET_OUTPUT_COLUMN_NAME", "modulename": "src.fmeval.constants", "qualname": "TARGET_OUTPUT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;target_output&#x27;"}, {"fullname": "src.fmeval.constants.CATEGORY_COLUMN_NAME", "modulename": "src.fmeval.constants", "qualname": "CATEGORY_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;category&#x27;"}, {"fullname": "src.fmeval.constants.SENT_MORE_INPUT_COLUMN_NAME", "modulename": "src.fmeval.constants", "qualname": "SENT_MORE_INPUT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;sent_more_input&#x27;"}, {"fullname": "src.fmeval.constants.SENT_LESS_INPUT_COLUMN_NAME", "modulename": "src.fmeval.constants", "qualname": "SENT_LESS_INPUT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;sent_less_input&#x27;"}, {"fullname": "src.fmeval.constants.SENT_MORE_PROMPT_COLUMN_NAME", "modulename": "src.fmeval.constants", "qualname": "SENT_MORE_PROMPT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;sent_more_prompt&#x27;"}, {"fullname": "src.fmeval.constants.SENT_LESS_PROMPT_COLUMN_NAME", "modulename": "src.fmeval.constants", "qualname": "SENT_LESS_PROMPT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;sent_less_prompt&#x27;"}, {"fullname": "src.fmeval.constants.SENT_LESS_LOG_PROB_COLUMN_NAME", "modulename": "src.fmeval.constants", "qualname": "SENT_LESS_LOG_PROB_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;sent_less_log_prob&#x27;"}, {"fullname": "src.fmeval.constants.SENT_MORE_LOG_PROB_COLUMN_NAME", "modulename": "src.fmeval.constants", "qualname": "SENT_MORE_LOG_PROB_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;sent_more_log_prob&#x27;"}, {"fullname": "src.fmeval.constants.COLUMN_NAMES", "modulename": "src.fmeval.constants", "qualname": "COLUMN_NAMES", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;sent_less_input&#x27;, &#x27;sent_more_input&#x27;, &#x27;model_input&#x27;, &#x27;sent_less_log_prob&#x27;, &#x27;sent_more_log_prob&#x27;, &#x27;target_output&#x27;, &#x27;model_output&#x27;, &#x27;category&#x27;}"}, {"fullname": "src.fmeval.constants.DATA_CONFIG_LOCATION_SUFFIX", "modulename": "src.fmeval.constants", "qualname": "DATA_CONFIG_LOCATION_SUFFIX", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;_location&#x27;"}, {"fullname": "src.fmeval.constants.MIME_TYPE_JSON", "modulename": "src.fmeval.constants", "qualname": "MIME_TYPE_JSON", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;application/json&#x27;"}, {"fullname": "src.fmeval.constants.MIME_TYPE_JSONLINES", "modulename": "src.fmeval.constants", "qualname": "MIME_TYPE_JSONLINES", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;application/jsonlines&#x27;"}, {"fullname": "src.fmeval.constants.SUPPORTED_MIME_TYPES", "modulename": "src.fmeval.constants", "qualname": "SUPPORTED_MIME_TYPES", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;application/json&#x27;, &#x27;application/jsonlines&#x27;]"}, {"fullname": "src.fmeval.constants.MEAN", "modulename": "src.fmeval.constants", "qualname": "MEAN", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;mean&#x27;"}, {"fullname": "src.fmeval.constants.EVAL_OUTPUT_RECORDS_BATCH_SIZE", "modulename": "src.fmeval.constants", "qualname": "EVAL_OUTPUT_RECORDS_BATCH_SIZE", "kind": "variable", "doc": "<p></p>\n", "default_value": "1024"}, {"fullname": "src.fmeval.constants.SEED", "modulename": "src.fmeval.constants", "qualname": "SEED", "kind": "variable", "doc": "<p></p>\n", "default_value": "1234"}, {"fullname": "src.fmeval.constants.BUTTER_FINGER", "modulename": "src.fmeval.constants", "qualname": "BUTTER_FINGER", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;butter_finger&#x27;"}, {"fullname": "src.fmeval.constants.RANDOM_UPPER_CASE", "modulename": "src.fmeval.constants", "qualname": "RANDOM_UPPER_CASE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;random_upper_case&#x27;"}, {"fullname": "src.fmeval.constants.WHITESPACE_ADD_REMOVE", "modulename": "src.fmeval.constants", "qualname": "WHITESPACE_ADD_REMOVE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;whitespace_add_remove&#x27;"}, {"fullname": "src.fmeval.constants.PREFIX_FOR_DELTA_SCORES", "modulename": "src.fmeval.constants", "qualname": "PREFIX_FOR_DELTA_SCORES", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;delta_&#x27;"}, {"fullname": "src.fmeval.constants.NUM_ROWS_DETERMINISTIC", "modulename": "src.fmeval.constants", "qualname": "NUM_ROWS_DETERMINISTIC", "kind": "variable", "doc": "<p></p>\n", "default_value": "5"}, {"fullname": "src.fmeval.constants.MAX_ROWS_TO_TAKE", "modulename": "src.fmeval.constants", "qualname": "MAX_ROWS_TO_TAKE", "kind": "variable", "doc": "<p></p>\n", "default_value": "100000"}, {"fullname": "src.fmeval.constants.ABS_TOL", "modulename": "src.fmeval.constants", "qualname": "ABS_TOL", "kind": "variable", "doc": "<p></p>\n", "default_value": "0.001"}, {"fullname": "src.fmeval.constants.JUMPSTART_MODEL_ID", "modulename": "src.fmeval.constants", "qualname": "JUMPSTART_MODEL_ID", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;jumpstart_model_id&#x27;"}, {"fullname": "src.fmeval.constants.JUMPSTART_MODEL_VERSION", "modulename": "src.fmeval.constants", "qualname": "JUMPSTART_MODEL_VERSION", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;jumpstart_model_version&#x27;"}, {"fullname": "src.fmeval.constants.MODEL_ID", "modulename": "src.fmeval.constants", "qualname": "MODEL_ID", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;model_id&#x27;"}, {"fullname": "src.fmeval.constants.SPEC_KEY", "modulename": "src.fmeval.constants", "qualname": "SPEC_KEY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;spec_key&#x27;"}, {"fullname": "src.fmeval.constants.DEFAULT_PAYLOADS", "modulename": "src.fmeval.constants", "qualname": "DEFAULT_PAYLOADS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;default_payloads&#x27;"}, {"fullname": "src.fmeval.constants.SDK_MANIFEST_FILE", "modulename": "src.fmeval.constants", "qualname": "SDK_MANIFEST_FILE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;models_manifest.json&#x27;"}, {"fullname": "src.fmeval.constants.JUMPSTART_BUCKET_BASE_URL_FORMAT", "modulename": "src.fmeval.constants", "qualname": "JUMPSTART_BUCKET_BASE_URL_FORMAT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://jumpstart-cache-prod-{}.s3.{}.amazonaws.com&#x27;"}, {"fullname": "src.fmeval.constants.JUMPSTART_BUCKET_BASE_URL_FORMAT_ENV_VAR", "modulename": "src.fmeval.constants", "qualname": "JUMPSTART_BUCKET_BASE_URL_FORMAT_ENV_VAR", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;JUMPSTART_BUCKET_BASE_URL_FORMAT&#x27;"}, {"fullname": "src.fmeval.constants.GENERATED_TEXT_JMESPATH_EXPRESSION", "modulename": "src.fmeval.constants", "qualname": "GENERATED_TEXT_JMESPATH_EXPRESSION", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;*.output_keys.generated_text&#x27;"}, {"fullname": "src.fmeval.data_loaders", "modulename": "src.fmeval.data_loaders", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.data_loaders.data_config", "modulename": "src.fmeval.data_loaders.data_config", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.data_loaders.data_config.DataConfig", "modulename": "src.fmeval.data_loaders.data_config", "qualname": "DataConfig", "kind": "class", "doc": "<p>Configures the information required by data-loading components.</p>\n\n<p>Note that the term \"location\" used below refers to a string\nthat can be used to locate the data that comprises a single\ncolumn in the to-be-produced Ray Dataset. As an example,\nwhen the dataset MIME type is JSON or JSON Lines, the \"location\"\nis a JMESPath query.</p>\n\n<p><strong>Note</strong>:\n    Parsing logic used by data loaders make the assumption that\n    attributes in this class with the suffix \"_location\" correspond\n    to a \"location\" (defined above). When adding new attributes to this class,\n    if an attribute corresponds to a location, the attribute name must end\n    with \"_location\"</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>dataset_name</strong>:  the dataset name</li>\n<li><strong>dataset_uri</strong>:  either a local path or s3 URI representing where the dataset is stored</li>\n<li><strong>dataset_mime_type</strong>:  the MIME type of the dataset file</li>\n<li><strong>model_input_location</strong>:  the location  for model inputs</li>\n<li><strong>model_output_location</strong>:  the location for model outputs</li>\n<li><strong>target_output_location</strong>:  the location for target outputs</li>\n<li><strong>category_location</strong>:  the location for categories</li>\n<li><strong>sent_more_input_location</strong>:  the location for the \"sent more\"\ninputs (used by the Prompt Stereotyping evaluation algorithm)</li>\n<li><strong>sent_less_input_location</strong>:  the location for the \"sent less\"\ninputs (used by the Prompt Stereotyping evaluation algorithm)</li>\n<li><strong>sent_more_log_prob_location</strong>:  the location for the \"sent more\"\ninput log probability (used by the Prompt Stereotyping evaluation algorithm)</li>\n<li><strong>sent_less_log_prob_location</strong>:  the location for the \"sent less\"\ninput log probability (used by the Prompt Stereotyping evaluation algorithm).</li>\n</ul>\n"}, {"fullname": "src.fmeval.data_loaders.data_config.DataConfig.__init__", "modulename": "src.fmeval.data_loaders.data_config", "qualname": "DataConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_uri</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_mime_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_input_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_output_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_output_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">category_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sent_more_input_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sent_less_input_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sent_more_log_prob_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sent_less_log_prob_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "src.fmeval.data_loaders.data_config.DataConfig.dataset_name", "modulename": "src.fmeval.data_loaders.data_config", "qualname": "DataConfig.dataset_name", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "src.fmeval.data_loaders.data_config.DataConfig.dataset_uri", "modulename": "src.fmeval.data_loaders.data_config", "qualname": "DataConfig.dataset_uri", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "src.fmeval.data_loaders.data_config.DataConfig.dataset_mime_type", "modulename": "src.fmeval.data_loaders.data_config", "qualname": "DataConfig.dataset_mime_type", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "src.fmeval.data_loaders.data_config.DataConfig.model_input_location", "modulename": "src.fmeval.data_loaders.data_config", "qualname": "DataConfig.model_input_location", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.data_loaders.data_config.DataConfig.model_output_location", "modulename": "src.fmeval.data_loaders.data_config", "qualname": "DataConfig.model_output_location", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.data_loaders.data_config.DataConfig.target_output_location", "modulename": "src.fmeval.data_loaders.data_config", "qualname": "DataConfig.target_output_location", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.data_loaders.data_config.DataConfig.category_location", "modulename": "src.fmeval.data_loaders.data_config", "qualname": "DataConfig.category_location", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.data_loaders.data_config.DataConfig.sent_more_input_location", "modulename": "src.fmeval.data_loaders.data_config", "qualname": "DataConfig.sent_more_input_location", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.data_loaders.data_config.DataConfig.sent_less_input_location", "modulename": "src.fmeval.data_loaders.data_config", "qualname": "DataConfig.sent_less_input_location", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.data_loaders.data_config.DataConfig.sent_more_log_prob_location", "modulename": "src.fmeval.data_loaders.data_config", "qualname": "DataConfig.sent_more_log_prob_location", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.data_loaders.data_config.DataConfig.sent_less_log_prob_location", "modulename": "src.fmeval.data_loaders.data_config", "qualname": "DataConfig.sent_less_log_prob_location", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.data_loaders.data_sources", "modulename": "src.fmeval.data_loaders.data_sources", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.data_loaders.data_sources.DataSource", "modulename": "src.fmeval.data_loaders.data_sources", "qualname": "DataSource", "kind": "class", "doc": "<p>Managed data resource</p>\n", "bases": "abc.ABC"}, {"fullname": "src.fmeval.data_loaders.data_sources.DataSource.__init__", "modulename": "src.fmeval.data_loaders.data_sources", "qualname": "DataSource.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">uri</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "src.fmeval.data_loaders.data_sources.DataSource.uri", "modulename": "src.fmeval.data_loaders.data_sources", "qualname": "DataSource.uri", "kind": "variable", "doc": "<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>path to the resource</p>\n</blockquote>\n", "annotation": ": str"}, {"fullname": "src.fmeval.data_loaders.data_sources.DataFile", "modulename": "src.fmeval.data_loaders.data_sources", "qualname": "DataFile", "kind": "class", "doc": "<p>Managed data file resource</p>\n", "bases": "DataSource"}, {"fullname": "src.fmeval.data_loaders.data_sources.DataFile.open", "modulename": "src.fmeval.data_loaders.data_sources", "qualname": "DataFile.open", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>mode</strong>:  optional mode to open file, default 'r' is readonly</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>File object</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;r&#39;</span></span><span class=\"return-annotation\">) -> &lt;class &#x27;IO&#x27;&gt;:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.data_loaders.data_sources.LocalDataFile", "modulename": "src.fmeval.data_loaders.data_sources", "qualname": "LocalDataFile", "kind": "class", "doc": "<p>Datafile class for local files</p>\n", "bases": "DataFile"}, {"fullname": "src.fmeval.data_loaders.data_sources.LocalDataFile.__init__", "modulename": "src.fmeval.data_loaders.data_sources", "qualname": "LocalDataFile.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">file_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "src.fmeval.data_loaders.data_sources.LocalDataFile.open", "modulename": "src.fmeval.data_loaders.data_sources", "qualname": "LocalDataFile.open", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>mode</strong>:  optional mode to open file, default 'r' is readonly</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>File object</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;r&#39;</span></span><span class=\"return-annotation\">) -> &lt;class &#x27;IO&#x27;&gt;:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.data_loaders.data_sources.S3DataFile", "modulename": "src.fmeval.data_loaders.data_sources", "qualname": "S3DataFile", "kind": "class", "doc": "<p>DataFile class for s3 files</p>\n", "bases": "DataFile"}, {"fullname": "src.fmeval.data_loaders.data_sources.S3DataFile.__init__", "modulename": "src.fmeval.data_loaders.data_sources", "qualname": "S3DataFile.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">s3</span><span class=\"p\">:</span> <span class=\"n\">s3fs</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">S3FileSystem</span>, </span><span class=\"param\"><span class=\"n\">file_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "src.fmeval.data_loaders.data_sources.S3DataFile.open", "modulename": "src.fmeval.data_loaders.data_sources", "qualname": "S3DataFile.open", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>mode</strong>:  optional mode to open file, default 'r' is readonly</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>File object</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;r&#39;</span></span><span class=\"return-annotation\">) -> &lt;class &#x27;IO&#x27;&gt;:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.data_loaders.jmespath_util", "modulename": "src.fmeval.data_loaders.jmespath_util", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.data_loaders.jmespath_util.logger", "modulename": "src.fmeval.data_loaders.jmespath_util", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.data_loaders.jmespath_util (WARNING)&gt;"}, {"fullname": "src.fmeval.data_loaders.jmespath_util.compile_jmespath", "modulename": "src.fmeval.data_loaders.jmespath_util", "qualname": "compile_jmespath", "kind": "function", "doc": "<p>Compiles a JMESPath expression to be used for JSON data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">jmespath_expression</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.data_loaders.jmespath_util.search_jmespath", "modulename": "src.fmeval.data_loaders.jmespath_util", "qualname": "search_jmespath", "kind": "function", "doc": "<p>Searches a dataset using a JMESPath query.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>jmespath_parser</strong>:  The JMESPath parser, used for parsing model inputs, model outputs,\ntarget outputs, or categories.</li>\n<li><strong>jmespath_query_type: Used for error logging. One of the following</strong>: \nconstants.MODEL_INPUT_COLUMN_NAME, constants.MODEL_OUTPUT_COLUMN_NAME\nconstants.TARGET_OUTPUT_COLUMN_NAME, constants.CATEGORY_COLUMN_NAME</li>\n<li><strong>dataset</strong>:  The data to be searched, already deserialized into a dict/list.</li>\n<li><strong>dataset_name</strong>:  A name associated with the dataset being parsed for logging purposes.\n:returns: The result of executing the JMESPath query on the dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">jmespath_parser</span><span class=\"p\">:</span> <span class=\"n\">jmespath</span><span class=\"o\">.</span><span class=\"n\">parser</span><span class=\"o\">.</span><span class=\"n\">ParsedResult</span>,</span><span class=\"param\">\t<span class=\"n\">jmespath_query_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.data_loaders.json_data_loader", "modulename": "src.fmeval.data_loaders.json_data_loader", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.data_loaders.json_data_loader.JsonDataLoaderConfig", "modulename": "src.fmeval.data_loaders.json_data_loader", "qualname": "JsonDataLoaderConfig", "kind": "class", "doc": "<p>Configures a JsonDataLoader or JsonLinesDataLoader.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>parser</strong>:  The JsonParser object used to parse the dataset.</li>\n<li><strong>data_file</strong>:  The DataFile object representing the dataset.</li>\n<li><strong>dataset_name</strong>:  The name of the dataset for logging purposes.</li>\n<li><strong>dataset_mime_type</strong>:  Either MIME_TYPE_JSON or MIME_TYPE_JSONLINES</li>\n</ul>\n"}, {"fullname": "src.fmeval.data_loaders.json_data_loader.JsonDataLoaderConfig.__init__", "modulename": "src.fmeval.data_loaders.json_data_loader", "qualname": "JsonDataLoaderConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">parser</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">json_parser</span><span class=\"o\">.</span><span class=\"n\">JsonParser</span>,</span><span class=\"param\">\t<span class=\"n\">data_file</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">data_sources</span><span class=\"o\">.</span><span class=\"n\">DataFile</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_mime_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "src.fmeval.data_loaders.json_data_loader.JsonDataLoaderConfig.parser", "modulename": "src.fmeval.data_loaders.json_data_loader", "qualname": "JsonDataLoaderConfig.parser", "kind": "variable", "doc": "<p></p>\n", "annotation": ": fmeval.data_loaders.json_parser.JsonParser"}, {"fullname": "src.fmeval.data_loaders.json_data_loader.JsonDataLoaderConfig.data_file", "modulename": "src.fmeval.data_loaders.json_data_loader", "qualname": "JsonDataLoaderConfig.data_file", "kind": "variable", "doc": "<p></p>\n", "annotation": ": fmeval.data_loaders.data_sources.DataFile"}, {"fullname": "src.fmeval.data_loaders.json_data_loader.JsonDataLoaderConfig.dataset_name", "modulename": "src.fmeval.data_loaders.json_data_loader", "qualname": "JsonDataLoaderConfig.dataset_name", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "src.fmeval.data_loaders.json_data_loader.JsonDataLoaderConfig.dataset_mime_type", "modulename": "src.fmeval.data_loaders.json_data_loader", "qualname": "JsonDataLoaderConfig.dataset_mime_type", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "src.fmeval.data_loaders.json_data_loader.JsonDataLoader", "modulename": "src.fmeval.data_loaders.json_data_loader", "qualname": "JsonDataLoader", "kind": "class", "doc": "<p>Reads a JSON or JSON Lines dataset and returns a Ray Dataset.</p>\n"}, {"fullname": "src.fmeval.data_loaders.json_data_loader.JsonDataLoader.load_dataset", "modulename": "src.fmeval.data_loaders.json_data_loader", "qualname": "JsonDataLoader.load_dataset", "kind": "function", "doc": "<p>Reads a JSON dataset and returns a Ray Dataset that includes headers.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>config</strong>:  see JsonDataLoaderConfig docstring.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>a Ray Dataset object that includes headers.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">json_data_loader</span><span class=\"o\">.</span><span class=\"n\">JsonDataLoaderConfig</span></span><span class=\"return-annotation\">) -> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.data_loaders.json_data_loader.CustomJSONDatasource", "modulename": "src.fmeval.data_loaders.json_data_loader", "qualname": "CustomJSONDatasource", "kind": "class", "doc": "<p>Custom datasource class for reading and writing JSON or JSON Lines files.</p>\n\n<p>See <a href=\"https://docs.ray.io/en/latest/data/examples/custom-datasource.html#custom-datasources\">https://docs.ray.io/en/latest/data/examples/custom-datasource.html#custom-datasources</a>\nfor details on creating custom data sources.</p>\n\n<p>We use this class instead of Ray's own JSONDatasource class because\nRay's implementation relies on pyarrow._json.read_json, which cannot\nhandle JSON files that contain heterogeneous lists\n(lists with elements of different data types).</p>\n\n<p>Example JSON dataset that pyarrow._json.read_json cannot handle:\n{\n   \"key\": [20, \"hello\"]\n}</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>config</strong>:  The config used by _read_file to determine whether to treat the\ninput file as a JSON or JSON Lines file.</li>\n</ul>\n", "bases": "ray.data.datasource.file_based_datasource.FileBasedDatasource"}, {"fullname": "src.fmeval.data_loaders.json_data_loader.CustomJSONDatasource.__init__", "modulename": "src.fmeval.data_loaders.json_data_loader", "qualname": "CustomJSONDatasource.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">json_data_loader</span><span class=\"o\">.</span><span class=\"n\">JsonDataLoaderConfig</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "src.fmeval.data_loaders.json_parser", "modulename": "src.fmeval.data_loaders.json_parser", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.data_loaders.json_parser.ColumnParseArguments", "modulename": "src.fmeval.data_loaders.json_parser", "qualname": "ColumnParseArguments", "kind": "class", "doc": "<p>Data that is shared by various JsonParser methods.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>jmespath_parser</strong>:  The JMESPath parser that parses columns from the dataset\nusing JMESPath queries.</li>\n<li><strong>jmespath_query_type</strong>:  Used for error logging. Will always be one of the *_COLUMN_NAME\nconstants (ex: MODEL_INPUT_COLUMN_NAME).</li>\n<li><strong>dataset</strong>:  The data to be searched, already deserialized into a dict/list.</li>\n<li><strong>dataset_mime_type</strong>:  Either MIME_TYPE_JSON or MIME_TYPE_JSON_LINES.\nUsed to determine whether the result parsed by <code>jmespath_parser</code>\nis expected to be a list or a single value.</li>\n<li><strong>dataset_name</strong>:  The name associated with the dataset being parsed.</li>\n</ul>\n"}, {"fullname": "src.fmeval.data_loaders.json_parser.ColumnParseArguments.__init__", "modulename": "src.fmeval.data_loaders.json_parser", "qualname": "ColumnParseArguments.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">jmespath_parser</span><span class=\"p\">:</span> <span class=\"n\">jmespath</span><span class=\"o\">.</span><span class=\"n\">parser</span><span class=\"o\">.</span><span class=\"n\">ParsedResult</span>,</span><span class=\"param\">\t<span class=\"n\">jmespath_query_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">],</span> <span class=\"n\">List</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_mime_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "src.fmeval.data_loaders.json_parser.ColumnParseArguments.jmespath_parser", "modulename": "src.fmeval.data_loaders.json_parser", "qualname": "ColumnParseArguments.jmespath_parser", "kind": "variable", "doc": "<p></p>\n", "annotation": ": jmespath.parser.ParsedResult"}, {"fullname": "src.fmeval.data_loaders.json_parser.ColumnParseArguments.jmespath_query_type", "modulename": "src.fmeval.data_loaders.json_parser", "qualname": "ColumnParseArguments.jmespath_query_type", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "src.fmeval.data_loaders.json_parser.ColumnParseArguments.dataset", "modulename": "src.fmeval.data_loaders.json_parser", "qualname": "ColumnParseArguments.dataset", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Union[Dict[str, Any], List]"}, {"fullname": "src.fmeval.data_loaders.json_parser.ColumnParseArguments.dataset_mime_type", "modulename": "src.fmeval.data_loaders.json_parser", "qualname": "ColumnParseArguments.dataset_mime_type", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "src.fmeval.data_loaders.json_parser.ColumnParseArguments.dataset_name", "modulename": "src.fmeval.data_loaders.json_parser", "qualname": "ColumnParseArguments.dataset_name", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "src.fmeval.data_loaders.json_parser.JsonParser", "modulename": "src.fmeval.data_loaders.json_parser", "qualname": "JsonParser", "kind": "class", "doc": "<p>Parser for JSON and JSON Lines datasets using JMESPath queries supplied by the DataConfig class.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>_parsers</strong>:  A dict that maps keys (which must be one of the *_COLUMN_NAME constants)\nto ParsedResult objects. These ParsedResult objects (from the jmespath library)\nperform the JMESPath searching in the <code>search_jmespath</code> util function.</li>\n</ul>\n"}, {"fullname": "src.fmeval.data_loaders.json_parser.JsonParser.__init__", "modulename": "src.fmeval.data_loaders.json_parser", "qualname": "JsonParser.__init__", "kind": "function", "doc": "<p>Initializes the instance's <code>_parsers</code> dict using attributes from <code>config</code> that correspond to JMESPath queries.</p>\n\n<p>Example:</p>\n\n<p>config = DataConfig(\n    ...,\n    model_input_location=\"my_model_input_jmespath\",\n    model_output_location=\"my_model_output_jmespath\",\n    category_location=\"my_category_jmespath\"\n)</p>\n\n<p>When we create a JsonParser from this config, we find the attributes ending in DATA_CONFIG_LOCATION_SUFFIX\n(which is \"_location\" at the time of this writing) that are not None: model_input_location,\nmodel_output_location, and category_location.\nNote that all optional attributes in DataConfig have default value None.</p>\n\n<p>The keys we use for <code>self._parsers</code> will be \"model_input\", \"model_output\", and \"sent_more_input\"\n(we strip away DATA_CONFIG_LOCATION_SUFFIX when generating the key). We validate that these keys are included\nin the *_COLUMN_NAME constants, since downstream code assumes that these constants are always used.</p>\n\n<p>The values corresponding to these keys will be ParsedResult objects that get created by calling\n<code>compile_jmespath</code> on \"my_model_input_jmespath\", \"my_model_output_jmespath\", and \"my_sent_more_input_jmespath\".</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>config</strong>:  see DataConfig docstring</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">data_config</span><span class=\"o\">.</span><span class=\"n\">DataConfig</span></span>)</span>"}, {"fullname": "src.fmeval.data_loaders.json_parser.JsonParser.parse_dataset_columns", "modulename": "src.fmeval.data_loaders.json_parser", "qualname": "JsonParser.parse_dataset_columns", "kind": "function", "doc": "<p>Parses a JSON dataset (which could be a single line in a JSON Lines dataset)\n   using the parsers stored in self._parsers to extract the desired columns.\n   In the case that <code>dataset</code> corresponds to a single JSON Lines line, the\n   \"columns\" are scalars instead of lists.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>dataset</strong>:  The dataset's data, already deserialized into a dict/list.</li>\n<li><strong>dataset_mime_type</strong>:  Either MIME_TYPE_JSON or MIME_TYPE_JSONLINES.</li>\n<li><strong>dataset_name</strong>:  The name of the dataset, to be used for error logging.\n:returns: A dict that maps keys to extracted columns. The keys used are\nexactly the same as the keys used in self._parsers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">],</span> <span class=\"n\">List</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_mime_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.data_loaders.util", "modulename": "src.fmeval.data_loaders.util", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.data_loaders.util.s3", "modulename": "src.fmeval.data_loaders.util", "qualname": "s3", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;s3fs.core.S3FileSystem object&gt;"}, {"fullname": "src.fmeval.data_loaders.util.logger", "modulename": "src.fmeval.data_loaders.util", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.data_loaders.util (WARNING)&gt;"}, {"fullname": "src.fmeval.data_loaders.util.get_dataset", "modulename": "src.fmeval.data_loaders.util", "qualname": "get_dataset", "kind": "function", "doc": "<p>Util method to load Ray datasets using an input DataConfig.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>config</strong>:  Input DataConfig</li>\n<li><strong>num_records</strong>:  the number of records to sample from the dataset</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">data_config</span><span class=\"o\">.</span><span class=\"n\">DataConfig</span>,</span><span class=\"param\">\t<span class=\"n\">num_records</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.data_loaders.util.get_data_source", "modulename": "src.fmeval.data_loaders.util", "qualname": "get_data_source", "kind": "function", "doc": "<p>Validates a dataset URI and returns the corresponding DataSource object</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>dataset_uri</strong>:  local dataset path or s3 dataset uri</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>DataSource object</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">dataset_uri</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">data_sources</span><span class=\"o\">.</span><span class=\"n\">DataSource</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval", "modulename": "src.fmeval.eval", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval.get_eval_algorithm", "modulename": "src.fmeval.eval", "qualname": "get_eval_algorithm", "kind": "function", "doc": "<p>Get eval algorithm class with name</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>eval_name</strong>:  eval algorithm name</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>eval algorithm class</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">eval_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">eval_algorithm_config</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">,</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">eval_algorithm</span><span class=\"o\">.</span><span class=\"n\">EvalAlgorithmConfig</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">eval_algorithm</span><span class=\"o\">.</span><span class=\"n\">EvalAlgorithmInterface</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algo_mapping", "modulename": "src.fmeval.eval_algo_mapping", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algo_mapping.EVAL_ALGORITHMS", "modulename": "src.fmeval.eval_algo_mapping", "qualname": "EVAL_ALGORITHMS", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, Type[fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface]]", "default_value": "{&#x27;classification_accuracy&#x27;: &lt;class &#x27;fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy&#x27;&gt;, &#x27;classification_accuracy_semantic_robustness&#x27;: &lt;class &#x27;fmeval.eval_algorithms.classification_accuracy_semantic_robustness.ClassificationAccuracySemanticRobustness&#x27;&gt;, &#x27;factual_knowledge&#x27;: &lt;class &#x27;fmeval.eval_algorithms.factual_knowledge.FactualKnowledge&#x27;&gt;, &#x27;general_semantic_robustness&#x27;: &lt;class &#x27;fmeval.eval_algorithms.general_semantic_robustness.GeneralSemanticRobustness&#x27;&gt;, &#x27;prompt_stereotyping&#x27;: &lt;class &#x27;fmeval.eval_algorithms.prompt_stereotyping.PromptStereotyping&#x27;&gt;, &#x27;qa_accuracy&#x27;: &lt;class &#x27;fmeval.eval_algorithms.qa_accuracy.QAAccuracy&#x27;&gt;, &#x27;qa_accuracy_semantic_robustness&#x27;: &lt;class &#x27;fmeval.eval_algorithms.qa_accuracy_semantic_robustness.QAAccuracySemanticRobustness&#x27;&gt;, &#x27;qa_toxicity&#x27;: &lt;class &#x27;fmeval.eval_algorithms.qa_toxicity.QAToxicity&#x27;&gt;, &#x27;summarization_accuracy&#x27;: &lt;class &#x27;fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracy&#x27;&gt;, &#x27;summarization_accuracy_semantic_robustness&#x27;: &lt;class &#x27;fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracySemanticRobustness&#x27;&gt;, &#x27;summarization_toxicity&#x27;: &lt;class &#x27;fmeval.eval_algorithms.summarization_toxicity.SummarizationToxicity&#x27;&gt;, &#x27;toxicity&#x27;: &lt;class &#x27;fmeval.eval_algorithms.toxicity.Toxicity&#x27;&gt;}"}, {"fullname": "src.fmeval.eval_algorithms", "modulename": "src.fmeval.eval_algorithms", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.EvalScore", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalScore", "kind": "class", "doc": "<p>The class that contains the aggregated scores computed for different eval offerings</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>name</strong>:  The name of the eval score offering</li>\n<li><strong>value</strong>:  The aggregated score computed for the given eval offering</li>\n</ul>\n"}, {"fullname": "src.fmeval.eval_algorithms.EvalScore.__init__", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalScore.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.EvalScore.name", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalScore.name", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "src.fmeval.eval_algorithms.EvalScore.value", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalScore.value", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "src.fmeval.eval_algorithms.EvalAlgorithm", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalAlgorithm", "kind": "class", "doc": "<p>The evaluation types supported by Amazon Foundation Model Evaluations.</p>\n\n<p>The evaluation types are used to determine the evaluation metrics for the\nmodel.</p>\n", "bases": "enum.Enum"}, {"fullname": "src.fmeval.eval_algorithms.EvalAlgorithm.PROMPT_STEREOTYPING", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalAlgorithm.PROMPT_STEREOTYPING", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;EvalAlgorithm.PROMPT_STEREOTYPING: &#x27;prompt_stereotyping&#x27;&gt;"}, {"fullname": "src.fmeval.eval_algorithms.EvalAlgorithm.FACTUAL_KNOWLEDGE", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalAlgorithm.FACTUAL_KNOWLEDGE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;EvalAlgorithm.FACTUAL_KNOWLEDGE: &#x27;factual_knowledge&#x27;&gt;"}, {"fullname": "src.fmeval.eval_algorithms.EvalAlgorithm.TOXICITY", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalAlgorithm.TOXICITY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;EvalAlgorithm.TOXICITY: &#x27;toxicity&#x27;&gt;"}, {"fullname": "src.fmeval.eval_algorithms.EvalAlgorithm.QA_TOXICITY", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalAlgorithm.QA_TOXICITY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;EvalAlgorithm.QA_TOXICITY: &#x27;qa_toxicity&#x27;&gt;"}, {"fullname": "src.fmeval.eval_algorithms.EvalAlgorithm.SUMMARIZATION_TOXICITY", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalAlgorithm.SUMMARIZATION_TOXICITY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;EvalAlgorithm.SUMMARIZATION_TOXICITY: &#x27;summarization_toxicity&#x27;&gt;"}, {"fullname": "src.fmeval.eval_algorithms.EvalAlgorithm.GENERAL_SEMANTIC_ROBUSTNESS", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalAlgorithm.GENERAL_SEMANTIC_ROBUSTNESS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;EvalAlgorithm.GENERAL_SEMANTIC_ROBUSTNESS: &#x27;general_semantic_robustness&#x27;&gt;"}, {"fullname": "src.fmeval.eval_algorithms.EvalAlgorithm.ACCURACY", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalAlgorithm.ACCURACY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;EvalAlgorithm.ACCURACY: &#x27;accuracy&#x27;&gt;"}, {"fullname": "src.fmeval.eval_algorithms.EvalAlgorithm.QA_ACCURACY", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalAlgorithm.QA_ACCURACY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;EvalAlgorithm.QA_ACCURACY: &#x27;qa_accuracy&#x27;&gt;"}, {"fullname": "src.fmeval.eval_algorithms.EvalAlgorithm.QA_ACCURACY_SEMANTIC_ROBUSTNESS", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalAlgorithm.QA_ACCURACY_SEMANTIC_ROBUSTNESS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;EvalAlgorithm.QA_ACCURACY_SEMANTIC_ROBUSTNESS: &#x27;qa_accuracy_semantic_robustness&#x27;&gt;"}, {"fullname": "src.fmeval.eval_algorithms.EvalAlgorithm.SUMMARIZATION_ACCURACY", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalAlgorithm.SUMMARIZATION_ACCURACY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;EvalAlgorithm.SUMMARIZATION_ACCURACY: &#x27;summarization_accuracy&#x27;&gt;"}, {"fullname": "src.fmeval.eval_algorithms.EvalAlgorithm.SUMMARIZATION_ACCURACY_SEMANTIC_ROBUSTNESS", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalAlgorithm.SUMMARIZATION_ACCURACY_SEMANTIC_ROBUSTNESS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;EvalAlgorithm.SUMMARIZATION_ACCURACY_SEMANTIC_ROBUSTNESS: &#x27;summarization_accuracy_semantic_robustness&#x27;&gt;"}, {"fullname": "src.fmeval.eval_algorithms.EvalAlgorithm.CLASSIFICATION_ACCURACY", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalAlgorithm.CLASSIFICATION_ACCURACY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;EvalAlgorithm.CLASSIFICATION_ACCURACY: &#x27;classification_accuracy&#x27;&gt;"}, {"fullname": "src.fmeval.eval_algorithms.EvalAlgorithm.CLASSIFICATION_ACCURACY_SEMANTIC_ROBUSTNESS", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalAlgorithm.CLASSIFICATION_ACCURACY_SEMANTIC_ROBUSTNESS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;EvalAlgorithm.CLASSIFICATION_ACCURACY_SEMANTIC_ROBUSTNESS: &#x27;classification_accuracy_semantic_robustness&#x27;&gt;"}, {"fullname": "src.fmeval.eval_algorithms.CategoryScore", "modulename": "src.fmeval.eval_algorithms", "qualname": "CategoryScore", "kind": "class", "doc": "<p>The class that contains the aggregated scores computed across specific categories in the dataset.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>name</strong>:  The name of the category.</li>\n<li><strong>scores</strong>:  The aggregated score computed for the given category.</li>\n</ul>\n"}, {"fullname": "src.fmeval.eval_algorithms.CategoryScore.__init__", "modulename": "src.fmeval.eval_algorithms", "qualname": "CategoryScore.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">scores</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalScore</span><span class=\"p\">]</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.CategoryScore.name", "modulename": "src.fmeval.eval_algorithms", "qualname": "CategoryScore.name", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "src.fmeval.eval_algorithms.CategoryScore.scores", "modulename": "src.fmeval.eval_algorithms", "qualname": "CategoryScore.scores", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List[src.fmeval.eval_algorithms.EvalScore]"}, {"fullname": "src.fmeval.eval_algorithms.EvalOutput", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalOutput", "kind": "class", "doc": "<p>The class that contains evaluation scores from <code>EvalAlgorithmInterface</code>.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>eval_name</strong>:  The name of the evaluation</li>\n<li><strong>dataset_name</strong>:  The name of dataset used by eval_algo</li>\n<li><strong>prompt_template</strong>:  A template used to compose prompts, only consumed if model_output is not provided in dataset</li>\n<li><strong>dataset_scores</strong>:  The aggregated score computed across the whole dataset.</li>\n<li><strong>category_scores</strong>:  A list of CategoryScore object that contain the scores for each category in the dataset.</li>\n<li><strong>output_path</strong>:  Local path of eval output on dataset. This output contains prompt-response with\nrecord wise eval scores</li>\n<li><strong>error</strong>:  A string error message for a failed evaluation.</li>\n</ul>\n"}, {"fullname": "src.fmeval.eval_algorithms.EvalOutput.__init__", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalOutput.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">eval_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_scores</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalScore</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_template</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">category_scores</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">CategoryScore</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">output_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.EvalOutput.eval_name", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalOutput.eval_name", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "src.fmeval.eval_algorithms.EvalOutput.dataset_name", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalOutput.dataset_name", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "src.fmeval.eval_algorithms.EvalOutput.dataset_scores", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalOutput.dataset_scores", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[List[src.fmeval.eval_algorithms.EvalScore]]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.EvalOutput.prompt_template", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalOutput.prompt_template", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.EvalOutput.category_scores", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalOutput.category_scores", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[List[src.fmeval.eval_algorithms.CategoryScore]]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.EvalOutput.output_path", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalOutput.output_path", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.EvalOutput.error", "modulename": "src.fmeval.eval_algorithms", "qualname": "EvalOutput.error", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.ModelTask", "modulename": "src.fmeval.eval_algorithms", "qualname": "ModelTask", "kind": "class", "doc": "<p>The different types of tasks that are supported by the evaluations.</p>\n\n<p>The model tasks are used to determine the evaluation metrics for the\nmodel.</p>\n", "bases": "enum.Enum"}, {"fullname": "src.fmeval.eval_algorithms.ModelTask.NO_TASK", "modulename": "src.fmeval.eval_algorithms", "qualname": "ModelTask.NO_TASK", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ModelTask.NO_TASK: &#x27;no_task&#x27;&gt;"}, {"fullname": "src.fmeval.eval_algorithms.ModelTask.CLASSIFICATION", "modulename": "src.fmeval.eval_algorithms", "qualname": "ModelTask.CLASSIFICATION", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ModelTask.CLASSIFICATION: &#x27;classification&#x27;&gt;"}, {"fullname": "src.fmeval.eval_algorithms.ModelTask.QUESTION_ANSWERING", "modulename": "src.fmeval.eval_algorithms", "qualname": "ModelTask.QUESTION_ANSWERING", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ModelTask.QUESTION_ANSWERING: &#x27;question_answering&#x27;&gt;"}, {"fullname": "src.fmeval.eval_algorithms.ModelTask.SUMMARIZATION", "modulename": "src.fmeval.eval_algorithms", "qualname": "ModelTask.SUMMARIZATION", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ModelTask.SUMMARIZATION: &#x27;summarization&#x27;&gt;"}, {"fullname": "src.fmeval.eval_algorithms.MODEL_TASK_EVALUATION_MAP", "modulename": "src.fmeval.eval_algorithms", "qualname": "MODEL_TASK_EVALUATION_MAP", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&lt;ModelTask.NO_TASK: &#x27;no_task&#x27;&gt;: [&lt;EvalAlgorithm.PROMPT_STEREOTYPING: &#x27;prompt_stereotyping&#x27;&gt;, &lt;EvalAlgorithm.FACTUAL_KNOWLEDGE: &#x27;factual_knowledge&#x27;&gt;, &lt;EvalAlgorithm.TOXICITY: &#x27;toxicity&#x27;&gt;, &lt;EvalAlgorithm.GENERAL_SEMANTIC_ROBUSTNESS: &#x27;general_semantic_robustness&#x27;&gt;], &lt;ModelTask.CLASSIFICATION: &#x27;classification&#x27;&gt;: [&lt;EvalAlgorithm.CLASSIFICATION_ACCURACY: &#x27;classification_accuracy&#x27;&gt;, &lt;EvalAlgorithm.CLASSIFICATION_ACCURACY_SEMANTIC_ROBUSTNESS: &#x27;classification_accuracy_semantic_robustness&#x27;&gt;], &lt;ModelTask.QUESTION_ANSWERING: &#x27;question_answering&#x27;&gt;: [&lt;EvalAlgorithm.QA_TOXICITY: &#x27;qa_toxicity&#x27;&gt;, &lt;EvalAlgorithm.QA_ACCURACY: &#x27;qa_accuracy&#x27;&gt;, &lt;EvalAlgorithm.QA_ACCURACY_SEMANTIC_ROBUSTNESS: &#x27;qa_accuracy_semantic_robustness&#x27;&gt;], &lt;ModelTask.SUMMARIZATION: &#x27;summarization&#x27;&gt;: [&lt;EvalAlgorithm.SUMMARIZATION_TOXICITY: &#x27;summarization_toxicity&#x27;&gt;, &lt;EvalAlgorithm.SUMMARIZATION_ACCURACY: &#x27;summarization_accuracy&#x27;&gt;, &lt;EvalAlgorithm.SUMMARIZATION_ACCURACY_SEMANTIC_ROBUSTNESS: &#x27;summarization_accuracy_semantic_robustness&#x27;&gt;]}"}, {"fullname": "src.fmeval.eval_algorithms.TREX", "modulename": "src.fmeval.eval_algorithms", "qualname": "TREX", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;trex&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.BOOLQ", "modulename": "src.fmeval.eval_algorithms", "qualname": "BOOLQ", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;boolq&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.TRIVIA_QA", "modulename": "src.fmeval.eval_algorithms", "qualname": "TRIVIA_QA", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;trivia_qa&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.NATURAL_QUESTIONS", "modulename": "src.fmeval.eval_algorithms", "qualname": "NATURAL_QUESTIONS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;natural_questions&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.CROWS_PAIRS", "modulename": "src.fmeval.eval_algorithms", "qualname": "CROWS_PAIRS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;crows-pairs&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.XSUM", "modulename": "src.fmeval.eval_algorithms", "qualname": "XSUM", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;xsum&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.GIGAWORD", "modulename": "src.fmeval.eval_algorithms", "qualname": "GIGAWORD", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;gigaword&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.GOV_REPORT", "modulename": "src.fmeval.eval_algorithms", "qualname": "GOV_REPORT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;gov_report&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.WOMENS_CLOTHING_ECOMMERCE_REVIEWS", "modulename": "src.fmeval.eval_algorithms", "qualname": "WOMENS_CLOTHING_ECOMMERCE_REVIEWS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;womens_clothing_ecommerce_reviews&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.BOLD", "modulename": "src.fmeval.eval_algorithms", "qualname": "BOLD", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;bold&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.WIKITEXT2", "modulename": "src.fmeval.eval_algorithms", "qualname": "WIKITEXT2", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;wikitext2&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.REAL_TOXICITY_PROMPTS", "modulename": "src.fmeval.eval_algorithms", "qualname": "REAL_TOXICITY_PROMPTS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;real_toxicity_prompts&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.REAL_TOXICITY_PROMPTS_CHALLENGING", "modulename": "src.fmeval.eval_algorithms", "qualname": "REAL_TOXICITY_PROMPTS_CHALLENGING", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;real_toxicity_prompts_challenging&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.EVAL_DATASETS", "modulename": "src.fmeval.eval_algorithms", "qualname": "EVAL_DATASETS", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, List[str]]", "default_value": "{&#x27;factual_knowledge&#x27;: [&#x27;trex&#x27;], &#x27;qa_accuracy&#x27;: [&#x27;boolq&#x27;, &#x27;trivia_qa&#x27;, &#x27;natural_questions&#x27;], &#x27;qa_accuracy_semantic_robustness&#x27;: [&#x27;boolq&#x27;, &#x27;trivia_qa&#x27;, &#x27;natural_questions&#x27;], &#x27;prompt_stereotyping&#x27;: [&#x27;crows-pairs&#x27;], &#x27;summarization_accuracy&#x27;: [&#x27;xsum&#x27;, &#x27;gigaword&#x27;, &#x27;gov_report&#x27;], &#x27;general_semantic_robustness&#x27;: [&#x27;bold&#x27;, &#x27;trex&#x27;, &#x27;wikitext2&#x27;], &#x27;classification_accuracy&#x27;: [&#x27;womens_clothing_ecommerce_reviews&#x27;], &#x27;classification_accuracy_semantic_robustness&#x27;: [&#x27;womens_clothing_ecommerce_reviews&#x27;], &#x27;summarization_accuracy_semantic_robustness&#x27;: [&#x27;xsum&#x27;, &#x27;gigaword&#x27;, &#x27;gov_report&#x27;], &#x27;toxicity&#x27;: [&#x27;bold&#x27;, &#x27;real_toxicity_prompts&#x27;, &#x27;real_toxicity_prompts_challenging&#x27;], &#x27;qa_toxicity&#x27;: [&#x27;boolq&#x27;, &#x27;trivia_qa&#x27;, &#x27;natural_questions&#x27;], &#x27;summarization_toxicity&#x27;: [&#x27;xsum&#x27;, &#x27;gigaword&#x27;, &#x27;gov_report&#x27;]}"}, {"fullname": "src.fmeval.eval_algorithms.DEFAULT_PROMPT_TEMPLATE", "modulename": "src.fmeval.eval_algorithms", "qualname": "DEFAULT_PROMPT_TEMPLATE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;$feature&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.BUILT_IN_DATASET_DEFAULT_PROMPT_TEMPLATES", "modulename": "src.fmeval.eval_algorithms", "qualname": "BUILT_IN_DATASET_DEFAULT_PROMPT_TEMPLATES", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;boolq&#x27;: &#x27;Respond to the following question. Valid answers are &quot;true&quot; or &quot;false&quot;. $feature Answer:&#x27;, &#x27;trivia_qa&#x27;: &#x27;Respond to the following question with a short answer: $feature Answer:&#x27;, &#x27;natural_questions&#x27;: &#x27;Respond to the following question with a short answer: $feature Answer:&#x27;, &#x27;xsum&#x27;: &#x27;Summarise the following text in one sentence: $feature&#x27;, &#x27;gigaword&#x27;: &#x27;Summarise the following text in one sentence: $feature&#x27;, &#x27;gov_report&#x27;: &#x27;Summarise the following text in a few sentences: $feature&#x27;, &#x27;womens_clothing_ecommerce_reviews&#x27;: &#x27;Classify the sentiment of the following review with 0 (negative sentiment) or 1 (positive sentiment). Review: $feature. Classification:&#x27;}"}, {"fullname": "src.fmeval.eval_algorithms.get_default_prompt_template", "modulename": "src.fmeval.eval_algorithms", "qualname": "get_default_prompt_template", "kind": "function", "doc": "<p>Util method to provide dataset specific default prompt templates. If not default is configured for the dataset,\n    the method returns a generic default prompt template.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>dataset_name</strong>:  Name of dataset</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">dataset_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.DATASET_CONFIGS", "modulename": "src.fmeval.eval_algorithms", "qualname": "DATASET_CONFIGS", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, fmeval.data_loaders.data_config.DataConfig]", "default_value": "{&#x27;trex&#x27;: DataConfig(dataset_name=&#x27;trex&#x27;, dataset_uri=&#x27;s3://fmeval/datasets/trex/trex.jsonl&#x27;, dataset_mime_type=&#x27;application/jsonlines&#x27;, model_input_location=&#x27;question&#x27;, model_output_location=None, target_output_location=&#x27;answers&#x27;, category_location=&#x27;knowledge_category&#x27;, sent_more_input_location=None, sent_less_input_location=None, sent_more_log_prob_location=None, sent_less_log_prob_location=None), &#x27;boolq&#x27;: DataConfig(dataset_name=&#x27;boolq&#x27;, dataset_uri=&#x27;s3://fmeval/datasets/boolq/boolq.jsonl&#x27;, dataset_mime_type=&#x27;application/jsonlines&#x27;, model_input_location=&#x27;question&#x27;, model_output_location=None, target_output_location=&#x27;answer&#x27;, category_location=None, sent_more_input_location=None, sent_less_input_location=None, sent_more_log_prob_location=None, sent_less_log_prob_location=None), &#x27;trivia_qa&#x27;: DataConfig(dataset_name=&#x27;trivia_qa&#x27;, dataset_uri=&#x27;s3://fmeval/datasets/triviaQA/triviaQA.jsonl&#x27;, dataset_mime_type=&#x27;application/jsonlines&#x27;, model_input_location=&#x27;question&#x27;, model_output_location=None, target_output_location=&#x27;answer&#x27;, category_location=None, sent_more_input_location=None, sent_less_input_location=None, sent_more_log_prob_location=None, sent_less_log_prob_location=None), &#x27;natural_questions&#x27;: DataConfig(dataset_name=&#x27;natural_questions&#x27;, dataset_uri=&#x27;s3://fmeval/datasets/natural_questions/natural_questions.jsonl&#x27;, dataset_mime_type=&#x27;application/jsonlines&#x27;, model_input_location=&#x27;question&#x27;, model_output_location=None, target_output_location=&#x27;answer&#x27;, category_location=None, sent_more_input_location=None, sent_less_input_location=None, sent_more_log_prob_location=None, sent_less_log_prob_location=None), &#x27;crows-pairs&#x27;: DataConfig(dataset_name=&#x27;crows-pairs&#x27;, dataset_uri=&#x27;s3://fmeval/datasets/crows-pairs/crows-pairs.jsonl&#x27;, dataset_mime_type=&#x27;application/jsonlines&#x27;, model_input_location=None, model_output_location=None, target_output_location=None, category_location=&#x27;bias_type&#x27;, sent_more_input_location=&#x27;sent_more&#x27;, sent_less_input_location=&#x27;sent_less&#x27;, sent_more_log_prob_location=None, sent_less_log_prob_location=None), &#x27;xsum&#x27;: DataConfig(dataset_name=&#x27;xsum&#x27;, dataset_uri=&#x27;s3://fmeval/datasets/xsum/xsum.jsonl&#x27;, dataset_mime_type=&#x27;application/jsonlines&#x27;, model_input_location=&#x27;document&#x27;, model_output_location=None, target_output_location=&#x27;summary&#x27;, category_location=None, sent_more_input_location=None, sent_less_input_location=None, sent_more_log_prob_location=None, sent_less_log_prob_location=None), &#x27;womens_clothing_ecommerce_reviews&#x27;: DataConfig(dataset_name=&#x27;womens_clothing_ecommerce_reviews&#x27;, dataset_uri=&#x27;s3://fmeval/datasets/womens_clothing_reviews/womens_clothing_reviews.jsonl&#x27;, dataset_mime_type=&#x27;application/jsonlines&#x27;, model_input_location=&#x27;&quot;Review Text&quot;&#x27;, model_output_location=None, target_output_location=&#x27;&quot;Recommended IND&quot;&#x27;, category_location=&#x27;&quot;Class Name&quot;&#x27;, sent_more_input_location=None, sent_less_input_location=None, sent_more_log_prob_location=None, sent_less_log_prob_location=None), &#x27;bold&#x27;: DataConfig(dataset_name=&#x27;bold&#x27;, dataset_uri=&#x27;s3://fmeval/datasets/bold/bold.jsonl&#x27;, dataset_mime_type=&#x27;application/jsonlines&#x27;, model_input_location=&#x27;prompt&#x27;, model_output_location=None, target_output_location=None, category_location=&#x27;category&#x27;, sent_more_input_location=None, sent_less_input_location=None, sent_more_log_prob_location=None, sent_less_log_prob_location=None), &#x27;wikitext2&#x27;: DataConfig(dataset_name=&#x27;wikitext2&#x27;, dataset_uri=&#x27;s3://fmeval/datasets/wikitext2/wikitext2.jsonl&#x27;, dataset_mime_type=&#x27;application/jsonlines&#x27;, model_input_location=&#x27;prompt&#x27;, model_output_location=None, target_output_location=None, category_location=None, sent_more_input_location=None, sent_less_input_location=None, sent_more_log_prob_location=None, sent_less_log_prob_location=None), &#x27;real_toxicity_prompts&#x27;: DataConfig(dataset_name=&#x27;real_toxicity_prompts&#x27;, dataset_uri=&#x27;s3://fmeval/datasets/real_toxicity/real_toxicity.jsonl&#x27;, dataset_mime_type=&#x27;application/jsonlines&#x27;, model_input_location=&#x27;prompt&#x27;, model_output_location=None, target_output_location=None, category_location=None, sent_more_input_location=None, sent_less_input_location=None, sent_more_log_prob_location=None, sent_less_log_prob_location=None), &#x27;real_toxicity_prompts_challenging&#x27;: DataConfig(dataset_name=&#x27;real_toxicity_prompts_challenging&#x27;, dataset_uri=&#x27;s3://fmeval/datasets/real_toxicity/real_toxicity_challenging.jsonl&#x27;, dataset_mime_type=&#x27;application/jsonlines&#x27;, model_input_location=&#x27;prompt&#x27;, model_output_location=None, target_output_location=None, category_location=None, sent_more_input_location=None, sent_less_input_location=None, sent_more_log_prob_location=None, sent_less_log_prob_location=None), &#x27;gigaword&#x27;: DataConfig(dataset_name=&#x27;gigaword&#x27;, dataset_uri=&#x27;s3://fmeval/datasets/gigaword/gigaword.jsonl&#x27;, dataset_mime_type=&#x27;application/jsonlines&#x27;, model_input_location=&#x27;document&#x27;, model_output_location=None, target_output_location=&#x27;summary&#x27;, category_location=None, sent_more_input_location=None, sent_less_input_location=None, sent_more_log_prob_location=None, sent_less_log_prob_location=None), &#x27;gov_report&#x27;: DataConfig(dataset_name=&#x27;gov_report&#x27;, dataset_uri=&#x27;s3://fmeval/datasets/gov_report/gov_report.jsonl&#x27;, dataset_mime_type=&#x27;application/jsonlines&#x27;, model_input_location=&#x27;report&#x27;, model_output_location=None, target_output_location=&#x27;summary&#x27;, category_location=None, sent_more_input_location=None, sent_less_input_location=None, sent_more_log_prob_location=None, sent_less_log_prob_location=None)}"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.CLASSIFICATION_ACCURACY_SCORE", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "CLASSIFICATION_ACCURACY_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;classification_accuracy_score&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.BALANCED_ACCURACY_SCORE", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "BALANCED_ACCURACY_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;balanced_accuracy_score&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.PRECISION_SCORE", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "PRECISION_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;precision_score&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.RECALL_SCORE", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "RECALL_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;recall_score&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.UNKNOWN_LABEL", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "UNKNOWN_LABEL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;unknown&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.PROMPT_COLUMN_NAME", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "PROMPT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;prompt&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.CLASSIFIED_MODEL_OUTPUT_COLUMN_NAME", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "CLASSIFIED_MODEL_OUTPUT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;classified_model_output&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.CLASSIFICATION_ACCURACY_SCORES_TO_FUNCS", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "CLASSIFICATION_ACCURACY_SCORES_TO_FUNCS", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, Callable[..., float]]", "default_value": "{&#x27;balanced_accuracy_score&#x27;: &lt;function balanced_accuracy_score&gt;, &#x27;precision_score&#x27;: &lt;function precision_score&gt;, &#x27;recall_score&#x27;: &lt;function recall_score&gt;}"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.UNIQUENESS_FACTOR", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "UNIQUENESS_FACTOR", "kind": "variable", "doc": "<p></p>\n", "default_value": "0.05"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.logger", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.eval_algorithms.classification_accuracy (WARNING)&gt;"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.convert_model_output_to_label", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "convert_model_output_to_label", "kind": "function", "doc": "<p>Convert model output to string class label. The model is expected to return a label directly (if it has a\nclassification head), or a string containing a label (if it has a language modelling head). In the latter case we\nstrip any additional text (e.g. \"The answer is 2.\" --> \"2\"). If no valid labels is contained in the\n<code>model_output</code> an \"unknown\" label is returned. Users can define other <code>converter_fn</code>s, e.g. to translate a text\nlabel to string (\"NEGATIVE\" --> \"0\").</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model_output</strong>:  Value returned by the model.</li>\n<li><strong>valid_labels</strong>:  Valid labels.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p><code>model_output</code> transformed into a label</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">valid_labels</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "ClassificationAccuracyConfig", "kind": "class", "doc": "<p>Configuration for the Classification Accuracy Evaluation</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>valid_labels</strong>:  The labels of the classes predicted from the model.</li>\n<li><strong>converter_fn</strong>:  Function to process model output to labels, defaults to simple integer conversion.</li>\n<li><strong>multiclass_average_strategy</strong>:  <code>average</code> to be passed to sklearn's precision and recall scores.\nThis determines how scores are aggregated in the multiclass classification setting\n(see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html).\nOptions are {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, default='micro'.</li>\n</ul>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmConfig"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig.__init__", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "ClassificationAccuracyConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">valid_labels</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">converter_fn</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]],</span> <span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">function</span> <span class=\"n\">convert_model_output_to_label</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">multiclass_average_strategy</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;micro&#39;</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig.valid_labels", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "ClassificationAccuracyConfig.valid_labels", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[List[str]]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig.converter_fn", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "ClassificationAccuracyConfig.converter_fn", "kind": "function", "doc": "<p>Convert model output to string class label. The model is expected to return a label directly (if it has a\nclassification head), or a string containing a label (if it has a language modelling head). In the latter case we\nstrip any additional text (e.g. \"The answer is 2.\" --> \"2\"). If no valid labels is contained in the\n<code>model_output</code> an \"unknown\" label is returned. Users can define other <code>converter_fn</code>s, e.g. to translate a text\nlabel to string (\"NEGATIVE\" --> \"0\").</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model_output</strong>:  Value returned by the model.</li>\n<li><strong>valid_labels</strong>:  Valid labels.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p><code>model_output</code> transformed into a label</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">valid_labels</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracyConfig.multiclass_average_strategy", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "ClassificationAccuracyConfig.multiclass_average_strategy", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "&#x27;micro&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "ClassificationAccuracy", "kind": "class", "doc": "<p>Interface class for eval algorithms.</p>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy.__init__", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "ClassificationAccuracy.__init__", "kind": "function", "doc": "<p>Default constructor</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>eval_algorithm_config</strong>:  Classification Accuracy eval algorithm config.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">eval_algorithm_config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">classification_accuracy</span><span class=\"o\">.</span><span class=\"n\">ClassificationAccuracyConfig</span> <span class=\"o\">=</span> <span class=\"n\">ClassificationAccuracyConfig</span><span class=\"p\">(</span><span class=\"n\">valid_labels</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">converter_fn</span><span class=\"o\">=&lt;</span><span class=\"n\">function</span> <span class=\"n\">convert_model_output_to_label</span> <span class=\"n\">at</span> <span class=\"mh\">0x289a68ca0</span><span class=\"o\">&gt;</span><span class=\"p\">,</span> <span class=\"n\">multiclass_average_strategy</span><span class=\"o\">=</span><span class=\"s1\">&#39;micro&#39;</span><span class=\"p\">)</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy.eval_name", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "ClassificationAccuracy.eval_name", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;classification_accuracy&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy.evaluate", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "ClassificationAccuracy.evaluate", "kind": "function", "doc": "<p>Classification Accuracy evaluate.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model</strong>:  An instance of ModelRunner which is the model under evaluation</li>\n<li><strong>dataset_config</strong>:  The config to load the dataset to use for evaluation. If not provided, model will be\nevaluated on all built-in datasets configured for this evaluation.</li>\n<li><strong>prompt_template</strong>:  A template which can be used to generate prompts, optional, if not provided defaults\nwill be used.</li>\n<li><strong>save</strong>:  If set to true, prompt responses and scores will be saved to file. The output is written to\nEvalAlgorithmInterface.EVAL_RESULTS_PATH</li>\n<li><strong>num_records</strong>:  The number of records to be sampled randomly from the input dataset to perform the\nevaluation\n:returns: List of EvalOutput objects. Current implementation returns only one score.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">model_runner</span><span class=\"o\">.</span><span class=\"n\">ModelRunner</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_config</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">data_config</span><span class=\"o\">.</span><span class=\"n\">DataConfig</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_template</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">num_records</span><span class=\"o\">=</span><span class=\"mi\">100</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalOutput</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy.ClassificationAccuracy.evaluate_sample", "modulename": "src.fmeval.eval_algorithms.classification_accuracy", "qualname": "ClassificationAccuracy.evaluate_sample", "kind": "function", "doc": "<p>Evaluate a single Classification record.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model_output</strong>:  An instance of ModelOutput which contains the responses from the model needed for this\nevaluation.</li>\n<li><strong>target_output</strong>:  The expected responses from the model.\n:returns: A List of EvalScores computed for prompts and responses.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">target_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalScore</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.PERTURBATION_TYPE_TO_HELPER_CLASS", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "PERTURBATION_TYPE_TO_HELPER_CLASS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;butter_finger&#x27;: &lt;class &#x27;fmeval.eval_algorithms.semantic_perturbation_utils.ButterFinger&#x27;&gt;, &#x27;random_upper_case&#x27;: &lt;class &#x27;fmeval.eval_algorithms.semantic_perturbation_utils.RandomUpperCase&#x27;&gt;, &#x27;whitespace_add_remove&#x27;: &lt;class &#x27;fmeval.eval_algorithms.semantic_perturbation_utils.WhitespaceAddRemove&#x27;&gt;}"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.PREFIX_FOR_DELTA_SCORES", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "PREFIX_FOR_DELTA_SCORES", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;delta_&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.DELTA_CLASSIFICATION_ACCURACY_SCORE", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "DELTA_CLASSIFICATION_ACCURACY_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;delta_classification_accuracy_score&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.PROMPT_COLUMN_NAME", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "PROMPT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;prompt&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.logger", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness (WARNING)&gt;"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.ClassificationAccuracySemanticRobustnessConfig", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "ClassificationAccuracySemanticRobustnessConfig", "kind": "class", "doc": "<p>Configuration for the Classification Accuracy Semantic Robustness Evaluation</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>valid_labels</strong>:  List of valid string label</li>\n<li><strong>converter_fn</strong>:  Function to process model output to labels, defaults to simple integer conversion</li>\n<li><strong>perturbation_type</strong>:  perturbation type for generating perturbed inputs</li>\n<li><strong>num_perturbations</strong>:  Number of perturbed inputs to be generated for robustness evaluation</li>\n<li><strong>butter_finger_perturbation_prob</strong>:  The probability that a given character will be perturbed. Used for\nbutter_finger perturbation_type</li>\n<li><strong>random_uppercase_corrupt_proportion</strong>:  Fraction of characters to be changed to uppercase. Used for\nrandom_upper_case perturbation_type</li>\n<li><strong>whitespace_remove_prob</strong>:  Given a whitespace, remove it with this much probability. Used for\nwhitespace_add_remove perturbation_type</li>\n<li><strong>whitespace_add_prob</strong>:  Given a non-whitespace, add a whitespace before it with this probability. Used for\nwhitespace_add_remove perturbation_type</li>\n</ul>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmConfig"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.ClassificationAccuracySemanticRobustnessConfig.__init__", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "ClassificationAccuracySemanticRobustnessConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">valid_labels</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">converter_fn</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]],</span> <span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">function</span> <span class=\"n\">convert_model_output_to_label</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">perturbation_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;butter_finger&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">num_perturbations</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">butter_finger_perturbation_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">random_uppercase_corrupt_proportion</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">whitespace_remove_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">whitespace_add_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.05</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.ClassificationAccuracySemanticRobustnessConfig.valid_labels", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "ClassificationAccuracySemanticRobustnessConfig.valid_labels", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[List[str]]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.ClassificationAccuracySemanticRobustnessConfig.converter_fn", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "ClassificationAccuracySemanticRobustnessConfig.converter_fn", "kind": "function", "doc": "<p>Convert model output to string class label. The model is expected to return a label directly (if it has a\nclassification head), or a string containing a label (if it has a language modelling head). In the latter case we\nstrip any additional text (e.g. \"The answer is 2.\" --> \"2\"). If no valid labels is contained in the\n<code>model_output</code> an \"unknown\" label is returned. Users can define other <code>converter_fn</code>s, e.g. to translate a text\nlabel to string (\"NEGATIVE\" --> \"0\").</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model_output</strong>:  Value returned by the model.</li>\n<li><strong>valid_labels</strong>:  Valid labels.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p><code>model_output</code> transformed into a label</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">valid_labels</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.ClassificationAccuracySemanticRobustnessConfig.perturbation_type", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "ClassificationAccuracySemanticRobustnessConfig.perturbation_type", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;butter_finger&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.ClassificationAccuracySemanticRobustnessConfig.num_perturbations", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "ClassificationAccuracySemanticRobustnessConfig.num_perturbations", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "5"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.ClassificationAccuracySemanticRobustnessConfig.butter_finger_perturbation_prob", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "ClassificationAccuracySemanticRobustnessConfig.butter_finger_perturbation_prob", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.1"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.ClassificationAccuracySemanticRobustnessConfig.random_uppercase_corrupt_proportion", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "ClassificationAccuracySemanticRobustnessConfig.random_uppercase_corrupt_proportion", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.1"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.ClassificationAccuracySemanticRobustnessConfig.whitespace_remove_prob", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "ClassificationAccuracySemanticRobustnessConfig.whitespace_remove_prob", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.1"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.ClassificationAccuracySemanticRobustnessConfig.whitespace_add_prob", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "ClassificationAccuracySemanticRobustnessConfig.whitespace_add_prob", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.05"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.CLASSIFICATION_ACCURACY_SEMANTIC_ROBUSTNESS", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "CLASSIFICATION_ACCURACY_SEMANTIC_ROBUSTNESS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;classification_accuracy_semantic_robustness&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.ClassificationAccuracySemanticRobustness", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "ClassificationAccuracySemanticRobustness", "kind": "class", "doc": "<p>Classification Accuracy Eval algorithm</p>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.ClassificationAccuracySemanticRobustness.__init__", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "ClassificationAccuracySemanticRobustness.__init__", "kind": "function", "doc": "<p>Default constructor</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>eval_algorithm_config</strong>:  Classification Accuracy Semantic Robustness eval algorithm config.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">eval_algorithm_config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">classification_accuracy_semantic_robustness</span><span class=\"o\">.</span><span class=\"n\">ClassificationAccuracySemanticRobustnessConfig</span> <span class=\"o\">=</span> <span class=\"n\">ClassificationAccuracySemanticRobustnessConfig</span><span class=\"p\">(</span><span class=\"n\">valid_labels</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">converter_fn</span><span class=\"o\">=&lt;</span><span class=\"n\">function</span> <span class=\"n\">convert_model_output_to_label</span> <span class=\"n\">at</span> <span class=\"mh\">0x17abfe9e0</span><span class=\"o\">&gt;</span><span class=\"p\">,</span> <span class=\"n\">perturbation_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;butter_finger&#39;</span><span class=\"p\">,</span> <span class=\"n\">num_perturbations</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">butter_finger_perturbation_prob</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">random_uppercase_corrupt_proportion</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">whitespace_remove_prob</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">whitespace_add_prob</span><span class=\"o\">=</span><span class=\"mf\">0.05</span><span class=\"p\">)</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.ClassificationAccuracySemanticRobustness.eval_name", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "ClassificationAccuracySemanticRobustness.eval_name", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;classification_accuracy_semantic_robustness&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.ClassificationAccuracySemanticRobustness.evaluate", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "ClassificationAccuracySemanticRobustness.evaluate", "kind": "function", "doc": "<p>Classification Accuracy Semantic Robustness evaluate.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model</strong>:  An instance of ModelRunner which is the model under evaluation</li>\n<li><strong>dataset_config</strong>:  Configures the single dataset used for evaluation. If not provided,\nevaluation will use all of it's supported built-in datasets</li>\n<li><strong>prompt_template</strong>:  A template which can be used to generate prompts, optional, if not provided defaults\nwill be used.</li>\n<li><strong>save</strong>:  If set to true, prompt responses and scores will be saved to file. The output is written to\nEvalAlgorithmInterface.EVAL_RESULTS_PATH</li>\n<li><strong>num_records</strong>:  The number of records to be sampled randomly from the input dataset to perform the\nevaluation\n:returns: A List of EvalOutput objects.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">model_runner</span><span class=\"o\">.</span><span class=\"n\">ModelRunner</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_config</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">data_config</span><span class=\"o\">.</span><span class=\"n\">DataConfig</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_template</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">num_records</span><span class=\"o\">=</span><span class=\"mi\">100</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalOutput</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness.ClassificationAccuracySemanticRobustness.evaluate_sample", "modulename": "src.fmeval.eval_algorithms.classification_accuracy_semantic_robustness", "qualname": "ClassificationAccuracySemanticRobustness.evaluate_sample", "kind": "function", "doc": "<p>Evaluate a single record for Classification Accuracy Semantic Robustness.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model_input</strong>:  text input for model</li>\n<li><strong>model</strong>:  An instance of ModelRunner which is the model under evaluation</li>\n<li><strong>target_output</strong>:  The expected responses from the model</li>\n<li><strong>model_output</strong>:  The output of a model that we want to evaluate</li>\n<li><strong>prompt_template</strong>:  A template which can be used to compose prompt using model_input\n:returns: A List of EvalScores computed for prompts and responses.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model_input</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">model_runner</span><span class=\"o\">.</span><span class=\"n\">ModelRunner</span>,</span><span class=\"param\">\t<span class=\"n\">target_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_output</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_template</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;$feature&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalScore</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.eval_algorithm", "modulename": "src.fmeval.eval_algorithms.eval_algorithm", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmConfig", "modulename": "src.fmeval.eval_algorithms.eval_algorithm", "qualname": "EvalAlgorithmConfig", "kind": "class", "doc": "<p>Config class to be used or extended to provide eval algorithm specific parameters.</p>\n"}, {"fullname": "src.fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface", "modulename": "src.fmeval.eval_algorithms.eval_algorithm", "qualname": "EvalAlgorithmInterface", "kind": "class", "doc": "<p>Interface class for eval algorithms.</p>\n", "bases": "abc.ABC"}, {"fullname": "src.fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface.__init__", "modulename": "src.fmeval.eval_algorithms.eval_algorithm", "qualname": "EvalAlgorithmInterface.__init__", "kind": "function", "doc": "<p>Initialize an instance of a subclass of EvalAlgorithmConfig</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>eval_algorithm_config</strong>:  An instance of the subclass of EvalAlgorithmConfig specific to the\ncurrent evaluation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">eval_algorithm_config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">eval_algorithm</span><span class=\"o\">.</span><span class=\"n\">EvalAlgorithmConfig</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface.eval_name", "modulename": "src.fmeval.eval_algorithms.eval_algorithm", "qualname": "EvalAlgorithmInterface.eval_name", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "src.fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface.evaluate", "modulename": "src.fmeval.eval_algorithms.eval_algorithm", "qualname": "EvalAlgorithmInterface.evaluate", "kind": "function", "doc": "<p>Computes the evaluation score for dataset(s).</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model</strong>:  An instance of ModelRunner which is the model under evaluation</li>\n<li><strong>dataset_config</strong>:  Configures the single dataset used for evaluation. If not provided,\nevaluation will use all of it's supported built-in datasets</li>\n<li><strong>prompt_template</strong>:  A template which can be used to generate prompts, optional for the built-in datasets.</li>\n<li><strong>save</strong>:  If set to true, prompt responses and scores will be saved to file. The output is written to\nEvalAlgorithmInterface.EVAL_RESULTS_PATH</li>\n<li><strong>num_records</strong>:  The number of records to be sampled randomly from the input dataset to perform the\nevaluation</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>List of EvalOutput objects.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">model_runner</span><span class=\"o\">.</span><span class=\"n\">ModelRunner</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_config</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">data_config</span><span class=\"o\">.</span><span class=\"n\">DataConfig</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_template</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">num_records</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">100</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalOutput</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface.evaluate_sample", "modulename": "src.fmeval.eval_algorithms.eval_algorithm", "qualname": "EvalAlgorithmInterface.evaluate_sample", "kind": "function", "doc": "<p>Computes the evaluation score for one instance.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model_output</strong>:  An instance of ModelOutput which contains the responses from the model needed for this\nevaluation</li>\n<li><strong>model_input</strong>:  An instance of ModelInput which contains the prompts on which the model needs to be\nevaluated on</li>\n<li><strong>target_output</strong>:  The expected responses for the prompts in model_input</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>list evaluation scores for the sample.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model_input</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_output</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_output</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalScore</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.factual_knowledge", "modulename": "src.fmeval.eval_algorithms.factual_knowledge", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.factual_knowledge.PROMPT_COLUMN_NAME", "modulename": "src.fmeval.eval_algorithms.factual_knowledge", "qualname": "PROMPT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;prompt&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.factual_knowledge.logger", "modulename": "src.fmeval.eval_algorithms.factual_knowledge", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.eval_algorithms.factual_knowledge (WARNING)&gt;"}, {"fullname": "src.fmeval.eval_algorithms.factual_knowledge.FactualKnowledgeConfig", "modulename": "src.fmeval.eval_algorithms.factual_knowledge", "qualname": "FactualKnowledgeConfig", "kind": "class", "doc": "<p>Configuration for the factual knowledge eval algorithm</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>target_output_delimiter</strong>:  Target Output can have multiple answers. We expect customer to combine all the\npossible answers into a single string and use the delimiter to separate them. For instance,\nif the answers are [\"UK\", \"England\"] and the delimiter=\"<OR>\", then the target_output should be \"UK<OR>England\".</li>\n</ul>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmConfig"}, {"fullname": "src.fmeval.eval_algorithms.factual_knowledge.FactualKnowledgeConfig.__init__", "modulename": "src.fmeval.eval_algorithms.factual_knowledge", "qualname": "FactualKnowledgeConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">target_output_delimiter</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;&lt;OR&gt;&#39;</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.factual_knowledge.FactualKnowledgeConfig.target_output_delimiter", "modulename": "src.fmeval.eval_algorithms.factual_knowledge", "qualname": "FactualKnowledgeConfig.target_output_delimiter", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "&#x27;&lt;OR&gt;&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.factual_knowledge.FACTUAL_KNOWLEDGE", "modulename": "src.fmeval.eval_algorithms.factual_knowledge", "qualname": "FACTUAL_KNOWLEDGE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;factual_knowledge&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.factual_knowledge.FactualKnowledge", "modulename": "src.fmeval.eval_algorithms.factual_knowledge", "qualname": "FactualKnowledge", "kind": "class", "doc": "<p>Factual Knowledge Eval algorithm</p>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface"}, {"fullname": "src.fmeval.eval_algorithms.factual_knowledge.FactualKnowledge.__init__", "modulename": "src.fmeval.eval_algorithms.factual_knowledge", "qualname": "FactualKnowledge.__init__", "kind": "function", "doc": "<p>Default constructor</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>eval_algorithm_config</strong>:  Factual knowledge eval algorithm config.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">eval_algorithm_config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">factual_knowledge</span><span class=\"o\">.</span><span class=\"n\">FactualKnowledgeConfig</span> <span class=\"o\">=</span> <span class=\"n\">FactualKnowledgeConfig</span><span class=\"p\">(</span><span class=\"n\">target_output_delimiter</span><span class=\"o\">=</span><span class=\"s1\">&#39;&lt;OR&gt;&#39;</span><span class=\"p\">)</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.factual_knowledge.FactualKnowledge.eval_name", "modulename": "src.fmeval.eval_algorithms.factual_knowledge", "qualname": "FactualKnowledge.eval_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.factual_knowledge.FactualKnowledge.evaluate_sample", "modulename": "src.fmeval.eval_algorithms.factual_knowledge", "qualname": "FactualKnowledge.evaluate_sample", "kind": "function", "doc": "<p>Factual knowledge evaluate sample.</p>\n\n<p>Given an input prompt e.g., \"London is the capital of\" and expected answers(target_output) like\n[\"United Kingdom\", \"England\"], if the model is able to arrive at the correct completion(model_output).\nGenerating any of the expected answers is considered a correct completion.\nSince models might generate long outputs, this evaluation does not look for an exact match.\nIt considers the completion to be correct if the answer is contained within the model output generated.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>target_output</strong>:  The expected responses from the model</li>\n<li><strong>model_output</strong>:  The output of a model that we want to evaluate.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>list of EvalScore object</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">target_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalScore</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.factual_knowledge.FactualKnowledge.evaluate", "modulename": "src.fmeval.eval_algorithms.factual_knowledge", "qualname": "FactualKnowledge.evaluate", "kind": "function", "doc": "<p>Factual knowledge evaluate.</p>\n\n<p>Given an input prompt e.g., \"London is the capital of\" and expected answers(target_output) like\n[\"United Kingdom\", \"England\"], if the model is able to arrive at the correct completion(model_output).\nGenerating any of the expected answers is considered a correct completion.\nSince models might generate long outputs, this evaluation does not look for an exact match.\nIt considers the completion to be correct if the answer is contained within the model output generated.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model</strong>:  An instance of ModelRunner which is the model under evaluation</li>\n<li><strong>dataset_config</strong>:  The config to load the dataset to use for evaluation. If not provided, model will be\nevaluated on all built-in datasets configured for this evaluation.</li>\n<li><strong>prompt_template</strong>:  A template which can be used to generate prompts, optional, if not provided defaults\nwill be used.</li>\n<li><strong>save</strong>:  If set to true, prompt responses and scores will be saved to file. The output is written to\nEvalAlgorithmInterface.EVAL_RESULTS_PATH</li>\n<li><strong>num_records</strong>:  The number of records to be sampled randomly from the input dataset to perform the\nevaluation</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>List of EvalOutput objects. Current implementation returns only one score.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">model_runner</span><span class=\"o\">.</span><span class=\"n\">ModelRunner</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_config</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">data_config</span><span class=\"o\">.</span><span class=\"n\">DataConfig</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_template</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">num_records</span><span class=\"o\">=</span><span class=\"mi\">300</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalOutput</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness.logger", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.eval_algorithms.general_semantic_robustness (WARNING)&gt;"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness.PERTURBATION_TYPE_TO_HELPER_CLASS", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "qualname": "PERTURBATION_TYPE_TO_HELPER_CLASS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;butter_finger&#x27;: &lt;class &#x27;fmeval.eval_algorithms.semantic_perturbation_utils.ButterFinger&#x27;&gt;, &#x27;random_upper_case&#x27;: &lt;class &#x27;fmeval.eval_algorithms.semantic_perturbation_utils.RandomUpperCase&#x27;&gt;, &#x27;whitespace_add_remove&#x27;: &lt;class &#x27;fmeval.eval_algorithms.semantic_perturbation_utils.WhitespaceAddRemove&#x27;&gt;}"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness.WER_SCORE", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "qualname": "WER_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;word_error_rate&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness.PROMPT_COLUMN_NAME", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "qualname": "PROMPT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;prompt&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness.GeneralSemanticRobustnessConfig", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "qualname": "GeneralSemanticRobustnessConfig", "kind": "class", "doc": "<p>Configuration for the general semantic robustness eval algorithm.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>perturbation_type</strong>:  perturbation type for generating perturbed inputs</li>\n<li><strong>num_perturbations</strong>:  Number of perturbed inputs to be generated for robustness evaluation</li>\n<li><strong>butter_finger_perturbation_prob</strong>:  The probability that a given character will be perturbed. Used for\nbutter_finger perturbation_type</li>\n<li><strong>random_uppercase_corrupt_proportion</strong>:  Fraction of characters to be changed to uppercase. Used for\nrandom_upper_case perturbation_type</li>\n<li><strong>whitespace_remove_prob</strong>:  Given a whitespace, remove it with this much probability. Used for\nwhitespace_add_remove perturbation_type</li>\n<li><strong>whitespace_add_prob</strong>:  Given a non-whitespace, add a whitespace before it with this probability. Used for\nwhitespace_add_remove perturbation_type</li>\n</ul>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmConfig"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness.GeneralSemanticRobustnessConfig.__init__", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "qualname": "GeneralSemanticRobustnessConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">perturbation_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;butter_finger&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">num_perturbations</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">butter_finger_perturbation_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">random_uppercase_corrupt_proportion</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">whitespace_remove_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">whitespace_add_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.05</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness.GeneralSemanticRobustnessConfig.perturbation_type", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "qualname": "GeneralSemanticRobustnessConfig.perturbation_type", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;butter_finger&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness.GeneralSemanticRobustnessConfig.num_perturbations", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "qualname": "GeneralSemanticRobustnessConfig.num_perturbations", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "5"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness.GeneralSemanticRobustnessConfig.butter_finger_perturbation_prob", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "qualname": "GeneralSemanticRobustnessConfig.butter_finger_perturbation_prob", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.1"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness.GeneralSemanticRobustnessConfig.random_uppercase_corrupt_proportion", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "qualname": "GeneralSemanticRobustnessConfig.random_uppercase_corrupt_proportion", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.1"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness.GeneralSemanticRobustnessConfig.whitespace_remove_prob", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "qualname": "GeneralSemanticRobustnessConfig.whitespace_remove_prob", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.1"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness.GeneralSemanticRobustnessConfig.whitespace_add_prob", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "qualname": "GeneralSemanticRobustnessConfig.whitespace_add_prob", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.05"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness.GeneralSemanticRobustness", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "qualname": "GeneralSemanticRobustness", "kind": "class", "doc": "<p>Semantic Robustness Eval algorithm for General task LLMs</p>\n\n<p>This evaluation measures how much the model output changes as a result of semantic preserving\nperturbations. Given the input, e.g., \"A quick brown fox jumps over the lazy dog\", the\nevaluation creates a perturbation that preserves the semantic meaning of the input e.g.,\nwhitespace perturbation that changes the input text to \"A q uick bro wn fox ju mps overthe lazy\ndog\". The evaluation then measures how much the model output changes when prompted with the\noriginal vs. perturbed input. The output difference is measured using Word Error Rate (WER).\n<a href=\"https://huggingface.co/spaces/evaluate-metric/wer\">https://huggingface.co/spaces/evaluate-metric/wer</a></p>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness.GeneralSemanticRobustness.__init__", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "qualname": "GeneralSemanticRobustness.__init__", "kind": "function", "doc": "<p>Default constructor</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>eval_algorithm_config</strong>:  General Semantic Robustness eval algorithm config.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">eval_algorithm_config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">general_semantic_robustness</span><span class=\"o\">.</span><span class=\"n\">GeneralSemanticRobustnessConfig</span> <span class=\"o\">=</span> <span class=\"n\">GeneralSemanticRobustnessConfig</span><span class=\"p\">(</span><span class=\"n\">perturbation_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;butter_finger&#39;</span><span class=\"p\">,</span> <span class=\"n\">num_perturbations</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">butter_finger_perturbation_prob</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">random_uppercase_corrupt_proportion</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">whitespace_remove_prob</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">whitespace_add_prob</span><span class=\"o\">=</span><span class=\"mf\">0.05</span><span class=\"p\">)</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness.GeneralSemanticRobustness.eval_name", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "qualname": "GeneralSemanticRobustness.eval_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness.GeneralSemanticRobustness.evaluate_sample", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "qualname": "GeneralSemanticRobustness.evaluate_sample", "kind": "function", "doc": "<p>Semantic Robustness evaluate sample.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model_input</strong>:  text input for model</li>\n<li><strong>model</strong>:  An instance of ModelRunner which is the model under evaluation</li>\n<li><strong>model_output</strong>:  The output of a model that we want to evaluate.</li>\n<li><strong>prompt_template</strong>:  A template which can be used to compose prompt using model_input</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>list of EvalScore object</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model_input</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">model_runner</span><span class=\"o\">.</span><span class=\"n\">ModelRunner</span>,</span><span class=\"param\">\t<span class=\"n\">model_output</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_template</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;$feature&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalScore</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.general_semantic_robustness.GeneralSemanticRobustness.evaluate", "modulename": "src.fmeval.eval_algorithms.general_semantic_robustness", "qualname": "GeneralSemanticRobustness.evaluate", "kind": "function", "doc": "<p>Semantic Robustness evaluate.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model</strong>:  An instance of ModelRunner which is the model under evaluation</li>\n<li><strong>dataset_config</strong>:  Configures the single dataset used for evaluation. If not provided,\nevaluation will use all of it's supported built-in datasets</li>\n<li><strong>prompt_template</strong>:  A template which can be used to generate prompts, optional, if not provided defaults\nwill be used.</li>\n<li><strong>save</strong>:  If set to true, prompt responses and scores will be saved to file. The output is written to\nEvalAlgorithmInterface.EVAL_RESULTS_PATH</li>\n<li><strong>num_records</strong>:  The number of records to be sampled randomly from the input dataset to perform the\nevaluation</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>List of EvalOutput objects.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">model_runner</span><span class=\"o\">.</span><span class=\"n\">ModelRunner</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_config</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">data_config</span><span class=\"o\">.</span><span class=\"n\">DataConfig</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_template</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">num_records</span><span class=\"o\">=</span><span class=\"mi\">100</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalOutput</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.helper_models", "modulename": "src.fmeval.eval_algorithms.helper_models", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.TOXIGEN_SCORE_NAME", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "TOXIGEN_SCORE_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;toxicity&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.DETOXIFY_SCORE_TOXICITY", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "DETOXIFY_SCORE_TOXICITY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;toxicity&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.DETOXIFY_SCORE_SEVERE_TOXICITY", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "DETOXIFY_SCORE_SEVERE_TOXICITY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;severe_toxicity&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.DETOXIFY_SCORE_OBSCENE", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "DETOXIFY_SCORE_OBSCENE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;obscene&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.DETOXIFY_SCORE_IDENTITY_ATTACK", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "DETOXIFY_SCORE_IDENTITY_ATTACK", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;identity_attack&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.DETOXIFY_SCORE_INSULT", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "DETOXIFY_SCORE_INSULT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;insult&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.DETOXIFY_SCORE_THREAT", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "DETOXIFY_SCORE_THREAT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;threat&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.DETOXIFY_SCORE_SEXUAL_EXPLICIT", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "DETOXIFY_SCORE_SEXUAL_EXPLICIT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;sexual_explicit&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.DETOXIFY_SCORE_NAMES", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "DETOXIFY_SCORE_NAMES", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;toxicity&#x27;, &#x27;severe_toxicity&#x27;, &#x27;obscene&#x27;, &#x27;identity_attack&#x27;, &#x27;insult&#x27;, &#x27;threat&#x27;, &#x27;sexual_explicit&#x27;]"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.BaseHelperModel", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "BaseHelperModel", "kind": "class", "doc": "<p>Base class for 3P helper model invoker. Note: These Helper models are inherently\nMachine learning models being used by Evaluation algorithms.</p>\n", "bases": "abc.ABC"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.BaseHelperModel.get_helper_scores", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "BaseHelperModel.get_helper_scores", "kind": "function", "doc": "<p>Method to invoke helper model</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>text_input</strong>:  model text input\n:returns: model output</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">text_input</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.ToxigenHelperModel", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "ToxigenHelperModel", "kind": "class", "doc": "<p>Helper model for toxigen model: <a href=\"https://huggingface.co/tomh/toxigen_roberta/tree/main\">https://huggingface.co/tomh/toxigen_roberta/tree/main</a></p>\n", "bases": "BaseHelperModel"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.ToxigenHelperModel.__init__", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "ToxigenHelperModel.__init__", "kind": "function", "doc": "<p>Constructor to locally load the helper model for inference.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>column_name</strong>:  column name used to fetch input texts in __call__ method</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">column_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;model_output&#39;</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.ToxigenHelperModel.TOXIGEN_MODEL_NAME", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "ToxigenHelperModel.TOXIGEN_MODEL_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;tomh/toxigen_roberta&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.ToxigenHelperModel.COLUMN_NAME", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "ToxigenHelperModel.COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;model_output&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.ToxigenHelperModel.get_helper_scores", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "ToxigenHelperModel.get_helper_scores", "kind": "function", "doc": "<p>Method to get scores from ToxigenHelper</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>text_input</strong>:  list of text inputs for the model\n:returns: dict with key as score name and value being list of scores for text inputs</li>\n</ul>\n\n<p>Note: Toxigen scores are for label: LABEL_1</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">text_input</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.ToxigenHelperModel.get_score_names", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "ToxigenHelperModel.get_score_names", "kind": "function", "doc": "<p>Util method to return name of scores generated by helper model\n:returns: List of score names</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.DetoxifyHelperModel", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "DetoxifyHelperModel", "kind": "class", "doc": "<p>Helper model for Detoxify: <a href=\"https://github.com/unitaryai/detoxify\">https://github.com/unitaryai/detoxify</a></p>\n\n<p>TODO: To be switched to consuming HF model once consistency issue is resolved:\n<a href=\"https://huggingface.co/unitary/unbiased-toxic-roberta\">https://huggingface.co/unitary/unbiased-toxic-roberta</a>. This will allow removing detoxify PyPI as a dependency,\nupdate transformers version we are consuming.</p>\n", "bases": "BaseHelperModel"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.DetoxifyHelperModel.__init__", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "DetoxifyHelperModel.__init__", "kind": "function", "doc": "<p>Constructor to locally load the helper model for inference.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>column_name</strong>:  column name used to fetch input texts in __call__ method</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">column_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;model_output&#39;</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.DetoxifyHelperModel.DETOXIFY_MODEL_TYPE", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "DetoxifyHelperModel.DETOXIFY_MODEL_TYPE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;unbiased&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.DetoxifyHelperModel.COLUMN_NAME", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "DetoxifyHelperModel.COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;model_output&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.DetoxifyHelperModel.get_helper_scores", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "DetoxifyHelperModel.get_helper_scores", "kind": "function", "doc": "<p>Method to get scores from DetoxifyHelper</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>text_input</strong>:  list of text inputs for the model\n:returns: dict with keys as score name and value being list of scores for text inputs</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">text_input</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.DetoxifyHelperModel.get_score_names", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "DetoxifyHelperModel.get_score_names", "kind": "function", "doc": "<p>Util method to return name of scores generated by helper model\n:returns: List of score names</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.helper_models.helper_model.BertscoreHelperModel", "modulename": "src.fmeval.eval_algorithms.helper_models.helper_model", "qualname": "BertscoreHelperModel", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;src.fmeval.eval_algorithms.helper_models.helper_model.ActorClass(BertscoreHelperModel) object&gt;"}, {"fullname": "src.fmeval.eval_algorithms.prompt_stereotyping", "modulename": "src.fmeval.eval_algorithms.prompt_stereotyping", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.prompt_stereotyping.LOG_PROBABILITY_DIFFERENCE", "modulename": "src.fmeval.eval_algorithms.prompt_stereotyping", "qualname": "LOG_PROBABILITY_DIFFERENCE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;log_probability_difference&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.prompt_stereotyping.PROMPT_STEREOTYPING", "modulename": "src.fmeval.eval_algorithms.prompt_stereotyping", "qualname": "PROMPT_STEREOTYPING", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;prompt_stereotyping&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.prompt_stereotyping.logger", "modulename": "src.fmeval.eval_algorithms.prompt_stereotyping", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.eval_algorithms.prompt_stereotyping (WARNING)&gt;"}, {"fullname": "src.fmeval.eval_algorithms.prompt_stereotyping.PromptStereotyping", "modulename": "src.fmeval.eval_algorithms.prompt_stereotyping", "qualname": "PromptStereotyping", "kind": "class", "doc": "<p>Stereotyping evaluation algorithm.</p>\n\n<p>This evaluation is based on the idea in Nangia et al. (https://arxiv.org/pdf/2010.00133.pdf). The dataset consists\nof pairs of sentences, one that is more stereotyping and the other that is less stereotyping. The evaluation\ncomputes the difference in likelihood that the model assigns to each of the sentences. If $p_{more}$ is the\nprobability assigned to the more stereotypical sentence and $p_{less}$ is the probability assigned to the less\nstereotypical sentence, then the model exhibits stereotypes on this pair if $p_{more} &gt; p_{less}$. The degree of\nstereotyping is quantified as $\\log(p_{more} / p_{less}) = \\log(p_{more}) - \\log(p_{less}) $</p>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface"}, {"fullname": "src.fmeval.eval_algorithms.prompt_stereotyping.PromptStereotyping.__init__", "modulename": "src.fmeval.eval_algorithms.prompt_stereotyping", "qualname": "PromptStereotyping.__init__", "kind": "function", "doc": "<p>Initialize an instance of a subclass of EvalAlgorithmConfig</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>eval_algorithm_config</strong>:  An instance of the subclass of EvalAlgorithmConfig specific to the\ncurrent evaluation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">()</span>"}, {"fullname": "src.fmeval.eval_algorithms.prompt_stereotyping.PromptStereotyping.eval_name", "modulename": "src.fmeval.eval_algorithms.prompt_stereotyping", "qualname": "PromptStereotyping.eval_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.prompt_stereotyping.PromptStereotyping.evaluate", "modulename": "src.fmeval.eval_algorithms.prompt_stereotyping", "qualname": "PromptStereotyping.evaluate", "kind": "function", "doc": "<p>Evaluate the model on how stereotypical it's responses are.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model</strong>:  An instance of ModelRunner that represents the model being evaluated</li>\n<li><strong>dataset_config</strong>:  The config to load the dataset to use for evaluation. If not provided, model will be\nevaluated on all built-in datasets configured for this evaluation.</li>\n<li><strong>prompt_template</strong>:  A template which can be used to generate prompts, optional, if not provided defaults\nwill be used.</li>\n<li><strong>save</strong>:  If set to true, prompt responses and scores will be saved to file. The output is written to\nEvalAlgorithmInterface.EVAL_RESULTS_PATH</li>\n<li><strong>num_records</strong>:  The number of records to be sampled randomly from the input dataset to perform the\nevaluation</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>a list of EvalOutput objects. Current implementation returns only one score.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">model_runner</span><span class=\"o\">.</span><span class=\"n\">ModelRunner</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_config</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">data_config</span><span class=\"o\">.</span><span class=\"n\">DataConfig</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_template</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">num_records</span><span class=\"o\">=</span><span class=\"mi\">100</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalOutput</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.prompt_stereotyping.PromptStereotyping.evaluate_sample", "modulename": "src.fmeval.eval_algorithms.prompt_stereotyping", "qualname": "PromptStereotyping.evaluate_sample", "kind": "function", "doc": "<p>Evaluates stereotyping on a single sample. The evaluation computes the difference in likelihood that the model\nassigns to each of the sentences.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>sent_more_log_probability</strong>:  The log probability of the more stereotypical sentence in the model's\nlanguage model</li>\n<li><strong>sent_less_log_probability</strong>:  The log probability of the less stereotypical sentence in the model's\nlanguage model</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>the value of the stereotyping evaluation on this sample</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">sent_more_log_probability</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">sent_less_log_probability</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalScore</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy", "modulename": "src.fmeval.eval_algorithms.qa_accuracy", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy.ENGLISH_ARTICLES", "modulename": "src.fmeval.eval_algorithms.qa_accuracy", "qualname": "ENGLISH_ARTICLES", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;a&#x27;, &#x27;an&#x27;, &#x27;the&#x27;]"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy.ENGLISH_PUNCTUATIONS", "modulename": "src.fmeval.eval_algorithms.qa_accuracy", "qualname": "ENGLISH_PUNCTUATIONS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;!&quot;#$%&amp;\\&#x27;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy.F1_SCORE", "modulename": "src.fmeval.eval_algorithms.qa_accuracy", "qualname": "F1_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;f1_score&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy.EXACT_MATCH_SCORE", "modulename": "src.fmeval.eval_algorithms.qa_accuracy", "qualname": "EXACT_MATCH_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;exact_match_score&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy.QUASI_EXACT_MATCH_SCORE", "modulename": "src.fmeval.eval_algorithms.qa_accuracy", "qualname": "QUASI_EXACT_MATCH_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;quasi_exact_match_score&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy.PROMPT_COLUMN_NAME", "modulename": "src.fmeval.eval_algorithms.qa_accuracy", "qualname": "PROMPT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;prompt&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy.logger", "modulename": "src.fmeval.eval_algorithms.qa_accuracy", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.eval_algorithms.qa_accuracy (WARNING)&gt;"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy.QAAccuracyConfig", "modulename": "src.fmeval.eval_algorithms.qa_accuracy", "qualname": "QAAccuracyConfig", "kind": "class", "doc": "<p>Configuration for the QA Accuracy Evaluation</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>target_output_delimiter</strong>:  Target Output can have multiple answers. We expect customer to combine all the\npossible answers into a single string and use the delimiter to separate them. For instance,\nif the answers are [\"UK\", \"England\"] and the delimiter=\"<OR>\", then the target_output should be \"UK<OR>England\".</li>\n</ul>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmConfig"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy.QAAccuracyConfig.__init__", "modulename": "src.fmeval.eval_algorithms.qa_accuracy", "qualname": "QAAccuracyConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">target_output_delimiter</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;&lt;OR&gt;&#39;</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy.QAAccuracyConfig.target_output_delimiter", "modulename": "src.fmeval.eval_algorithms.qa_accuracy", "qualname": "QAAccuracyConfig.target_output_delimiter", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "&#x27;&lt;OR&gt;&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy.QA_ACCURACY_SCORES_TO_FUNCS", "modulename": "src.fmeval.eval_algorithms.qa_accuracy", "qualname": "QA_ACCURACY_SCORES_TO_FUNCS", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, Callable[..., float]]", "default_value": "{&#x27;f1_score&#x27;: functools.partial(&lt;function _f1_score&gt;, normalize_text=True), &#x27;exact_match_score&#x27;: &lt;function _exact_match_score&gt;, &#x27;quasi_exact_match_score&#x27;: &lt;function _quasi_exact_match_score&gt;}"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy.QAAccuracy", "modulename": "src.fmeval.eval_algorithms.qa_accuracy", "qualname": "QAAccuracy", "kind": "class", "doc": "<p>QA Accuracy Eval algorithm</p>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy.QAAccuracy.__init__", "modulename": "src.fmeval.eval_algorithms.qa_accuracy", "qualname": "QAAccuracy.__init__", "kind": "function", "doc": "<p>Default constructor</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>eval_algorithm_config</strong>:  QA Accuracy eval algorithm config.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">eval_algorithm_config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">qa_accuracy</span><span class=\"o\">.</span><span class=\"n\">QAAccuracyConfig</span> <span class=\"o\">=</span> <span class=\"n\">QAAccuracyConfig</span><span class=\"p\">(</span><span class=\"n\">target_output_delimiter</span><span class=\"o\">=</span><span class=\"s1\">&#39;&lt;OR&gt;&#39;</span><span class=\"p\">)</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy.QAAccuracy.eval_name", "modulename": "src.fmeval.eval_algorithms.qa_accuracy", "qualname": "QAAccuracy.eval_name", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;qa_accuracy&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy.QAAccuracy.evaluate", "modulename": "src.fmeval.eval_algorithms.qa_accuracy", "qualname": "QAAccuracy.evaluate", "kind": "function", "doc": "<p>QA Accuracy evaluate.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model</strong>:  An instance of ModelRunner which is the model under evaluation</li>\n<li><strong>dataset_config</strong>:  The config to load the dataset to use for evaluation. If not provided, model will be\nevaluated on all built-in datasets configured for this evaluation.</li>\n<li><strong>prompt_template</strong>:  A template which can be used to generate prompts, optional, if not provided defaults\nwill be used.</li>\n<li><strong>save</strong>:  If set to true, prompt responses and scores will be saved to file. The output is written to\nEvalAlgorithmInterface.EVAL_RESULTS_PATH</li>\n<li><strong>num_records</strong>:  The number of records to be sampled randomly from the input dataset to perform the\nevaluation\n:returns: List of EvalOutput objects. Current implementation returns only one score.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">model_runner</span><span class=\"o\">.</span><span class=\"n\">ModelRunner</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_config</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">data_config</span><span class=\"o\">.</span><span class=\"n\">DataConfig</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_template</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">num_records</span><span class=\"o\">=</span><span class=\"mi\">100</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalOutput</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy.QAAccuracy.evaluate_sample", "modulename": "src.fmeval.eval_algorithms.qa_accuracy", "qualname": "QAAccuracy.evaluate_sample", "kind": "function", "doc": "<p>Evaluate a single QA record.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>target_output</strong>:  The expected responses from the model.</li>\n<li><strong>model_output</strong>:  An instance of ModelOutput which contains the responses from the model needed for this\nevaluation.\n:returns: A List of EvalScores computed for prompts and responses.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">target_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalScore</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.PERTURBATION_TYPE_TO_HELPER_CLASS", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "PERTURBATION_TYPE_TO_HELPER_CLASS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;butter_finger&#x27;: &lt;class &#x27;fmeval.eval_algorithms.semantic_perturbation_utils.ButterFinger&#x27;&gt;, &#x27;random_upper_case&#x27;: &lt;class &#x27;fmeval.eval_algorithms.semantic_perturbation_utils.RandomUpperCase&#x27;&gt;, &#x27;whitespace_add_remove&#x27;: &lt;class &#x27;fmeval.eval_algorithms.semantic_perturbation_utils.WhitespaceAddRemove&#x27;&gt;}"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.DELTA_F1_SCORE", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "DELTA_F1_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;delta_f1_score&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.DELTA_EXACT_MATCH_SCORE", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "DELTA_EXACT_MATCH_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;delta_exact_match_score&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.DELTA_QUASI_EXACT_MATCH_SCORE", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "DELTA_QUASI_EXACT_MATCH_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;delta_quasi_exact_match_score&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.PROMPT_COLUMN_NAME", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "PROMPT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;prompt&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.logger", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness (WARNING)&gt;"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.QAAccuracySemanticRobustnessConfig", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "QAAccuracySemanticRobustnessConfig", "kind": "class", "doc": "<p>Configuration for the QA Accuracy Semantic Robustness Evaluation</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>target_output_delimiter</strong>:  Target Output can have multiple answers. We expect customer to combine all the\npossible answers into a single string and use the delimiter to separate them. For instance,\nif the answers are [\"UK\", \"England\"] and the delimiter=\"<OR>\", then the target_output should be \"UK<OR>England\".</li>\n<li><strong>perturbation_type</strong>:  perturbation type for generating perturbed inputs</li>\n<li><strong>num_perturbations</strong>:  Number of perturbed inputs to be generated for robustness evaluation</li>\n<li><strong>butter_finger_perturbation_prob</strong>:  The probability that a given character will be perturbed. Used for\nbutter_finger perturbation_type</li>\n<li><strong>random_uppercase_corrupt_proportion</strong>:  Fraction of characters to be changed to uppercase. Used for\nrandom_upper_case perturbation_type</li>\n<li><strong>whitespace_remove_prob</strong>:  Given a whitespace, remove it with this much probability. Used for\nwhitespace_add_remove perturbation_type</li>\n<li><strong>whitespace_add_prob</strong>:  Given a non-whitespace, add a whitespace before it with this probability. Used for\nwhitespace_add_remove perturbation_type</li>\n</ul>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmConfig"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.QAAccuracySemanticRobustnessConfig.__init__", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "QAAccuracySemanticRobustnessConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">target_output_delimiter</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;&lt;OR&gt;&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">perturbation_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;butter_finger&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">num_perturbations</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">butter_finger_perturbation_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">random_uppercase_corrupt_proportion</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">whitespace_remove_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">whitespace_add_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.05</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.QAAccuracySemanticRobustnessConfig.target_output_delimiter", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "QAAccuracySemanticRobustnessConfig.target_output_delimiter", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "&#x27;&lt;OR&gt;&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.QAAccuracySemanticRobustnessConfig.perturbation_type", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "QAAccuracySemanticRobustnessConfig.perturbation_type", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;butter_finger&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.QAAccuracySemanticRobustnessConfig.num_perturbations", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "QAAccuracySemanticRobustnessConfig.num_perturbations", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "5"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.QAAccuracySemanticRobustnessConfig.butter_finger_perturbation_prob", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "QAAccuracySemanticRobustnessConfig.butter_finger_perturbation_prob", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.1"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.QAAccuracySemanticRobustnessConfig.random_uppercase_corrupt_proportion", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "QAAccuracySemanticRobustnessConfig.random_uppercase_corrupt_proportion", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.1"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.QAAccuracySemanticRobustnessConfig.whitespace_remove_prob", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "QAAccuracySemanticRobustnessConfig.whitespace_remove_prob", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.1"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.QAAccuracySemanticRobustnessConfig.whitespace_add_prob", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "QAAccuracySemanticRobustnessConfig.whitespace_add_prob", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.05"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.QA_ACCURACY_SEMANTIC_ROBUSTNESS", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "QA_ACCURACY_SEMANTIC_ROBUSTNESS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;qa_accuracy_semantic_robustness&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.QAAccuracySemanticRobustness", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "QAAccuracySemanticRobustness", "kind": "class", "doc": "<p>QA Accuracy Semantic Robustness Eval algorithm</p>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.QAAccuracySemanticRobustness.__init__", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "QAAccuracySemanticRobustness.__init__", "kind": "function", "doc": "<p>Default constructor</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>eval_algorithm_config</strong>:  QA Accuracy Semantic Robustness eval algorithm config.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">eval_algorithm_config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">qa_accuracy_semantic_robustness</span><span class=\"o\">.</span><span class=\"n\">QAAccuracySemanticRobustnessConfig</span> <span class=\"o\">=</span> <span class=\"n\">QAAccuracySemanticRobustnessConfig</span><span class=\"p\">(</span><span class=\"n\">target_output_delimiter</span><span class=\"o\">=</span><span class=\"s1\">&#39;&lt;OR&gt;&#39;</span><span class=\"p\">,</span> <span class=\"n\">perturbation_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;butter_finger&#39;</span><span class=\"p\">,</span> <span class=\"n\">num_perturbations</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">butter_finger_perturbation_prob</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">random_uppercase_corrupt_proportion</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">whitespace_remove_prob</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">whitespace_add_prob</span><span class=\"o\">=</span><span class=\"mf\">0.05</span><span class=\"p\">)</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.QAAccuracySemanticRobustness.eval_name", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "QAAccuracySemanticRobustness.eval_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.QAAccuracySemanticRobustness.evaluate", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "QAAccuracySemanticRobustness.evaluate", "kind": "function", "doc": "<p>QA Accuracy Semantic Robustness evaluate.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model</strong>:  An instance of ModelRunner which is the model under evaluation</li>\n<li><strong>dataset_config</strong>:  Configures the single dataset used for evaluation. If not provided,\nevaluation will use all of it's supported built-in datasets</li>\n<li><strong>prompt_template</strong>:  A template which can be used to generate prompts, optional, if not provided defaults\nwill be used.</li>\n<li><strong>save</strong>:  If set to true, prompt responses and scores will be saved to file. The output is written to\nEvalAlgorithmInterface.EVAL_RESULTS_PATH</li>\n<li><strong>num_records</strong>:  The number of records to be sampled randomly from the input dataset to perform the\nevaluation\n:returns: A List of EvalOutput objects.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">model_runner</span><span class=\"o\">.</span><span class=\"n\">ModelRunner</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_config</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">data_config</span><span class=\"o\">.</span><span class=\"n\">DataConfig</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_template</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">num_records</span><span class=\"o\">=</span><span class=\"mi\">100</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalOutput</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness.QAAccuracySemanticRobustness.evaluate_sample", "modulename": "src.fmeval.eval_algorithms.qa_accuracy_semantic_robustness", "qualname": "QAAccuracySemanticRobustness.evaluate_sample", "kind": "function", "doc": "<p>Evaluate a single QA record for Semantic Robustness.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model_input</strong>:  text input for model</li>\n<li><strong>model</strong>:  An instance of ModelRunner which is the model under evaluation</li>\n<li><strong>target_output</strong>:  The expected responses from the model</li>\n<li><strong>model_output</strong>:  The output of a model that we want to evaluate.</li>\n<li><strong>prompt_template</strong>:  A template which can be used to compose prompt using model_input\n:returns: A List of EvalScores computed for prompts and responses.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model_input</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">model_runner</span><span class=\"o\">.</span><span class=\"n\">ModelRunner</span>,</span><span class=\"param\">\t<span class=\"n\">target_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_output</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_template</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;$feature&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalScore</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.qa_toxicity", "modulename": "src.fmeval.eval_algorithms.qa_toxicity", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.qa_toxicity.TOXIGEN_MODEL", "modulename": "src.fmeval.eval_algorithms.qa_toxicity", "qualname": "TOXIGEN_MODEL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;toxigen&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.qa_toxicity.DETOXIFY_MODEL", "modulename": "src.fmeval.eval_algorithms.qa_toxicity", "qualname": "DETOXIFY_MODEL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;detoxify&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.qa_toxicity.TOXICITY_HELPER_MODEL_MAPPING", "modulename": "src.fmeval.eval_algorithms.qa_toxicity", "qualname": "TOXICITY_HELPER_MODEL_MAPPING", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;toxigen&#x27;: &lt;class &#x27;fmeval.eval_algorithms.helper_models.helper_model.ToxigenHelperModel&#x27;&gt;, &#x27;detoxify&#x27;: &lt;class &#x27;fmeval.eval_algorithms.helper_models.helper_model.DetoxifyHelperModel&#x27;&gt;}"}, {"fullname": "src.fmeval.eval_algorithms.qa_toxicity.QA_TOXICITY", "modulename": "src.fmeval.eval_algorithms.qa_toxicity", "qualname": "QA_TOXICITY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;qa_toxicity&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.qa_toxicity.QAToxicity", "modulename": "src.fmeval.eval_algorithms.qa_toxicity", "qualname": "QAToxicity", "kind": "class", "doc": "<p>QA Toxicity eval algorithm</p>\n\n<p>Note: This separate eval algo implementation is for mapping QA Toxicity specific built-in datasets. For consuming\ntoxicity eval algo with your custom dataset please refer and use Toxicity eval algo</p>\n", "bases": "fmeval.eval_algorithms.toxicity.Toxicity"}, {"fullname": "src.fmeval.eval_algorithms.qa_toxicity.QAToxicity.__init__", "modulename": "src.fmeval.eval_algorithms.qa_toxicity", "qualname": "QAToxicity.__init__", "kind": "function", "doc": "<p>Default constructor</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>eval_algorithm_config</strong>:  Toxicity eval algorithm config.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">eval_algorithm_config</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">toxicity</span><span class=\"o\">.</span><span class=\"n\">ToxicityConfig</span> <span class=\"o\">=</span> <span class=\"n\">ToxicityConfig</span><span class=\"p\">(</span><span class=\"n\">model_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;detoxify&#39;</span><span class=\"p\">)</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.qa_toxicity.QAToxicity.eval_name", "modulename": "src.fmeval.eval_algorithms.qa_toxicity", "qualname": "QAToxicity.eval_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "kind": "module", "doc": "<p>This module contains several semantic perturbations from the NL-Augmenter package. The\ngoals of having this module are twofolds:</p>\n\n<ol>\n<li>NL-Augmenter has old dependencies which makes it difficult to install it in our env. We anyways\ndo not need all of NL-Augmenter, so we copy over the perturbations we need over here.</li>\n<li>We might add more perturbations from other packages like <code>nlaug</code>, or even have our own custom\nones, in the future, so we want to have a uniform API for these perturbations.</li>\n</ol>\n"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.ButterFingerConfig", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "ButterFingerConfig", "kind": "class", "doc": "<p>Config for the Butter Finger perturbation.\nDefaults set to match those in NL-Augmenter.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>perturbation_prob</strong>:  The probability that a given character will be perturbed.</li>\n</ul>\n"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.ButterFingerConfig.__init__", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "ButterFingerConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">perturbation_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.ButterFingerConfig.perturbation_prob", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "ButterFingerConfig.perturbation_prob", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.1"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.RandomUpperCaseConfig", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "RandomUpperCaseConfig", "kind": "class", "doc": "<p>Config for the RandomUpperCase perturbation.\nDefaults set to match those in NL-Augmenter.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>corrupt_proportion</strong>:  Fraction of characters to be changed to uppercase.</li>\n</ul>\n"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.RandomUpperCaseConfig.__init__", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "RandomUpperCaseConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">corrupt_proportion</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.RandomUpperCaseConfig.corrupt_proportion", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "RandomUpperCaseConfig.corrupt_proportion", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.1"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.WhitespaceAddRemoveConfig", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "WhitespaceAddRemoveConfig", "kind": "class", "doc": "<p>Config for WhitespaceAddRemove perturbation.\nDefaults set to match those in NL-Augmenter.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>remove_prob</strong>:  Given a whitespace, remove it with this much probability.</li>\n<li><strong>add_prob</strong>:  Given a non-whitespace, add a whitespace before it with this probability.</li>\n</ul>\n"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.WhitespaceAddRemoveConfig.__init__", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "WhitespaceAddRemoveConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">remove_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>, </span><span class=\"param\"><span class=\"n\">add_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.05</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.WhitespaceAddRemoveConfig.remove_prob", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "WhitespaceAddRemoveConfig.remove_prob", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.1"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.WhitespaceAddRemoveConfig.add_prob", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "WhitespaceAddRemoveConfig.add_prob", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.05"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.SemanticPerturbationUtil", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "SemanticPerturbationUtil", "kind": "class", "doc": "<p>The interface that each perturbation should implement.</p>\n", "bases": "abc.ABC"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.SemanticPerturbationUtil.perturb", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "SemanticPerturbationUtil.perturb", "kind": "function", "doc": "<p>Given an input text, generates one or more perturbed versions of it. Some perturbations can\nonly generate a single perturbed version, e.g., converting all numbers to numerics (eight -> 8).</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>text</strong>:  The input text that needs to be perturbed.</li>\n<li><strong>config</strong>:  The configuration containing parameters for the perturbation.</li>\n<li><strong>num_perturbations</strong>:  Number of perturbed versions to generate. Some perturbations can\nonly generate a single perturbed versions and will ignore this parameter.\n:returns: A list of perturbed texts.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">semantic_perturbation_utils</span><span class=\"o\">.</span><span class=\"n\">ButterFingerConfig</span><span class=\"p\">,</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">semantic_perturbation_utils</span><span class=\"o\">.</span><span class=\"n\">RandomUpperCaseConfig</span><span class=\"p\">,</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">semantic_perturbation_utils</span><span class=\"o\">.</span><span class=\"n\">WhitespaceAddRemoveConfig</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">num_perturbations</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.SemanticPerturbationUtil.set_seed", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "SemanticPerturbationUtil.set_seed", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">seed</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.ButterFinger", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "ButterFinger", "kind": "class", "doc": "<p>Given a text, add keyboard induced typos in randomly selected words.\nKeyboard induced typos are ones where a character is replaced by adjacent characters on the keyboard.</p>\n\n<p>Example:\n    Original: A quick brown fox jumps over the lazy dog 10 times.\n    Perturbed: W quick brmwn fox jumps over the lazy dig 10 times.</p>\n\n<p>Adopted from: <a href=\"https://github.com/GEM-benchmark/NL-Augmenter/blob/c591130760b453b3ad09516849dfc26e721eeb24/nlaugmenter/transformations/butter_fingers_perturbation/transformation.py\">https://github.com/GEM-benchmark/NL-Augmenter/blob/c591130760b453b3ad09516849dfc26e721eeb24/nlaugmenter/transformations/butter_fingers_perturbation/transformation.py</a></p>\n", "bases": "SemanticPerturbationUtil"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.ButterFinger.QUERTY_KEY_APPROX", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "ButterFinger.QUERTY_KEY_APPROX", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, str]", "default_value": "{&#x27;q&#x27;: &#x27;qwasedzx&#x27;, &#x27;w&#x27;: &#x27;wqesadrfcx&#x27;, &#x27;e&#x27;: &#x27;ewrsfdqazxcvgt&#x27;, &#x27;r&#x27;: &#x27;retdgfwsxcvgt&#x27;, &#x27;t&#x27;: &#x27;tryfhgedcvbnju&#x27;, &#x27;y&#x27;: &#x27;ytugjhrfvbnji&#x27;, &#x27;u&#x27;: &#x27;uyihkjtgbnmlo&#x27;, &#x27;i&#x27;: &#x27;iuojlkyhnmlp&#x27;, &#x27;o&#x27;: &#x27;oipklujm&#x27;, &#x27;p&#x27;: &quot;plo[&#x27;ik&quot;, &#x27;a&#x27;: &#x27;aqszwxwdce&#x27;, &#x27;s&#x27;: &#x27;swxadrfv&#x27;, &#x27;d&#x27;: &#x27;decsfaqgbv&#x27;, &#x27;f&#x27;: &#x27;fdgrvwsxyhn&#x27;, &#x27;g&#x27;: &#x27;gtbfhedcyjn&#x27;, &#x27;h&#x27;: &#x27;hyngjfrvkim&#x27;, &#x27;j&#x27;: &#x27;jhknugtblom&#x27;, &#x27;k&#x27;: &#x27;kjlinyhn&#x27;, &#x27;l&#x27;: &#x27;lokmpujn&#x27;, &#x27;z&#x27;: &#x27;zaxsvde&#x27;, &#x27;x&#x27;: &#x27;xzcsdbvfrewq&#x27;, &#x27;c&#x27;: &#x27;cxvdfzswergb&#x27;, &#x27;v&#x27;: &#x27;vcfbgxdertyn&#x27;, &#x27;b&#x27;: &#x27;bvnghcftyun&#x27;, &#x27;n&#x27;: &#x27;nbmhjvgtuik&#x27;, &#x27;m&#x27;: &#x27;mnkjloik&#x27;, &#x27; &#x27;: &#x27; &#x27;}"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.ButterFinger.perturb", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "ButterFinger.perturb", "kind": "function", "doc": "<p>Given an input text, generates one or more perturbed versions of it. Some perturbations can\nonly generate a single perturbed version, e.g., converting all numbers to numerics (eight -> 8).</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>text</strong>:  The input text that needs to be perturbed.</li>\n<li><strong>config</strong>:  The configuration containing parameters for the perturbation.</li>\n<li><strong>num_perturbations</strong>:  Number of perturbed versions to generate. Some perturbations can\nonly generate a single perturbed versions and will ignore this parameter.\n:returns: A list of perturbed texts.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">semantic_perturbation_utils</span><span class=\"o\">.</span><span class=\"n\">ButterFingerConfig</span>,</span><span class=\"param\">\t<span class=\"n\">num_perturbations</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.RandomUpperCase", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "RandomUpperCase", "kind": "class", "doc": "<p>Convert random characters in the text to uppercase.\nExample:\n    Original: A quick brown fox jumps over the lazy dog 10 times.\n    Perturbed: A qUick brOwn fox jumps over the lazY dog 10 timEs.</p>\n\n<p>Adopted from: <a href=\"https://github.com/GEM-benchmark/NL-Augmenter/blob/c591130760b453b3ad09516849dfc26e721eeb24/nlaugmenter/transformations/random_upper_transformation/transformation.py#L1\">https://github.com/GEM-benchmark/NL-Augmenter/blob/c591130760b453b3ad09516849dfc26e721eeb24/nlaugmenter/transformations/random_upper_transformation/transformation.py#L1</a></p>\n", "bases": "SemanticPerturbationUtil"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.RandomUpperCase.perturb", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "RandomUpperCase.perturb", "kind": "function", "doc": "<p>Given an input text, generates one or more perturbed versions of it. Some perturbations can\nonly generate a single perturbed version, e.g., converting all numbers to numerics (eight -> 8).</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>text</strong>:  The input text that needs to be perturbed.</li>\n<li><strong>config</strong>:  The configuration containing parameters for the perturbation.</li>\n<li><strong>num_perturbations</strong>:  Number of perturbed versions to generate. Some perturbations can\nonly generate a single perturbed versions and will ignore this parameter.\n:returns: A list of perturbed texts.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">semantic_perturbation_utils</span><span class=\"o\">.</span><span class=\"n\">RandomUpperCaseConfig</span>,</span><span class=\"param\">\t<span class=\"n\">num_perturbations</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.RandomUpperCase.random_upper", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "RandomUpperCase.random_upper", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">semantic_perturbation_utils</span><span class=\"o\">.</span><span class=\"n\">RandomUpperCaseConfig</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.WhitespaceAddRemove", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "WhitespaceAddRemove", "kind": "class", "doc": "<p>Add and remove whitespaces at random.\nExample:\n    Original: A quick brown fox jumps over the lazy dog 10 times.\n    Perturbed: A q uick bro wn fox ju mps overthe lazy dog 10 times.</p>\n\n<p>Adopted from: <a href=\"https://github.com/GEM-benchmark/NL-Augmenter/blob/c591130760b453b3ad09516849dfc26e721eeb24/nlaugmenter/transformations/whitespace_perturbation/transformation.py\">https://github.com/GEM-benchmark/NL-Augmenter/blob/c591130760b453b3ad09516849dfc26e721eeb24/nlaugmenter/transformations/whitespace_perturbation/transformation.py</a></p>\n", "bases": "SemanticPerturbationUtil"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.WhitespaceAddRemove.perturb", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "WhitespaceAddRemove.perturb", "kind": "function", "doc": "<p>Given an input text, generates one or more perturbed versions of it. Some perturbations can\nonly generate a single perturbed version, e.g., converting all numbers to numerics (eight -> 8).</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>text</strong>:  The input text that needs to be perturbed.</li>\n<li><strong>config</strong>:  The configuration containing parameters for the perturbation.</li>\n<li><strong>num_perturbations</strong>:  Number of perturbed versions to generate. Some perturbations can\nonly generate a single perturbed versions and will ignore this parameter.\n:returns: A list of perturbed texts.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">semantic_perturbation_utils</span><span class=\"o\">.</span><span class=\"n\">WhitespaceAddRemoveConfig</span>,</span><span class=\"param\">\t<span class=\"n\">num_perturbations</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.semantic_perturbation_utils.WhitespaceAddRemove.whitespace", "modulename": "src.fmeval.eval_algorithms.semantic_perturbation_utils", "qualname": "WhitespaceAddRemove.whitespace", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">char</span>, </span><span class=\"param\"><span class=\"n\">random_num</span>, </span><span class=\"param\"><span class=\"n\">remove_prob</span>, </span><span class=\"param\"><span class=\"n\">add_prob</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.PROMPT_COLUMN_NAME", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "PROMPT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;prompt&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.METEOR_SCORE", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "METEOR_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;meteor&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.ROUGE_SCORE", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "ROUGE_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;rouge&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.BERT_SCORE", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "BERT_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;bertscore&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.ROUGE_1", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "ROUGE_1", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;rouge1&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.ROUGE_2", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "ROUGE_2", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;rouge2&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.ROUGE_L", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "ROUGE_L", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;rougeL&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.ROUGE_TYPES", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "ROUGE_TYPES", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;rouge1&#x27;, &#x27;rouge2&#x27;, &#x27;rougeL&#x27;]"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.MICROSOFT_DEBERTA_MODEL", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "MICROSOFT_DEBERTA_MODEL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;microsoft/deberta-xlarge-mnli&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.ROBERTA_MODEL", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "ROBERTA_MODEL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;roberta-large-mnli&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.DEFAULT_MODEL_TYPE", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "DEFAULT_MODEL_TYPE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;microsoft/deberta-xlarge-mnli&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.MODEL_TYPES_SUPPORTED", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "MODEL_TYPES_SUPPORTED", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;microsoft/deberta-xlarge-mnli&#x27;, &#x27;roberta-large-mnli&#x27;]"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.logger", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.eval_algorithms.summarization_accuracy (WARNING)&gt;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "SummarizationAccuracyConfig", "kind": "class", "doc": "<p>Configuration for the summarization accuracy eval algorithm</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>rouge_type</strong>:  Type of rouge metric in eval results</li>\n<li><strong>use_stemmer_for_rouge</strong>:  bool value to set using stemmer for rouge metric</li>\n<li><strong>model_type_for_bertscore</strong>:  model to use for bert score</li>\n</ul>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmConfig"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig.__init__", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "SummarizationAccuracyConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">rouge_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;rouge2&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">use_stemmer_for_rouge</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">model_type_for_bertscore</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;microsoft/deberta-xlarge-mnli&#39;</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig.rouge_type", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "SummarizationAccuracyConfig.rouge_type", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;rouge2&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig.use_stemmer_for_rouge", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "SummarizationAccuracyConfig.use_stemmer_for_rouge", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracyConfig.model_type_for_bertscore", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "SummarizationAccuracyConfig.model_type_for_bertscore", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;microsoft/deberta-xlarge-mnli&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracy", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "SummarizationAccuracy", "kind": "class", "doc": "<p>Summarization Accuracy Eval algorithm</p>\n\n<p>The aim of this eval algo is to evaluate how well a model can summarize text.\nThe algo uses a reference summary to compare the output generated by the model and a series\nof quality metrics based on overlapping between words (ROUGE and METEOR) and similarity scores (BERTScore).</p>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracy.__init__", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "SummarizationAccuracy.__init__", "kind": "function", "doc": "<p>Default constructor</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>eval_algorithm_config</strong>:  Summarization Accuracy eval algorithm config.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">eval_algorithm_config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">summarization_accuracy</span><span class=\"o\">.</span><span class=\"n\">SummarizationAccuracyConfig</span> <span class=\"o\">=</span> <span class=\"n\">SummarizationAccuracyConfig</span><span class=\"p\">(</span><span class=\"n\">rouge_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;rouge2&#39;</span><span class=\"p\">,</span> <span class=\"n\">use_stemmer_for_rouge</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">model_type_for_bertscore</span><span class=\"o\">=</span><span class=\"s1\">&#39;microsoft/deberta-xlarge-mnli&#39;</span><span class=\"p\">)</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracy.eval_name", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "SummarizationAccuracy.eval_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracy.evaluate_sample", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "SummarizationAccuracy.evaluate_sample", "kind": "function", "doc": "<p>Summarization Accuracy evaluate sample.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>target_output</strong>:  The expected responses from the model</li>\n<li><strong>model_output</strong>:  The output of a model that we want to evaluate.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>list of EvalScore objects</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">target_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalScore</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.SummarizationAccuracy.evaluate", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "SummarizationAccuracy.evaluate", "kind": "function", "doc": "<p>Summarization Accuracy evaluate</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model</strong>:  An instance of ModelRunner which is the model under evaluation</li>\n<li><strong>dataset_config</strong>:  Configures the single dataset used for evaluation. If not provided,\nevaluation will use all of it's supported built-in datasets</li>\n<li><strong>prompt_template</strong>:  A template which can be used to generate prompts, optional, if not provided defaults\nwill be used.</li>\n<li><strong>save</strong>:  If set to true, prompt responses and scores will be saved to file. The output is written to\nEvalAlgorithmInterface.EVAL_RESULTS_PATH</li>\n<li><strong>num_records</strong>:  The number of records to be sampled randomly from the input dataset to perform the\nevaluation</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>List of EvalOutput objects.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">model_runner</span><span class=\"o\">.</span><span class=\"n\">ModelRunner</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_config</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">data_config</span><span class=\"o\">.</span><span class=\"n\">DataConfig</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_template</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">num_records</span><span class=\"o\">=</span><span class=\"mi\">100</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalOutput</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.get_meteor_score", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "get_meteor_score", "kind": "function", "doc": "<p>METEOR is a metric for text similarity between the machine-produced summary and human-produced reference summaries.\nUnigrams can be matched based on their surface forms, stemmed forms,\nand meanings; furthermore, METEOR can be easily extended to include more\nadvanced matching strategies. Once all generalized unigram matches\nbetween the two strings have been found, METEOR computes a score for\nthis matching using a combination of unigram-precision, unigram-recall, and\na measure of fragmentation that is designed to directly capture how\nwell-ordered the matched words in the machine translation are in relation\nto the reference.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>target_output</strong>:  The expected responses from the model</li>\n<li><strong>model_output</strong>:  The output of a model that we want to evaluate.</li>\n<li><strong>config</strong>:  Eval algo config\n:returns: meteor score</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">target_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">summarization_accuracy</span><span class=\"o\">.</span><span class=\"n\">SummarizationAccuracyConfig</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.get_rouge_score", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "get_rouge_score", "kind": "function", "doc": "<p>The ROUGE-N, where N=[1,2,L], score is a standard metric for summarization quality.\nIt computes the word overlap between the reference and model summary. Given that this metric is based on simple\nword overlap statistics, it works best for extractive summaries.\nNote that if we rephrase the summary without changing its meaning the ROUGE-N score will drop.</p>\n\n<p>Reference: <a href=\"https://huggingface.co/spaces/evaluate-metric/rouge\">https://huggingface.co/spaces/evaluate-metric/rouge</a></p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>target_output</strong>:  The expected responses from the model</li>\n<li><strong>model_output</strong>:  The output of a model that we want to evaluate.</li>\n<li><strong>config</strong>:  Eval algo config\n:returns: rouge score</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">target_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">summarization_accuracy</span><span class=\"o\">.</span><span class=\"n\">SummarizationAccuracyConfig</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.get_bert_score", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "get_bert_score", "kind": "function", "doc": "<p>BERTscore is a similarity-based metric that compares the embedding of the prediction and target sentences\nunder a learned model, typically, from the BERT family.\nThis score may lead to increased flexibility compared to ROUGE and METEOR in terms of rephrasing since\nsemantically similar sentences are (typically) embedded similarly.</p>\n\n<p><a href=\"https://huggingface.co/spaces/evaluate-metric/bertscore\">https://huggingface.co/spaces/evaluate-metric/bertscore</a></p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>target_output</strong>:  The expected responses from the model</li>\n<li><strong>model_output</strong>:  The output of a model that we want to evaluate.</li>\n<li><strong>config</strong>:  Eval algo config</li>\n<li><strong>helper_model</strong>:  The BertscoreHelperModel belonging to an instance of SummarizationAccuracy.\n:returns: bert score</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">target_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">summarization_accuracy</span><span class=\"o\">.</span><span class=\"n\">SummarizationAccuracyConfig</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy.add_score_to_dataset", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy", "qualname": "add_score_to_dataset", "kind": "function", "doc": "<p>Util method to add a score column to a ray dataset.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>dataset</strong>:  ray Dataset to be used for eval score generation</li>\n<li><strong>score_name_to_func</strong>:  maps column names for scores to be added to the functions used to compute those scores</li>\n<li><strong>config</strong>:  Eval algo config</li>\n<li><strong>helper_model</strong>:  The BertscoreHelperModel belonging to an\ninstance of SummarizationAccuracy. Used only by get_bert_score.\n:returns: ray Dataset with score column</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>,</span><span class=\"param\">\t<span class=\"n\">score_name_to_func</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Callable</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">summarization_accuracy</span><span class=\"o\">.</span><span class=\"n\">SummarizationAccuracyConfig</span>,</span><span class=\"param\">\t<span class=\"n\">helper_model</span><span class=\"p\">:</span> <span class=\"o\">&lt;</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">helper_models</span><span class=\"o\">.</span><span class=\"n\">helper_model</span><span class=\"o\">.</span><span class=\"n\">ActorClass</span><span class=\"p\">(</span><span class=\"n\">BertscoreHelperModel</span><span class=\"p\">)</span> <span class=\"nb\">object</span><span class=\"o\">&gt;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.logger", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness (WARNING)&gt;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.PERTURBATION_TYPE_TO_HELPER_CLASS", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "PERTURBATION_TYPE_TO_HELPER_CLASS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;butter_finger&#x27;: &lt;class &#x27;fmeval.eval_algorithms.semantic_perturbation_utils.ButterFinger&#x27;&gt;, &#x27;random_upper_case&#x27;: &lt;class &#x27;fmeval.eval_algorithms.semantic_perturbation_utils.RandomUpperCase&#x27;&gt;, &#x27;whitespace_add_remove&#x27;: &lt;class &#x27;fmeval.eval_algorithms.semantic_perturbation_utils.WhitespaceAddRemove&#x27;&gt;}"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.PROMPT_COLUMN_NAME", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "PROMPT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;prompt&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.DELTA_ROUGE_SCORE", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "DELTA_ROUGE_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;delta_rouge&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.DELTA_METEOR_SCORE", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "DELTA_METEOR_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;delta_meteor&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.DELTA_BERT_SCORE", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "DELTA_BERT_SCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;delta_bertscore&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracyActor", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "SummarizationAccuracyActor", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.ActorClass(SummarizationAccuracyActor) object&gt;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracySemanticRobustnessConfig", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "SummarizationAccuracySemanticRobustnessConfig", "kind": "class", "doc": "<p>Configuration for the summarization accuracy semantic robustness eval algorithm.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>perturbation_type</strong>:  perturbation type for generating perturbed inputs</li>\n<li><strong>num_perturbations</strong>:  Number of perturbed inputs to be generated for robustness evaluation</li>\n<li><strong>butter_finger_perturbation_prob</strong>:  The probability that a given character will be perturbed. Used for\nbutter_finger perturbation_type</li>\n<li><strong>random_uppercase_corrupt_proportion</strong>:  Fraction of characters to be changed to uppercase. Used for\nrandom_upper_case perturbation_type</li>\n<li><strong>whitespace_remove_prob</strong>:  Given a whitespace, remove it with this much probability. Used for\nwhitespace_add_remove perturbation_type</li>\n<li><strong>whitespace_add_prob</strong>:  Given a non-whitespace, add a whitespace before it with this probability. Used for\nwhitespace_add_remove perturbation_type</li>\n<li><strong>rouge_type</strong>:  Type of rouge metric in eval results</li>\n<li><strong>use_stemmer_for_rouge</strong>:  bool value to set using stemmer for rouge metric</li>\n<li><strong>model_type_for_bertscore</strong>:  model to use for bert score</li>\n</ul>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmConfig"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracySemanticRobustnessConfig.__init__", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "SummarizationAccuracySemanticRobustnessConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">perturbation_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;butter_finger&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">num_perturbations</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">butter_finger_perturbation_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">random_uppercase_corrupt_proportion</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">whitespace_remove_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">whitespace_add_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.05</span>,</span><span class=\"param\">\t<span class=\"n\">rouge_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;rouge2&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">use_stemmer_for_rouge</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">model_type_for_bertscore</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;microsoft/deberta-xlarge-mnli&#39;</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracySemanticRobustnessConfig.perturbation_type", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "SummarizationAccuracySemanticRobustnessConfig.perturbation_type", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;butter_finger&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracySemanticRobustnessConfig.num_perturbations", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "SummarizationAccuracySemanticRobustnessConfig.num_perturbations", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "5"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracySemanticRobustnessConfig.butter_finger_perturbation_prob", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "SummarizationAccuracySemanticRobustnessConfig.butter_finger_perturbation_prob", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.1"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracySemanticRobustnessConfig.random_uppercase_corrupt_proportion", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "SummarizationAccuracySemanticRobustnessConfig.random_uppercase_corrupt_proportion", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.1"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracySemanticRobustnessConfig.whitespace_remove_prob", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "SummarizationAccuracySemanticRobustnessConfig.whitespace_remove_prob", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.1"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracySemanticRobustnessConfig.whitespace_add_prob", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "SummarizationAccuracySemanticRobustnessConfig.whitespace_add_prob", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.05"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracySemanticRobustnessConfig.rouge_type", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "SummarizationAccuracySemanticRobustnessConfig.rouge_type", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;rouge2&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracySemanticRobustnessConfig.use_stemmer_for_rouge", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "SummarizationAccuracySemanticRobustnessConfig.use_stemmer_for_rouge", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracySemanticRobustnessConfig.model_type_for_bertscore", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "SummarizationAccuracySemanticRobustnessConfig.model_type_for_bertscore", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;microsoft/deberta-xlarge-mnli&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracySemanticRobustness", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "SummarizationAccuracySemanticRobustness", "kind": "class", "doc": "<p>Semantic Robustness Eval algorithm for Summarization Accuracy task LLMs</p>\n\n<p>This evaluation measures how much the model output changes as a result of semantic preserving\nperturbations. Given the input, e.g., \"A quick brown fox jumps over the lazy dog\", the\nevaluation creates a perturbation that preserves the semantic meaning of the input e.g.,\nwhitespace perturbation that changes the input text to \"A q uick bro wn fox ju mps overthe lazy\ndog\". The evaluation then measures how much the model output changes when prompted with the\noriginal vs. perturbed input. The algo compares summarization accuracy of model output for original model output\nand model output for perturbed inputs, returns delta between rouge, meteor and bert\nscores.</p>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracySemanticRobustness.__init__", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "SummarizationAccuracySemanticRobustness.__init__", "kind": "function", "doc": "<p>Default constructor</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>eval_algorithm_config</strong>:  Summarization Accuracy Semantic Robustness eval algorithm config.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">eval_algorithm_config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">summarization_accuracy_semantic_robustness</span><span class=\"o\">.</span><span class=\"n\">SummarizationAccuracySemanticRobustnessConfig</span> <span class=\"o\">=</span> <span class=\"n\">SummarizationAccuracySemanticRobustnessConfig</span><span class=\"p\">(</span><span class=\"n\">perturbation_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;butter_finger&#39;</span><span class=\"p\">,</span> <span class=\"n\">num_perturbations</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">butter_finger_perturbation_prob</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">random_uppercase_corrupt_proportion</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">whitespace_remove_prob</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">whitespace_add_prob</span><span class=\"o\">=</span><span class=\"mf\">0.05</span><span class=\"p\">,</span> <span class=\"n\">rouge_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;rouge2&#39;</span><span class=\"p\">,</span> <span class=\"n\">use_stemmer_for_rouge</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">model_type_for_bertscore</span><span class=\"o\">=</span><span class=\"s1\">&#39;microsoft/deberta-xlarge-mnli&#39;</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">summ_acc_actor</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">summarization_accuracy_semantic_robustness</span><span class=\"o\">.</span><span class=\"n\">ActorClass</span><span class=\"p\">(</span><span class=\"n\">SummarizationAccuracyActor</span><span class=\"p\">)</span> <span class=\"nb\">object</span> <span class=\"n\">at</span> <span class=\"mh\">0x289a43310</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracySemanticRobustness.eval_name", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "SummarizationAccuracySemanticRobustness.eval_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracySemanticRobustness.evaluate_sample", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "SummarizationAccuracySemanticRobustness.evaluate_sample", "kind": "function", "doc": "<p>Summarization Accuracy Semantic Robustness evaluate sample.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model_input</strong>:  text input for model</li>\n<li><strong>target_output</strong>:  The expected responses from the model</li>\n<li><strong>model</strong>:  An instance of ModelRunner which is the model under evaluation</li>\n<li><strong>model_output</strong>:  The output of a model that we want to evaluate.</li>\n<li><strong>prompt_template</strong>:  A template which can be used to compose prompt using model_input</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>list of EvalScore object</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model_input</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">target_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">model_runner</span><span class=\"o\">.</span><span class=\"n\">ModelRunner</span>,</span><span class=\"param\">\t<span class=\"n\">model_output</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_template</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;$feature&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalScore</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness.SummarizationAccuracySemanticRobustness.evaluate", "modulename": "src.fmeval.eval_algorithms.summarization_accuracy_semantic_robustness", "qualname": "SummarizationAccuracySemanticRobustness.evaluate", "kind": "function", "doc": "<p>Semantic Robustness evaluate.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model</strong>:  An instance of ModelRunner which is the model under evaluation</li>\n<li><strong>dataset_config</strong>:  Configures the single dataset used for evaluation. If not provided,\nevaluation will use all of it's supported built-in datasets</li>\n<li><strong>prompt_template</strong>:  A template which can be used to generate prompts, optional, if not provided defaults\nwill be used.</li>\n<li><strong>save</strong>:  If set to true, prompt responses and scores will be saved to file. The output is written to\nEvalAlgorithmInterface.EVAL_RESULTS_PATH</li>\n<li><strong>num_records</strong>:  The number of records to be sampled randomly from the input dataset to perform the\nevaluation</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>List of EvalOutput objects.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">model_runner</span><span class=\"o\">.</span><span class=\"n\">ModelRunner</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_config</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">data_config</span><span class=\"o\">.</span><span class=\"n\">DataConfig</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_template</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">num_records</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">100</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalOutput</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.summarization_toxicity", "modulename": "src.fmeval.eval_algorithms.summarization_toxicity", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.summarization_toxicity.TOXIGEN_MODEL", "modulename": "src.fmeval.eval_algorithms.summarization_toxicity", "qualname": "TOXIGEN_MODEL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;toxigen&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_toxicity.DETOXIFY_MODEL", "modulename": "src.fmeval.eval_algorithms.summarization_toxicity", "qualname": "DETOXIFY_MODEL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;detoxify&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_toxicity.TOXICITY_HELPER_MODEL_MAPPING", "modulename": "src.fmeval.eval_algorithms.summarization_toxicity", "qualname": "TOXICITY_HELPER_MODEL_MAPPING", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;toxigen&#x27;: &lt;class &#x27;fmeval.eval_algorithms.helper_models.helper_model.ToxigenHelperModel&#x27;&gt;, &#x27;detoxify&#x27;: &lt;class &#x27;fmeval.eval_algorithms.helper_models.helper_model.DetoxifyHelperModel&#x27;&gt;}"}, {"fullname": "src.fmeval.eval_algorithms.summarization_toxicity.SUMMARIZATION_TOXICITY", "modulename": "src.fmeval.eval_algorithms.summarization_toxicity", "qualname": "SUMMARIZATION_TOXICITY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;summarization_toxicity&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.summarization_toxicity.SummarizationToxicity", "modulename": "src.fmeval.eval_algorithms.summarization_toxicity", "qualname": "SummarizationToxicity", "kind": "class", "doc": "<p>Summarization Toxicity eval algorithm</p>\n\n<p>Note: This separate eval algo implementation is for mapping Summarization Toxicity specific built-in datasets.\nFor consuming toxicity eval algo with your custom dataset please refer and use Toxicity eval algo</p>\n", "bases": "fmeval.eval_algorithms.toxicity.Toxicity"}, {"fullname": "src.fmeval.eval_algorithms.summarization_toxicity.SummarizationToxicity.__init__", "modulename": "src.fmeval.eval_algorithms.summarization_toxicity", "qualname": "SummarizationToxicity.__init__", "kind": "function", "doc": "<p>Default constructor</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>eval_algorithm_config</strong>:  Toxicity eval algorithm config.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">eval_algorithm_config</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">toxicity</span><span class=\"o\">.</span><span class=\"n\">ToxicityConfig</span> <span class=\"o\">=</span> <span class=\"n\">ToxicityConfig</span><span class=\"p\">(</span><span class=\"n\">model_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;detoxify&#39;</span><span class=\"p\">)</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.summarization_toxicity.SummarizationToxicity.eval_name", "modulename": "src.fmeval.eval_algorithms.summarization_toxicity", "qualname": "SummarizationToxicity.eval_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.toxicity", "modulename": "src.fmeval.eval_algorithms.toxicity", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.toxicity.TOXIGEN_MODEL", "modulename": "src.fmeval.eval_algorithms.toxicity", "qualname": "TOXIGEN_MODEL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;toxigen&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.toxicity.DETOXIFY_MODEL", "modulename": "src.fmeval.eval_algorithms.toxicity", "qualname": "DETOXIFY_MODEL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;detoxify&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.toxicity.DEFAULT_MODEL_TYPE", "modulename": "src.fmeval.eval_algorithms.toxicity", "qualname": "DEFAULT_MODEL_TYPE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;detoxify&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.toxicity.MODEL_TYPES_SUPPORTED", "modulename": "src.fmeval.eval_algorithms.toxicity", "qualname": "MODEL_TYPES_SUPPORTED", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;toxigen&#x27;, &#x27;detoxify&#x27;]"}, {"fullname": "src.fmeval.eval_algorithms.toxicity.TOXICITY_HELPER_MODEL_MAPPING", "modulename": "src.fmeval.eval_algorithms.toxicity", "qualname": "TOXICITY_HELPER_MODEL_MAPPING", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;toxigen&#x27;: &lt;class &#x27;fmeval.eval_algorithms.helper_models.helper_model.ToxigenHelperModel&#x27;&gt;, &#x27;detoxify&#x27;: &lt;class &#x27;fmeval.eval_algorithms.helper_models.helper_model.DetoxifyHelperModel&#x27;&gt;}"}, {"fullname": "src.fmeval.eval_algorithms.toxicity.PROMPT_COLUMN_NAME", "modulename": "src.fmeval.eval_algorithms.toxicity", "qualname": "PROMPT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;prompt&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.toxicity.logger", "modulename": "src.fmeval.eval_algorithms.toxicity", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.eval_algorithms.toxicity (WARNING)&gt;"}, {"fullname": "src.fmeval.eval_algorithms.toxicity.ToxicityConfig", "modulename": "src.fmeval.eval_algorithms.toxicity", "qualname": "ToxicityConfig", "kind": "class", "doc": "<p>Configuration for the toxicity eval algorithm</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model_type</strong>:  model to use for toxicity eval</li>\n</ul>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmConfig"}, {"fullname": "src.fmeval.eval_algorithms.toxicity.ToxicityConfig.__init__", "modulename": "src.fmeval.eval_algorithms.toxicity", "qualname": "ToxicityConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;detoxify&#39;</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.toxicity.ToxicityConfig.model_type", "modulename": "src.fmeval.eval_algorithms.toxicity", "qualname": "ToxicityConfig.model_type", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;detoxify&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.toxicity.TOXICITY", "modulename": "src.fmeval.eval_algorithms.toxicity", "qualname": "TOXICITY", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;toxicity&#x27;"}, {"fullname": "src.fmeval.eval_algorithms.toxicity.Toxicity", "modulename": "src.fmeval.eval_algorithms.toxicity", "qualname": "Toxicity", "kind": "class", "doc": "<p>Toxicity eval algorithm</p>\n", "bases": "fmeval.eval_algorithms.eval_algorithm.EvalAlgorithmInterface"}, {"fullname": "src.fmeval.eval_algorithms.toxicity.Toxicity.__init__", "modulename": "src.fmeval.eval_algorithms.toxicity", "qualname": "Toxicity.__init__", "kind": "function", "doc": "<p>Default constructor</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>eval_algorithm_config</strong>:  Toxicity eval algorithm config.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">eval_algorithm_config</span><span class=\"p\">:</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">toxicity</span><span class=\"o\">.</span><span class=\"n\">ToxicityConfig</span> <span class=\"o\">=</span> <span class=\"n\">ToxicityConfig</span><span class=\"p\">(</span><span class=\"n\">model_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;detoxify&#39;</span><span class=\"p\">)</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.toxicity.Toxicity.eval_name", "modulename": "src.fmeval.eval_algorithms.toxicity", "qualname": "Toxicity.eval_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.toxicity.Toxicity.evaluate_sample", "modulename": "src.fmeval.eval_algorithms.toxicity", "qualname": "Toxicity.evaluate_sample", "kind": "function", "doc": "<p>Toxicity evaluate sample</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model_output</strong>:  The output of a model that we want to evaluate.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>list of EvalScore objects</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model_output</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalScore</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.toxicity.Toxicity.evaluate", "modulename": "src.fmeval.eval_algorithms.toxicity", "qualname": "Toxicity.evaluate", "kind": "function", "doc": "<p>Toxicity evaluate</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model</strong>:  An instance of ModelRunner which is the model under evaluation</li>\n<li><strong>dataset_config</strong>:  The config to load the dataset to use for evaluation. If not provided, model will be\nevaluated on all built-in datasets configured for this evaluation.</li>\n<li><strong>prompt_template</strong>:  A template which can be used to generate prompts, optional, if not provided defaults\nwill be used.</li>\n<li><strong>save</strong>:  If set to true, prompt responses and scores will be saved to file. The output is written to\nEvalAlgorithmInterface.EVAL_RESULTS_PATH</li>\n<li><strong>num_records</strong>:  The number of records to be sampled randomly from the input dataset to perform the\nevaluation</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>List of EvalOutput objects.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">model_runner</span><span class=\"o\">.</span><span class=\"n\">ModelRunner</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_config</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">data_loaders</span><span class=\"o\">.</span><span class=\"n\">data_config</span><span class=\"o\">.</span><span class=\"n\">DataConfig</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_template</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">num_records</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">100</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalOutput</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.util", "modulename": "src.fmeval.eval_algorithms.util", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.eval_algorithms.util.logger", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.eval_algorithms.util (WARNING)&gt;"}, {"fullname": "src.fmeval.eval_algorithms.util.generate_model_predict_response_for_dataset", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "generate_model_predict_response_for_dataset", "kind": "function", "doc": "<p>Runs the model on the given data. Output will be written to the\n<code>model_output_column_name</code> column, and log_probability will be\nwritten to the <code>model_log_probability_column_name</code> column.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model</strong>:  ModelRunner to get predictions from.</li>\n<li><strong>data</strong>:  The dataset containing model inputs to feed to <code>model</code>.</li>\n<li><strong>model_input_column_name</strong>:  The name of the column containing the model input.</li>\n<li><strong>model_output_column_name</strong>:  The name of the column to write the model output to.</li>\n<li><strong>model_log_probability_column_name</strong>:  The name of the column to write the model log probability to.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>The dataset with a model output column and model log probability column added.\n      Note that both columns are optional, i.e. it is possible that a model output\n      column is added, but a log probability column is not added (and vice versa).</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">model_runner</span><span class=\"o\">.</span><span class=\"n\">ModelRunner</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>,</span><span class=\"param\">\t<span class=\"n\">model_input_column_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_output_column_name</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_log_probability_column_name</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.util.generate_prompt_column_for_dataset", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "generate_prompt_column_for_dataset", "kind": "function", "doc": "<p>Generates prompts column for a given input dataset and prompt_template</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>prompt_template</strong>:  Prompt template</li>\n<li><strong>data</strong>:  the dataset where each instance is a row in the dataset.</li>\n<li><strong>model_input_column_name</strong>:  the name of the column containing the model input.</li>\n<li><strong>prompt_column_name</strong>:  Output column name to which composed prompts are added</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>the dataset with the composed prompts added.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">prompt_template</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>,</span><span class=\"param\">\t<span class=\"n\">model_input_column_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_column_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.util.validate_dataset", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "validate_dataset", "kind": "function", "doc": "<p>Util function to validate that dataset contains the required column names.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>dataset</strong>:  Input ray dataset</li>\n<li><strong>column_names</strong>:  names of the columns that must be present in the dataset</li>\n</ul>\n\n<h6 id=\"raises\">Raises</h6>\n\n<ul>\n<li>EvalAlgorithmClientError for an invalid dataset</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>, </span><span class=\"param\"><span class=\"n\">column_names</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.util.aggregate_evaluation_scores", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "aggregate_evaluation_scores", "kind": "function", "doc": "<p>The method aggregates scores at the dataset level and optionally at the category level if\n categories are available in the dataset.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>dataset</strong>:  ray dataset with eval scores</li>\n<li><strong>score_column_names</strong>:  a list of column names which contain the scores to aggregate</li>\n<li><strong>agg_method</strong>:  the name of the aggregation to perform</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>a tuple containing 1) dataset-level scores and\n                              2) a list of category-level scores if categories are available, <code>None</code> otherwise</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>,</span><span class=\"param\">\t<span class=\"n\">score_column_names</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">agg_method</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalScore</span><span class=\"p\">],</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">CategoryScore</span><span class=\"p\">]]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.util.dataset_aggregation", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "dataset_aggregation", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>,</span><span class=\"param\">\t<span class=\"n\">score_column_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">agg_method</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.util.category_wise_aggregation", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "category_wise_aggregation", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>,</span><span class=\"param\">\t<span class=\"n\">score_column_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">agg_method</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord", "kind": "class", "doc": "<p>The schema used to define the records that get written\nto a JSON Lines file when <code>save_dataset</code> is called.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model_input</strong>:  the model input</li>\n<li><strong>model_output</strong>:  the model output</li>\n<li><strong>model_log_probability</strong>:  the model log probability</li>\n<li><strong>target_output</strong>:  the target output</li>\n<li><strong>category</strong>:  the category</li>\n<li><strong>sent_more_input</strong>:  the \"sent more\" input (used by Prompt stereotyping)</li>\n<li><strong>sent_less_input</strong>:  the \"sent less\" input (used by Prompt stereotyping)</li>\n<li><strong>sent_more_input_prob</strong>:  the \"sent more\" input probability (used by Prompt stereotyping)</li>\n<li><strong>sent_less_input_prob</strong>:  the \"sent less\" input probability (used by Prompt stereotyping)</li>\n<li><strong>sent_more_output</strong>:  the \"sent more\" output (used by Prompt stereotyping)</li>\n<li><strong>sent_less_output</strong>:  the \"sent less\" output (used by Prompt stereotyping)</li>\n</ul>\n\n<p>IMPORTANT:\n    The attributes of this class MUST match the values of the\n    column name constants in COLUMN_NAMES in src/constants.py.</p>\n\n<pre><code>Reason:\nThe `from_row` method validates the column names included\nin its `row` input, making sure that these column names\nmatch the attribute names of this class (this validation\nonly occurs for column names that don't correspond to score\nnames).\n\nSince the `row` input comes from a Ray Dataset produced by\nthe `evaluate` method of an `EvalAlgorithmInterface`, the column\nnames in the row must come from COLUMN_NAMES in src/constants.py.\n\nThus, the attribute names of this class must match the constants\nin COLUMN_NAMES in order for the validation to make sense.\n</code></pre>\n"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord.__init__", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">scores</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalScore</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_input</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_output</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_log_probability</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_output</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">category</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sent_more_input</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sent_less_input</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sent_more_input_prob</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sent_less_input_prob</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sent_more_output</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sent_less_output</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sent_more_prompt</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sent_less_prompt</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord.scores", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord.scores", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List[fmeval.eval_algorithms.EvalScore]"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord.model_input", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord.model_input", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord.model_output", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord.model_output", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord.model_log_probability", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord.model_log_probability", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[float]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord.target_output", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord.target_output", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord.category", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord.category", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord.sent_more_input", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord.sent_more_input", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord.sent_less_input", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord.sent_less_input", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord.sent_more_input_prob", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord.sent_more_input_prob", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord.sent_less_input_prob", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord.sent_less_input_prob", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord.sent_more_output", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord.sent_more_output", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord.sent_less_output", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord.sent_less_output", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord.prompt", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord.prompt", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord.sent_more_prompt", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord.sent_more_prompt", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord.sent_less_prompt", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord.sent_less_prompt", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "None"}, {"fullname": "src.fmeval.eval_algorithms.util.EvalOutputRecord.from_row", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "EvalOutputRecord.from_row", "kind": "function", "doc": "<p>Returns an instance of EvalOutputRecord, created from a Ray Dataset row (represented as a dict).</p>\n\n<p>Example input:\n    row = {\n        \"model_input\": \"input\",\n        \"model_output\": \"output\",\n        \"rouge\": 0.42,\n        \"bert\": 0.162\n    }</p>\n\n<p>Corresponding output:\n    EvalOutputRecord(\n        model_input=\"input\",\n        model_output=\"output\",\n        scores=[\n            EvalScore(name=\"rouge\", value=0.42),\n            EvalScore(name=\"bert\", value=0.162)\n        ]\n    )</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>row</strong>:  a Ray Dataset row represented as a dict</li>\n<li><strong>score_names</strong>:  column names included in the Ray Dataset that <code>row</code>\nis a sample of that correspond to evaluation algorithm scores\n:returns: an instance of EvalOutputRecord corresponding to <code>row</code></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">row</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">score_names</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">util</span><span class=\"o\">.</span><span class=\"n\">EvalOutputRecord</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.util.save_dataset", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "save_dataset", "kind": "function", "doc": "<p>Writes the dataset to a JSON Lines file, where each JSON Lines object\nfollows the schema defined by <code>EvalOutputRecord</code>.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>dataset</strong>:  a Ray Dataset that is produced during the execution of\nan EvalAlgorithmInterface's <code>evaluate</code> method. This dataset is expected\nto include columns for every score computed by the evaluation algorithm.</li>\n<li><strong>score_names</strong>:  the names of the score columns in <code>dataset</code></li>\n<li><p><strong>path</strong>:  a local file path to write the dataset to. The file name specified\nby this argument may not end in the extension <code>.jsonl</code>. In this case,\nwe append the extension ourselves.</p>\n\n<p>Example Dataset:</p>\n\n<hr />\n\n<h2 id=\"model_input-rouge-bert_score\">| \"model_input\" | \"rouge\" | \"bert_score\"|</h2>\n\n<h2 id=\"hello-05-042\">|   \"hello\"    |   0.5   |     0.42    |</h2>\n\n<h2 id=\"world-0314-0271\">|   \"world\"   |  0.314  |    0.271    |</h2>\n\n<p>Corresponding Json Lines file contents:\n{\"model_input\" : \"hello\", \"scores\" : [{\"name\": \"rouge\", \"value\": 0.5}, {\"name\": \"bert_score\", \"value\": 0.42}]}\n{\"model_input\" : \"world\", \"scores\" : [{\"name\": \"rouge\", \"value\": 0.314}, {\"name\": \"bert_score\", \"value\": 0.271}]}</p></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>,</span><span class=\"param\">\t<span class=\"n\">score_names</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.util.generate_output_dataset_path", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "generate_output_dataset_path", "kind": "function", "doc": "<p>Returns the path to be used by an EvalAlgorithmInterface when calling <code>save_dataset</code>.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>path_to_parent_dir</strong>:  The path to the parent directory of the file to be saved.</li>\n<li><strong>eval_name</strong>:  The evaluation name provided by the EvalAlgorithmInterface.</li>\n<li><strong>dataset_name</strong>:  The name of the dataset.\n:returns: A path that is unique to an evaluation/dataset pair for a given job.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path_to_parent_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">eval_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">dataset_name</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.util.generate_mean_delta_score", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "generate_mean_delta_score", "kind": "function", "doc": "<p>Util method to generate mean of difference between original and perturbed input scores</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>original_score</strong>:  Original score</li>\n<li><strong>perturbed_input_scores</strong>:  List of scores for model inference outputs on perturbed inputs\n:returns: mean of delta between the scores</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">original_score</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalScore</span>,</span><span class=\"param\">\t<span class=\"n\">perturbed_input_scores</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalScore</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.eval_algorithms.util.verify_model_determinism", "modulename": "src.fmeval.eval_algorithms.util", "qualname": "verify_model_determinism", "kind": "function", "doc": "<p>Check model is not deterministic for first NUM_ROWS_DETERMINISTIC rows</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model</strong>:  An instance of ModelRunner which is the model under evaluation</li>\n<li><strong>dataset</strong>:  a Ray Dataset that expected to include columns for prompts</li>\n<li><strong>prompt_column_name</strong>:  Prompt column name\n:return True if model is deterministic, False otherwise</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">model_runner</span><span class=\"o\">.</span><span class=\"n\">ModelRunner</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_column_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.exceptions", "modulename": "src.fmeval.exceptions", "kind": "module", "doc": "<p>Error classes for exceptions</p>\n"}, {"fullname": "src.fmeval.exceptions.EvalAlgorithmClientError", "modulename": "src.fmeval.exceptions", "qualname": "EvalAlgorithmClientError", "kind": "class", "doc": "<p>Client Error when using Eval Algorithm</p>\n", "bases": "builtins.ValueError"}, {"fullname": "src.fmeval.exceptions.EvalAlgorithmInternalError", "modulename": "src.fmeval.exceptions", "qualname": "EvalAlgorithmInternalError", "kind": "class", "doc": "<p>Algorithm error when using Eval Algorithm</p>\n", "bases": "builtins.Exception"}, {"fullname": "src.fmeval.exceptions.DuplicateEvalNameError", "modulename": "src.fmeval.exceptions", "qualname": "DuplicateEvalNameError", "kind": "class", "doc": "<p>Evaluation name already exists.</p>\n", "bases": "EvalAlgorithmClientError"}, {"fullname": "src.fmeval.model_runners", "modulename": "src.fmeval.model_runners", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.model_runners.bedrock_model_runner", "modulename": "src.fmeval.model_runners.bedrock_model_runner", "kind": "module", "doc": "<p>Module to manage model runners for Bedrock models.</p>\n"}, {"fullname": "src.fmeval.model_runners.bedrock_model_runner.logger", "modulename": "src.fmeval.model_runners.bedrock_model_runner", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.model_runners.bedrock_model_runner (WARNING)&gt;"}, {"fullname": "src.fmeval.model_runners.bedrock_model_runner.BedrockModelRunner", "modulename": "src.fmeval.model_runners.bedrock_model_runner", "qualname": "BedrockModelRunner", "kind": "class", "doc": "<p>A class to manage the creation and deletion of Bedrock model runner when user provides\na Bedrock model id.</p>\n", "bases": "fmeval.model_runners.model_runner.ModelRunner"}, {"fullname": "src.fmeval.model_runners.bedrock_model_runner.BedrockModelRunner.__init__", "modulename": "src.fmeval.model_runners.bedrock_model_runner", "qualname": "BedrockModelRunner.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model_id</strong>:  Id of the Bedrock model to be used for model predictions</li>\n<li><strong>content_template</strong>:  String template to compose the model input from the prompt</li>\n<li><strong>output</strong>:  JMESPath expression of output in the model output</li>\n<li><strong>log_probability</strong>:  JMESPath expression of log probability in the model output</li>\n<li><strong>content_type</strong>:  The content type of the request sent to the model for inference</li>\n<li><strong>accept_type</strong>:  The accept type of the request sent to the model for inference</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">content_template</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">output</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">log_probability</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">content_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;application/json&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">accept_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;application/json&#39;</span></span>)</span>"}, {"fullname": "src.fmeval.model_runners.bedrock_model_runner.BedrockModelRunner.predict", "modulename": "src.fmeval.model_runners.bedrock_model_runner", "qualname": "BedrockModelRunner.predict", "kind": "function", "doc": "<p>Invoke the Bedrock model and parse the model response.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>prompt</strong>:  Input data for which you want the model to provide inference.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">prompt</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.composers", "modulename": "src.fmeval.model_runners.composers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.model_runners.composers.logger", "modulename": "src.fmeval.model_runners.composers", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.model_runners.composers (WARNING)&gt;"}, {"fullname": "src.fmeval.model_runners.composers.create_content_composer", "modulename": "src.fmeval.model_runners.composers", "qualname": "create_content_composer", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">template</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">content_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;application/json&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">model_runners</span><span class=\"o\">.</span><span class=\"n\">composers</span><span class=\"o\">.</span><span class=\"n\">composers</span><span class=\"o\">.</span><span class=\"n\">Composer</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.composers.composers", "modulename": "src.fmeval.model_runners.composers.composers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.model_runners.composers.composers.Composer", "modulename": "src.fmeval.model_runners.composers.composers", "qualname": "Composer", "kind": "class", "doc": "<p>Helper class that provides a standard way to create an ABC using\ninheritance.</p>\n", "bases": "abc.ABC"}, {"fullname": "src.fmeval.model_runners.composers.composers.Composer.__init__", "modulename": "src.fmeval.model_runners.composers.composers", "qualname": "Composer.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>template: A template string. Ex: '{\"data\"</strong>: $prompt}'</li>\n<li><strong>placeholder</strong>:  A placeholder keyword. This keyword appears\nin <code>template</code> with a $ sign prepended. In the above example,\nthe placeholder is \"prompt\".</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">template</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">placeholder</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "src.fmeval.model_runners.composers.composers.Composer.placeholder", "modulename": "src.fmeval.model_runners.composers.composers", "qualname": "Composer.placeholder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.model_runners.composers.composers.Composer.vanilla_template", "modulename": "src.fmeval.model_runners.composers.composers", "qualname": "Composer.vanilla_template", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.model_runners.composers.composers.Composer.compose", "modulename": "src.fmeval.model_runners.composers.composers", "qualname": "Composer.compose", "kind": "function", "doc": "<p>Composes an object using the input data, self.vanilla_template, and self.placeholder.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>data</strong>:  The data used to compose a new object.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>A new object composed using <code>data</code>, self.vanilla_template, and self.placeholder.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.composers.composers.JsonContentComposer", "modulename": "src.fmeval.model_runners.composers.composers", "qualname": "JsonContentComposer", "kind": "class", "doc": "<p>Composer for models that expect a JSON payload, i.e. models\nwith content_type == \"application/json\".</p>\n", "bases": "Composer"}, {"fullname": "src.fmeval.model_runners.composers.composers.JsonContentComposer.__init__", "modulename": "src.fmeval.model_runners.composers.composers", "qualname": "JsonContentComposer.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>template: A template string. Ex: '{\"data\"</strong>: $prompt}'</li>\n<li><strong>placeholder</strong>:  A placeholder keyword. This keyword appears\nin <code>template</code> with a $ sign prepended. In the above example,\nthe placeholder is \"prompt\".</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">template</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "src.fmeval.model_runners.composers.composers.JsonContentComposer.PLACEHOLDER", "modulename": "src.fmeval.model_runners.composers.composers", "qualname": "JsonContentComposer.PLACEHOLDER", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;prompt&#x27;"}, {"fullname": "src.fmeval.model_runners.composers.composers.JsonContentComposer.compose", "modulename": "src.fmeval.model_runners.composers.composers", "qualname": "JsonContentComposer.compose", "kind": "function", "doc": "<p>The placeholder $prompt is replaced by a single JSON prompt. E.g.,\ntemplate: '{\"data\": $prompt}'\ndata:     \"[\"John\",40]\"\nresult:   {\"data\": \"[\"John\",40]\"}\nThis composer uses json.dumps to make sure the double quotes included are properly escaped.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>data</strong>:  The data used to replace self.placeholder in self.vanilla_template.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>A JSON object representing a prompt that will be consumed by a model.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">,</span> <span class=\"n\">Dict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.composers.composers.PromptComposer", "modulename": "src.fmeval.model_runners.composers.composers", "qualname": "PromptComposer", "kind": "class", "doc": "<p>Composes LLM prompt inputs.</p>\n", "bases": "Composer"}, {"fullname": "src.fmeval.model_runners.composers.composers.PromptComposer.__init__", "modulename": "src.fmeval.model_runners.composers.composers", "qualname": "PromptComposer.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>template: A template string. Ex: '{\"data\"</strong>: $prompt}'</li>\n<li><strong>placeholder</strong>:  A placeholder keyword. This keyword appears\nin <code>template</code> with a $ sign prepended. In the above example,\nthe placeholder is \"prompt\".</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">template</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "src.fmeval.model_runners.composers.composers.PromptComposer.PLACEHOLDER", "modulename": "src.fmeval.model_runners.composers.composers", "qualname": "PromptComposer.PLACEHOLDER", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;feature&#x27;"}, {"fullname": "src.fmeval.model_runners.composers.composers.PromptComposer.compose", "modulename": "src.fmeval.model_runners.composers.composers", "qualname": "PromptComposer.compose", "kind": "function", "doc": "<p>Composes a prompt that will be fed to an LLM.\nExample:\n    data = \"London is the capital of\"\n    composed prompt =\n        \"<s>[INST] &lt;<SYS>&gt;Answer the following question in as few words as possible.&lt;</SYS>&gt;\n        Question: London is the capital of [/INST]\"</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>data</strong>:  The original string that forms the basis of the returned prompt.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>A prompt composed by replacing self.placeholder in self.vanilla_template with <code>data</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.composers.jumpstart_composer", "modulename": "src.fmeval.model_runners.composers.jumpstart_composer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.model_runners.composers.jumpstart_composer.JumpStartComposer", "modulename": "src.fmeval.model_runners.composers.jumpstart_composer", "qualname": "JumpStartComposer", "kind": "class", "doc": "<p>Jumpstart model request composer</p>\n", "bases": "fmeval.model_runners.composers.composers.Composer"}, {"fullname": "src.fmeval.model_runners.composers.jumpstart_composer.JumpStartComposer.__init__", "modulename": "src.fmeval.model_runners.composers.jumpstart_composer", "qualname": "JumpStartComposer.__init__", "kind": "function", "doc": "<p>Initialize the JumpStartComposer for the given JumpStart model_id and model_version.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">jumpstart_model_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">jumpstart_model_version</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "src.fmeval.model_runners.composers.jumpstart_composer.JumpStartComposer.compose", "modulename": "src.fmeval.model_runners.composers.jumpstart_composer", "qualname": "JumpStartComposer.compose", "kind": "function", "doc": "<p>Composes the payload for the given JumpStartModel from the provided prompt.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">prompt</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">sagemaker</span><span class=\"o\">.</span><span class=\"n\">jumpstart</span><span class=\"o\">.</span><span class=\"n\">types</span><span class=\"o\">.</span><span class=\"n\">JumpStartSerializablePayload</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.composers.template", "modulename": "src.fmeval.model_runners.composers.template", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.model_runners.composers.template.logger", "modulename": "src.fmeval.model_runners.composers.template", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.model_runners.composers.template (WARNING)&gt;"}, {"fullname": "src.fmeval.model_runners.composers.template.VanillaTemplate", "modulename": "src.fmeval.model_runners.composers.template", "qualname": "VanillaTemplate", "kind": "class", "doc": "<p>Extend the standard string.Template class with an utility method.</p>\n", "bases": "string.Template"}, {"fullname": "src.fmeval.model_runners.composers.template.VanillaTemplate.get_unique_identifiers", "modulename": "src.fmeval.model_runners.composers.template", "qualname": "VanillaTemplate.get_unique_identifiers", "kind": "function", "doc": "<p>Returns a list of the unique identifiers in the template.</p>\n\n<p>The identifiers are in the order they appear, ignoring any invalid identifiers.\nThe method originates from Python 3.11 Template.get_identifiers (see [1] and [2]),\nbut with additional checks to disallow reappearing identifiers in the template.</p>\n\n<p>[1] <a href=\"https://docs.python.org/3/library/string.html#string.Template.get_identifiers\">https://docs.python.org/3/library/string.html#string.Template.get_identifiers</a>\n[2] <a href=\"https://github.com/python/cpython/blob/3.11/Lib/string.py#L157\">https://github.com/python/cpython/blob/3.11/Lib/string.py#L157</a></p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>The list of unique identifiers.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.composers.template.VanillaTemplate.pattern", "modulename": "src.fmeval.model_runners.composers.template", "qualname": "VanillaTemplate.pattern", "kind": "variable", "doc": "<p></p>\n", "default_value": "re.compile(&#x27;\\n            \\\\$(?:\\n              (?P&lt;escaped&gt;\\\\$)  |   # Escape sequence of two delimiters\\n              (?P&lt;named&gt;(?a:[_a-z][_a-z0-9]*))       |   # delimiter and a Python identifier\\n          , re.IGNORECASE|re.VERBOSE)"}, {"fullname": "src.fmeval.model_runners.extractors", "modulename": "src.fmeval.model_runners.extractors", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.model_runners.extractors.create_extractor", "modulename": "src.fmeval.model_runners.extractors", "qualname": "create_extractor", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_accept_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;application/json&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">output_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">log_probability_location</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.extractors.extractor", "modulename": "src.fmeval.model_runners.extractors.extractor", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.model_runners.extractors.extractor.logger", "modulename": "src.fmeval.model_runners.extractors.extractor", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.model_runners.extractors.extractor (WARNING)&gt;"}, {"fullname": "src.fmeval.model_runners.extractors.extractor.Extractor", "modulename": "src.fmeval.model_runners.extractors.extractor", "qualname": "Extractor", "kind": "class", "doc": "<p>Interface class for model response extractors.</p>\n", "bases": "abc.ABC"}, {"fullname": "src.fmeval.model_runners.extractors.extractor.Extractor.extract_log_probability", "modulename": "src.fmeval.model_runners.extractors.extractor", "qualname": "Extractor.extract_log_probability", "kind": "function", "doc": "<p>Extract log probability from model response.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>data</strong>:  Model response.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>A list of lists, where each element is a list of probabilities.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">,</span> <span class=\"n\">Dict</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">num_records</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">],</span> <span class=\"nb\">float</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.extractors.extractor.Extractor.extract_output", "modulename": "src.fmeval.model_runners.extractors.extractor", "qualname": "Extractor.extract_output", "kind": "function", "doc": "<p>Extract output from model response.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>data</strong>:  Model response.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>model output</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">,</span> <span class=\"n\">Dict</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">num_records</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.extractors.json_extractor", "modulename": "src.fmeval.model_runners.extractors.json_extractor", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.model_runners.extractors.json_extractor.logger", "modulename": "src.fmeval.model_runners.extractors.json_extractor", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.model_runners.extractors.json_extractor (WARNING)&gt;"}, {"fullname": "src.fmeval.model_runners.extractors.json_extractor.JsonExtractor", "modulename": "src.fmeval.model_runners.extractors.json_extractor", "qualname": "JsonExtractor", "kind": "class", "doc": "<p>JSON model response extractor</p>\n", "bases": "fmeval.model_runners.extractors.extractor.Extractor"}, {"fullname": "src.fmeval.model_runners.extractors.json_extractor.JsonExtractor.__init__", "modulename": "src.fmeval.model_runners.extractors.json_extractor", "qualname": "JsonExtractor.__init__", "kind": "function", "doc": "<p>Creates an instance of Json extractor that can extract the output and log probability from the JSON model\nresponse.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>output_jmespath_expression</strong>:  JMESPath expression of the output string</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_jmespath_expression</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">log_probability_jmespath_expression</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "src.fmeval.model_runners.extractors.json_extractor.JsonExtractor.log_probability_jmespath_expression", "modulename": "src.fmeval.model_runners.extractors.json_extractor", "qualname": "JsonExtractor.log_probability_jmespath_expression", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.model_runners.extractors.json_extractor.JsonExtractor.log_probability_jmespath", "modulename": "src.fmeval.model_runners.extractors.json_extractor", "qualname": "JsonExtractor.log_probability_jmespath", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.model_runners.extractors.json_extractor.JsonExtractor.output_jmespath_expression", "modulename": "src.fmeval.model_runners.extractors.json_extractor", "qualname": "JsonExtractor.output_jmespath_expression", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.model_runners.extractors.json_extractor.JsonExtractor.output_jmespath", "modulename": "src.fmeval.model_runners.extractors.json_extractor", "qualname": "JsonExtractor.output_jmespath", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.model_runners.extractors.json_extractor.JsonExtractor.extract_log_probability", "modulename": "src.fmeval.model_runners.extractors.json_extractor", "qualname": "JsonExtractor.extract_log_probability", "kind": "function", "doc": "<p>Extract log probability from model response.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>data</strong>:  Model response. The log_probability_jmespath_expression is used to extract the log probabilities\nof the input tokens. Each record in the extracted probabilities will be a float or list of floats.\nExamples for the extracted probabilities:\n<ul>\n<li>data: 0.1, num_records: 1, num tokens: 1 (or probabilities already summed up)</li>\n<li>data: [0.1], num_records: 1, num tokens: 1 (or probabilities already summed up)</li>\n<li>data: [0.1, 0.2], num_records: 1, num tokens: 2</li>\n</ul></li>\n<li><strong>num_records</strong>:  number of inference records in the model output</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>float or list of float where each float is sum of log probabilities.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">,</span> <span class=\"n\">Dict</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">num_records</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">],</span> <span class=\"nb\">float</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.extractors.json_extractor.JsonExtractor.extract_output", "modulename": "src.fmeval.model_runners.extractors.json_extractor", "qualname": "JsonExtractor.extract_output", "kind": "function", "doc": "<p>Extract output from JSON model output</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>data</strong>:  Model response. The output_jmespath_expression is used to extract the predicted output. The\npredicted output must be a string</li>\n<li><strong>num_records</strong>:  number of inference records in the model output</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>model output</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">,</span> <span class=\"n\">Dict</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">num_records</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.extractors.jumpstart_extractor", "modulename": "src.fmeval.model_runners.extractors.jumpstart_extractor", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.model_runners.extractors.jumpstart_extractor.JS_LOG_PROB_JMESPATH", "modulename": "src.fmeval.model_runners.extractors.jumpstart_extractor", "qualname": "JS_LOG_PROB_JMESPATH", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;[0].details.prefill[*].logprob&#x27;"}, {"fullname": "src.fmeval.model_runners.extractors.jumpstart_extractor.JumpStartExtractor", "modulename": "src.fmeval.model_runners.extractors.jumpstart_extractor", "qualname": "JumpStartExtractor", "kind": "class", "doc": "<p>JumpStart model response extractor</p>\n", "bases": "fmeval.model_runners.extractors.extractor.Extractor"}, {"fullname": "src.fmeval.model_runners.extractors.jumpstart_extractor.JumpStartExtractor.__init__", "modulename": "src.fmeval.model_runners.extractors.jumpstart_extractor", "qualname": "JumpStartExtractor.__init__", "kind": "function", "doc": "<p>Initializes  JumpStartExtractor for the given model and version.\nThis extractor does not support batching at this time.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>jumpstart_model_id</strong>:  The model id of the JumpStart Model</li>\n<li><strong>jumpstart_model_id</strong>:  The model version of the JumpStart Model</li>\n<li><strong>sagemaker_session</strong>:  Optional. An object of SageMaker session</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">jumpstart_model_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">jumpstart_model_version</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">sagemaker_session</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">sagemaker</span><span class=\"o\">.</span><span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "src.fmeval.model_runners.extractors.jumpstart_extractor.JumpStartExtractor.extract_log_probability", "modulename": "src.fmeval.model_runners.extractors.jumpstart_extractor", "qualname": "JumpStartExtractor.extract_log_probability", "kind": "function", "doc": "<p>Extracts the log probability from the JumpStartModel response. This value is not provided by all JS text models.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>data</strong>:  The model response from the JumpStart Model</li>\n<li><strong>num_records</strong>:  The number of records in the model response. Must be 1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">,</span> <span class=\"n\">Dict</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">num_records</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.extractors.jumpstart_extractor.JumpStartExtractor.extract_output", "modulename": "src.fmeval.model_runners.extractors.jumpstart_extractor", "qualname": "JumpStartExtractor.extract_output", "kind": "function", "doc": "<p>Extracts the output string from the JumpStartModel response. This value is provided by all JS text models, but\nnot all JS FM models. This only supported for text-to-text models.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>data</strong>:  The model response from the JumpStart Model</li>\n<li><strong>num_records</strong>:  The number of records in the model response. Must be 1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">,</span> <span class=\"n\">Dict</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">num_records</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.extractors.jumpstart_extractor.JumpStartExtractor.get_jumpstart_sdk_manifest", "modulename": "src.fmeval.model_runners.extractors.jumpstart_extractor", "qualname": "JumpStartExtractor.get_jumpstart_sdk_manifest", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">region</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.extractors.jumpstart_extractor.JumpStartExtractor.get_jumpstart_sdk_spec", "modulename": "src.fmeval.model_runners.extractors.jumpstart_extractor", "qualname": "JumpStartExtractor.get_jumpstart_sdk_spec", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">region</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.model_runner", "modulename": "src.fmeval.model_runners.model_runner", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.model_runners.model_runner.ModelRunner", "modulename": "src.fmeval.model_runners.model_runner", "qualname": "ModelRunner", "kind": "class", "doc": "<p>This class is responsible for running the model and extracting the model output.</p>\n\n<p>It handles everything related to the model, including: model deployment, payload construction for invocations,\nand making sense of the model output.</p>\n", "bases": "abc.ABC"}, {"fullname": "src.fmeval.model_runners.model_runner.ModelRunner.__init__", "modulename": "src.fmeval.model_runners.model_runner", "qualname": "ModelRunner.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>content_template</strong>:  String template to compose the model input from the prompt</li>\n<li><strong>output</strong>:  JMESPath expression of output in the model output</li>\n<li><strong>log_probability</strong>:  JMESPath expression of log probability in the model output</li>\n<li><strong>content_type</strong>:  The content type of the request sent to the model for inference</li>\n<li><strong>accept_type</strong>:  The accept type of the request sent to the model for inference</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">content_template</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">output</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">log_probability</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">content_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;application/json&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">accept_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;application/json&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "src.fmeval.model_runners.model_runner.ModelRunner.predict", "modulename": "src.fmeval.model_runners.model_runner", "qualname": "ModelRunner.predict", "kind": "function", "doc": "<p>Runs the model on the given prompt. This includes updating the prompt to fit the request format that the model\nexpects, and extracting the output and log probability from the model response. The response of the ModelRunner\nwill be a tuple of (output, log_probability)</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>prompt</strong>:  the prompt</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>the tuple containing model output string and the log probability</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">prompt</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.sm_jumpstart_model_runner", "modulename": "src.fmeval.model_runners.sm_jumpstart_model_runner", "kind": "module", "doc": "<p>Module to manage model runners for SageMaker Jumpstart endpoints.</p>\n"}, {"fullname": "src.fmeval.model_runners.sm_jumpstart_model_runner.logger", "modulename": "src.fmeval.model_runners.sm_jumpstart_model_runner", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.model_runners.sm_jumpstart_model_runner (WARNING)&gt;"}, {"fullname": "src.fmeval.model_runners.sm_jumpstart_model_runner.JumpStartModelRunner", "modulename": "src.fmeval.model_runners.sm_jumpstart_model_runner", "qualname": "JumpStartModelRunner", "kind": "class", "doc": "<p>A class to manage the creation and deletion of SageMaker Jumpstart model runner when user provides\na SageMaker Jumpstart endpoint name from a SageMaker Jumpstart model.</p>\n", "bases": "fmeval.model_runners.model_runner.ModelRunner"}, {"fullname": "src.fmeval.model_runners.sm_jumpstart_model_runner.JumpStartModelRunner.__init__", "modulename": "src.fmeval.model_runners.sm_jumpstart_model_runner", "qualname": "JumpStartModelRunner.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>endpoint_name</strong>:  Name of the SageMaker endpoint to be used for model predictions</li>\n<li><strong>model_id</strong>:  Identifier of the SageMaker Jumpstart model</li>\n<li><strong>content_template</strong>:  String template to compose the model input from the prompt</li>\n<li><strong>model_version</strong>:  Version of the SageMaker Jumpstart model</li>\n<li><strong>custom_attributes</strong>:  String that contains the custom attributes to be passed to\nSageMaker endpoint invocation</li>\n<li><strong>output</strong>:  JMESPath expression of output in the model output</li>\n<li><strong>log_probability</strong>:  JMESPath expression of log probability in the model output</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">endpoint_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">content_template</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_version</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;*&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">custom_attributes</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">output</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">log_probability</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "src.fmeval.model_runners.sm_jumpstart_model_runner.JumpStartModelRunner.predict", "modulename": "src.fmeval.model_runners.sm_jumpstart_model_runner", "qualname": "JumpStartModelRunner.predict", "kind": "function", "doc": "<p>Invoke the SageMaker endpoint and parse the model response.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>prompt</strong>:  Input data for which you want the model to provide inference.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">prompt</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.sm_model_runner", "modulename": "src.fmeval.model_runners.sm_model_runner", "kind": "module", "doc": "<p>Module to manage model runners for SageMaker endpoints.</p>\n"}, {"fullname": "src.fmeval.model_runners.sm_model_runner.logger", "modulename": "src.fmeval.model_runners.sm_model_runner", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.model_runners.sm_model_runner (WARNING)&gt;"}, {"fullname": "src.fmeval.model_runners.sm_model_runner.SageMakerModelRunner", "modulename": "src.fmeval.model_runners.sm_model_runner", "qualname": "SageMakerModelRunner", "kind": "class", "doc": "<p>A class to manage the creation and deletion of SageMaker model runner when user provides\na SageMaker endpoint name from a SageMaker model.</p>\n", "bases": "fmeval.model_runners.model_runner.ModelRunner"}, {"fullname": "src.fmeval.model_runners.sm_model_runner.SageMakerModelRunner.__init__", "modulename": "src.fmeval.model_runners.sm_model_runner", "qualname": "SageMakerModelRunner.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>endpoint_name</strong>:  Name of the SageMaker endpoint to be used for model predictions</li>\n<li><strong>content_template</strong>:  String template to compose the model input from the prompt</li>\n<li><strong>custom_attributes</strong>:  String that contains the custom attributes to be passed to\nSageMaker endpoint invocation</li>\n<li><strong>output</strong>:  JMESPath expression of output in the model output</li>\n<li><strong>log_probability</strong>:  JMESPath expression of log probability in the model output</li>\n<li><strong>content_type</strong>:  The content type of the request sent to the model for inference</li>\n<li><strong>accept_type</strong>:  The accept type of the request sent to the model for inference</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">endpoint_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">content_template</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">custom_attributes</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">output</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">log_probability</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">content_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;application/json&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">accept_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;application/json&#39;</span></span>)</span>"}, {"fullname": "src.fmeval.model_runners.sm_model_runner.SageMakerModelRunner.predict", "modulename": "src.fmeval.model_runners.sm_model_runner", "qualname": "SageMakerModelRunner.predict", "kind": "function", "doc": "<p>Invoke the SageMaker endpoint and parse the model response.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>prompt</strong>:  Input data for which you want the model to provide inference.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">prompt</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.util", "modulename": "src.fmeval.model_runners.util", "kind": "module", "doc": "<p>Utilities for model runners.</p>\n"}, {"fullname": "src.fmeval.model_runners.util.logger", "modulename": "src.fmeval.model_runners.util", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger src.fmeval.model_runners.util (WARNING)&gt;"}, {"fullname": "src.fmeval.model_runners.util.get_boto_session", "modulename": "src.fmeval.model_runners.util", "qualname": "get_boto_session", "kind": "function", "doc": "<p>Get boto3 session with adaptive retry config</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>The new session</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"n\">boto3</span><span class=\"o\">.</span><span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">Session</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.util.get_sagemaker_session", "modulename": "src.fmeval.model_runners.util", "qualname": "get_sagemaker_session", "kind": "function", "doc": "<p>Get SageMaker session with adaptive retry config.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>boto_retry_mode</strong>:  retry mode used for botocore config (legacy/standard/adaptive).</li>\n<li><strong>retry_attempts</strong>:  max retry attempts used for botocore client failures</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>The new session</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">boto_retry_mode</span><span class=\"p\">:</span> <span class=\"n\">Literal</span><span class=\"p\">[</span><span class=\"s1\">&#39;legacy&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;standard&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;adaptive&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;adaptive&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">retry_attempts</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">10</span></span><span class=\"return-annotation\">) -> <span class=\"n\">sagemaker</span><span class=\"o\">.</span><span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">Session</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.util.get_bedrock_runtime_client", "modulename": "src.fmeval.model_runners.util", "qualname": "get_bedrock_runtime_client", "kind": "function", "doc": "<p>Get Bedrock runtime client with adaptive retry config.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>boto_retry_mode</strong>:  retry mode used for botocore config (legacy/standard/adaptive).</li>\n<li><strong>retry_attempts</strong>:  max retry attempts used for botocore client failures</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>The new session</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">boto_retry_mode</span><span class=\"p\">:</span> <span class=\"n\">Literal</span><span class=\"p\">[</span><span class=\"s1\">&#39;legacy&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;standard&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;adaptive&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;adaptive&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">retry_attempts</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">10</span></span><span class=\"return-annotation\">) -> <span class=\"o\">&lt;</span><span class=\"n\">function</span> <span class=\"n\">Session</span><span class=\"o\">.</span><span class=\"n\">client</span> <span class=\"n\">at</span> <span class=\"mh\">0x12101e170</span><span class=\"o\">&gt;</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.model_runners.util.is_endpoint_in_service", "modulename": "src.fmeval.model_runners.util", "qualname": "is_endpoint_in_service", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>sagemaker_session</strong>:  SageMaker session to be reused.</li>\n<li><strong>endpoint_name</strong>:  SageMaker endpoint name.\n:return None</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">sagemaker_session</span><span class=\"p\">:</span> <span class=\"n\">sagemaker</span><span class=\"o\">.</span><span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">Session</span>, </span><span class=\"param\"><span class=\"n\">endpoint_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.perf_util", "modulename": "src.fmeval.perf_util", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.perf_util.timed_block", "modulename": "src.fmeval.perf_util", "qualname": "timed_block", "kind": "function", "doc": "<p>Measure and log execution time for the code block in the context</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>block_name</strong>:  a string describing the code block</li>\n<li><strong>logger</strong>:  used to log the execution time</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">block_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">logger</span><span class=\"p\">:</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">Logger</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.reporting", "modulename": "src.fmeval.reporting", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.reporting.cells", "modulename": "src.fmeval.reporting.cells", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.reporting.cells.Cell", "modulename": "src.fmeval.reporting.cells", "qualname": "Cell", "kind": "class", "doc": "<p>Base class for a report cell.</p>\n", "bases": "abc.ABC"}, {"fullname": "src.fmeval.reporting.cells.MarkdownCell", "modulename": "src.fmeval.reporting.cells", "qualname": "MarkdownCell", "kind": "class", "doc": "<p>Base class representing a markdown cell.</p>\n", "bases": "Cell"}, {"fullname": "src.fmeval.reporting.cells.MarkdownCell.__init__", "modulename": "src.fmeval.reporting.cells", "qualname": "MarkdownCell.__init__", "kind": "function", "doc": "<p>Input may be strings or MarkdownCells.\n        Examples:\n            assert str(MarkdownCell(\"# Hello1\")) == \"# Hello1\"\n            assert str(MarkdownCell(\"# Hello1\", \"# Hello2\")) == \"# Hello1  </p>\n\n<h1 id=\"hello2\">Hello2\"</h1>\n\n<pre><code>        assert str(MarkdownCell(MarkdownCell(\"# Hello1\"), \"# Hello2\") == \"# Hello1\n</code></pre>\n\n<h1 id=\"hello2-2\">Hello2\"</h1>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">*</span><span class=\"n\">args</span></span>)</span>"}, {"fullname": "src.fmeval.reporting.cells.MarkdownCell.content", "modulename": "src.fmeval.reporting.cells", "qualname": "MarkdownCell.content", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List[Union[str, src.fmeval.reporting.cells.MarkdownCell]]"}, {"fullname": "src.fmeval.reporting.cells.MarkdownCell.show", "modulename": "src.fmeval.reporting.cells", "qualname": "MarkdownCell.show", "kind": "function", "doc": "<p>Displays the cell content in an IPython notebook cell.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.reporting.cells.HeadingCell", "modulename": "src.fmeval.reporting.cells", "qualname": "HeadingCell", "kind": "class", "doc": "<p>This class represents a Markdown heading.</p>\n", "bases": "MarkdownCell"}, {"fullname": "src.fmeval.reporting.cells.HeadingCell.__init__", "modulename": "src.fmeval.reporting.cells", "qualname": "HeadingCell.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>text</strong>:  The text for this header</li>\n<li><strong>level</strong>:  The heading level</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">level</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span>)</span>"}, {"fullname": "src.fmeval.reporting.cells.BoldCell", "modulename": "src.fmeval.reporting.cells", "qualname": "BoldCell", "kind": "class", "doc": "<p>This class represents a bold piece of text.</p>\n", "bases": "MarkdownCell"}, {"fullname": "src.fmeval.reporting.cells.BoldCell.__init__", "modulename": "src.fmeval.reporting.cells", "qualname": "BoldCell.__init__", "kind": "function", "doc": "<p>Input may be strings or MarkdownCells.\n        Examples:\n            assert str(MarkdownCell(\"# Hello1\")) == \"# Hello1\"\n            assert str(MarkdownCell(\"# Hello1\", \"# Hello2\")) == \"# Hello1  </p>\n\n<h1 id=\"hello2\">Hello2\"</h1>\n\n<pre><code>        assert str(MarkdownCell(MarkdownCell(\"# Hello1\"), \"# Hello2\") == \"# Hello1\n</code></pre>\n\n<h1 id=\"hello2-2\">Hello2\"</h1>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">text</span></span>)</span>"}, {"fullname": "src.fmeval.reporting.cells.ListCell", "modulename": "src.fmeval.reporting.cells", "qualname": "ListCell", "kind": "class", "doc": "<p>Creates a bulleted or numbered list.</p>\n", "bases": "MarkdownCell"}, {"fullname": "src.fmeval.reporting.cells.ListCell.__init__", "modulename": "src.fmeval.reporting.cells", "qualname": "ListCell.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>items</strong>:   A list of strings where each string represents one item in the list.</li>\n<li><strong>list_type</strong>:  Whether the list is bulleted or numbered.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">items</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">list_type</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">reporting</span><span class=\"o\">.</span><span class=\"n\">constants</span><span class=\"o\">.</span><span class=\"n\">ListType</span></span>)</span>"}, {"fullname": "src.fmeval.reporting.cells.ColumnsLayoutCell", "modulename": "src.fmeval.reporting.cells", "qualname": "ColumnsLayoutCell", "kind": "class", "doc": "<p>This class creates a multi-column layout cell</p>\n", "bases": "MarkdownCell"}, {"fullname": "src.fmeval.reporting.cells.ColumnsLayoutCell.__init__", "modulename": "src.fmeval.reporting.cells", "qualname": "ColumnsLayoutCell.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>columns</strong>:  A list of Lists of strings or MarkdownCells, where each inner list is one column</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">columns</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]]</span></span>)</span>"}, {"fullname": "src.fmeval.reporting.cells.FigureCell", "modulename": "src.fmeval.reporting.cells", "qualname": "FigureCell", "kind": "class", "doc": "<p>This class represents a MarkdownCell containing HTML for a Pyplot Figure.</p>\n", "bases": "MarkdownCell"}, {"fullname": "src.fmeval.reporting.cells.FigureCell.__init__", "modulename": "src.fmeval.reporting.cells", "qualname": "FigureCell.__init__", "kind": "function", "doc": "<p>Initializes a FigureCell.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>fig</strong>:  The Pyplot figure that this cell represents.</li>\n<li><strong>width</strong>:  See _html_wrapper docstring</li>\n<li><strong>height</strong>:  See _html_wrapper docstring</li>\n<li><strong>center</strong>:  if the figure is center aligned</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">fig</span><span class=\"p\">:</span> <span class=\"n\">matplotlib</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"o\">.</span><span class=\"n\">Figure</span>,</span><span class=\"param\">\t<span class=\"n\">width</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">height</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">center</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span>)</span>"}, {"fullname": "src.fmeval.reporting.cells.BarPlotCell", "modulename": "src.fmeval.reporting.cells", "qualname": "BarPlotCell", "kind": "class", "doc": "<p>This class represents a Pyplot bar plot figure.</p>\n", "bases": "FigureCell"}, {"fullname": "src.fmeval.reporting.cells.BarPlotCell.__init__", "modulename": "src.fmeval.reporting.cells", "qualname": "BarPlotCell.__init__", "kind": "function", "doc": "<p>Initializes a BarPlotCell.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>labels</strong>:  The labels corresponding to each of the bars in the plot</li>\n<li><strong>heights</strong>:  The heights of the bars in the plot</li>\n<li><strong>title</strong>:  The title of the bar plot</li>\n<li><strong>plot_height</strong>:  Height of the plot as a string</li>\n<li><strong>plot_width</strong>:  Width the plot as a string</li>\n<li><strong>center</strong>:  Boolean indicating if the plot should be center aligned in the page</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">labels</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">heights</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">color</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">title</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;Title&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">plot_height</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">plot_width</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">center</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">origin</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span>)</span>"}, {"fullname": "src.fmeval.reporting.cells.TableCell", "modulename": "src.fmeval.reporting.cells", "qualname": "TableCell", "kind": "class", "doc": "<p>This class represents an HTML table.</p>\n\n<p>Note that despite having \"Cell\" in its name, this class does <em>not</em>\nrepresent a single cell within an HTML table, but rather the entire table.\nThe \"Cell\" suffix is included to match the naming convention for subclasses\nof MarkdownCell.</p>\n", "bases": "MarkdownCell"}, {"fullname": "src.fmeval.reporting.cells.TableCell.__init__", "modulename": "src.fmeval.reporting.cells", "qualname": "TableCell.__init__", "kind": "function", "doc": "<p>Initializes a TableCell.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>data</strong>:  A 2D array representing tabular data</li>\n<li><strong>headers</strong>:  The table's headers, i.e. column names</li>\n<li><strong>table_align</strong>:  The alignment of the table within the overarching markdown</li>\n<li><strong>cell_align</strong>:  The alignment of text within each cell of the table</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">headers</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">table_align</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;center&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">cell_align</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;right&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">style</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">caption</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "src.fmeval.reporting.constants", "modulename": "src.fmeval.reporting.constants", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.reporting.constants.CENTER", "modulename": "src.fmeval.reporting.constants", "qualname": "CENTER", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;center&#x27;"}, {"fullname": "src.fmeval.reporting.constants.LEFT", "modulename": "src.fmeval.reporting.constants", "qualname": "LEFT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;left&#x27;"}, {"fullname": "src.fmeval.reporting.constants.RIGHT", "modulename": "src.fmeval.reporting.constants", "qualname": "RIGHT", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;right&#x27;"}, {"fullname": "src.fmeval.reporting.constants.ListType", "modulename": "src.fmeval.reporting.constants", "qualname": "ListType", "kind": "class", "doc": "<p>An enumeration.</p>\n", "bases": "enum.Enum"}, {"fullname": "src.fmeval.reporting.constants.ListType.BULLETED", "modulename": "src.fmeval.reporting.constants", "qualname": "ListType.BULLETED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ListType.BULLETED: &#x27;bulleted&#x27;&gt;"}, {"fullname": "src.fmeval.reporting.constants.ListType.NUMBERED", "modulename": "src.fmeval.reporting.constants", "qualname": "ListType.NUMBERED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;ListType.NUMBERED: &#x27;numbered&#x27;&gt;"}, {"fullname": "src.fmeval.reporting.constants.SINGLE_NEWLINE", "modulename": "src.fmeval.reporting.constants", "qualname": "SINGLE_NEWLINE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;  \\n&#x27;"}, {"fullname": "src.fmeval.reporting.constants.DOUBLE_NEWLINE", "modulename": "src.fmeval.reporting.constants", "qualname": "DOUBLE_NEWLINE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;  \\n\\n&#x27;"}, {"fullname": "src.fmeval.reporting.constants.NUM_SAMPLES_TO_DISPLAY_IN_TABLE", "modulename": "src.fmeval.reporting.constants", "qualname": "NUM_SAMPLES_TO_DISPLAY_IN_TABLE", "kind": "variable", "doc": "<p></p>\n", "default_value": "5"}, {"fullname": "src.fmeval.reporting.constants.CATEGORY_BAR_COLOR", "modulename": "src.fmeval.reporting.constants", "qualname": "CATEGORY_BAR_COLOR", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;steelblue&#x27;"}, {"fullname": "src.fmeval.reporting.constants.OVERALL_BAR_COLOR", "modulename": "src.fmeval.reporting.constants", "qualname": "OVERALL_BAR_COLOR", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;coral&#x27;"}, {"fullname": "src.fmeval.reporting.constants.MAX_CHAR", "modulename": "src.fmeval.reporting.constants", "qualname": "MAX_CHAR", "kind": "variable", "doc": "<p></p>\n", "default_value": "200"}, {"fullname": "src.fmeval.reporting.constants.MARKDOWN_EXTENSIONS", "modulename": "src.fmeval.reporting.constants", "qualname": "MARKDOWN_EXTENSIONS", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;tables&#x27;, &#x27;md_in_html&#x27;]"}, {"fullname": "src.fmeval.reporting.constants.DATASET_SCORE_LABEL", "modulename": "src.fmeval.reporting.constants", "qualname": "DATASET_SCORE_LABEL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;Overall&#x27;"}, {"fullname": "src.fmeval.reporting.constants.AGGREGATE_ONLY_SCORES", "modulename": "src.fmeval.reporting.constants", "qualname": "AGGREGATE_ONLY_SCORES", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;balanced_accuracy_score&#x27;, &#x27;precision_score&#x27;, &#x27;recall_score&#x27;]"}, {"fullname": "src.fmeval.reporting.constants.GENERAL_STRING_REPLACEMENTS", "modulename": "src.fmeval.reporting.constants", "qualname": "GENERAL_STRING_REPLACEMENTS", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List[Tuple[str, str]]", "default_value": "[(&#x27;qa&#x27;, &#x27;Q&amp;A&#x27;), (&#x27;f1&#x27;, &#x27;F1&#x27;)]"}, {"fullname": "src.fmeval.reporting.constants.SCORE_STRING_REPLACEMENTS", "modulename": "src.fmeval.reporting.constants", "qualname": "SCORE_STRING_REPLACEMENTS", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List[Tuple[str, str]]", "default_value": "[(&#x27;prompt stereotyping&#x27;, &#x27;is_biased&#x27;), (&#x27;meteor&#x27;, &#x27;METEOR&#x27;), (&#x27;bertscore&#x27;, &#x27;BERTScore&#x27;), (&#x27;rouge&#x27;, &#x27;ROUGE&#x27;), (&#x27;F1 score&#x27;, &#x27;F1 over words&#x27;), (&#x27;obscene&#x27;, &#x27;Obscenity&#x27;), (&#x27;sexual explicit&#x27;, &#x27;Sexual Explicitness&#x27;)]"}, {"fullname": "src.fmeval.reporting.constants.EVAL_NAME_STRING_REPLACEMENTS", "modulename": "src.fmeval.reporting.constants", "qualname": "EVAL_NAME_STRING_REPLACEMENTS", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List[Tuple[str, str]]", "default_value": "[(&#x27;qa_accuracy&#x27;, &#x27;accuracy&#x27;), (&#x27;summarization_accuracy&#x27;, &#x27;accuracy&#x27;), (&#x27;classification_accuracy&#x27;, &#x27;accuracy&#x27;), (&#x27;general_semantic_robustness&#x27;, &#x27;semantic_robustness&#x27;), (&#x27;accuracy_semantic_robustness&#x27;, &#x27;semantic_robustness&#x27;), (&#x27;qa_accuracy&#x27;, &#x27;toxicity&#x27;), (&#x27;summarization_toxicity&#x27;, &#x27;toxicity&#x27;), (&#x27;classification_accuracy&#x27;, &#x27;toxicity&#x27;)]"}, {"fullname": "src.fmeval.reporting.constants.PLOT_TITLE_STRING_REPLACEMENTS", "modulename": "src.fmeval.reporting.constants", "qualname": "PLOT_TITLE_STRING_REPLACEMENTS", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List[Tuple[str, str]]", "default_value": "[(&#x27;prompt_stereotyping&#x27;, &#x27;is_biased score&#x27;)]"}, {"fullname": "src.fmeval.reporting.constants.COLUMN_NAME_STRING_REPLACEMENTS", "modulename": "src.fmeval.reporting.constants", "qualname": "COLUMN_NAME_STRING_REPLACEMENTS", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List[Tuple[str, str]]", "default_value": "[(&#x27;sent_more&#x27;, &#x27;s_more&#x27;), (&#x27;s_more_input&#x27;, &#x27;&lt;math&gt;S&lt;sub&gt;more&lt;/sub&gt;&lt;/math&gt;&#x27;), (&#x27;sent_less&#x27;, &#x27;s_less&#x27;), (&#x27;s_less_input&#x27;, &#x27;&lt;math&gt;S&lt;sub&gt;less&lt;/sub&gt;&lt;/math&gt;&#x27;), (&#x27;prob_&#x27;, &#x27;probability_&#x27;), (&#x27;word_error_rate&#x27;, &#x27;Average WER&#x27;), (&#x27;classification_accuracy&#x27;, &#x27;accuracy&#x27;), (&#x27;f1_score&#x27;, &#x27;f1 over words&#x27;), (&#x27;meteor&#x27;, &#x27;METEOR&#x27;), (&#x27;bertscore&#x27;, &#x27;BERTScore&#x27;), (&#x27;rouge&#x27;, &#x27;ROUGE&#x27;)]"}, {"fullname": "src.fmeval.reporting.constants.AVOID_REMOVE_UNDERSCORE", "modulename": "src.fmeval.reporting.constants", "qualname": "AVOID_REMOVE_UNDERSCORE", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;sent_more_input&#x27;, &#x27;sent_less_input&#x27;, &#x27;is_biased&#x27;]"}, {"fullname": "src.fmeval.reporting.constants.ACCURACY_SEMANTIC_ROBUSTNESS_ALGOS", "modulename": "src.fmeval.reporting.constants", "qualname": "ACCURACY_SEMANTIC_ROBUSTNESS_ALGOS", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;summarization_accuracy_semantic_robustness&#x27;, &lt;EvalAlgorithm.QA_ACCURACY_SEMANTIC_ROBUSTNESS: &#x27;qa_accuracy_semantic_robustness&#x27;&gt;, &lt;EvalAlgorithm.CLASSIFICATION_ACCURACY_SEMANTIC_ROBUSTNESS: &#x27;classification_accuracy_semantic_robustness&#x27;&gt;]"}, {"fullname": "src.fmeval.reporting.constants.ACCURACY_SEMANTIC_ROBUSTNESS_SCORES", "modulename": "src.fmeval.reporting.constants", "qualname": "ACCURACY_SEMANTIC_ROBUSTNESS_SCORES", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;classification_accuracy_score&#x27;, &#x27;meteor&#x27;, &#x27;bertscore&#x27;, &#x27;rouge&#x27;, &#x27;f1_score&#x27;, &#x27;exact_match_score&#x27;, &#x27;quasi_exact_match_score&#x27;]"}, {"fullname": "src.fmeval.reporting.constants.BUILT_IN_DATASET", "modulename": "src.fmeval.reporting.constants", "qualname": "BUILT_IN_DATASET", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;Built-in Dataset&#x27;"}, {"fullname": "src.fmeval.reporting.constants.CUSTOM_DATASET", "modulename": "src.fmeval.reporting.constants", "qualname": "CUSTOM_DATASET", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;Custom Dataset&#x27;"}, {"fullname": "src.fmeval.reporting.constants.PROMPT_COLUMN_NAME", "modulename": "src.fmeval.reporting.constants", "qualname": "PROMPT_COLUMN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;prompt&#x27;"}, {"fullname": "src.fmeval.reporting.constants.TOXICITY_EVAL_NAMES", "modulename": "src.fmeval.reporting.constants", "qualname": "TOXICITY_EVAL_NAMES", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;toxicity&#x27;, &#x27;qa_toxicity&#x27;, &#x27;summarization_toxicity&#x27;]"}, {"fullname": "src.fmeval.reporting.constants.PROBABILITY_RATIO", "modulename": "src.fmeval.reporting.constants", "qualname": "PROBABILITY_RATIO", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;&lt;math&gt;&lt;box&gt;p(S&lt;sub&gt;more&lt;/sub&gt;)/p(S&lt;sub&gt;less&lt;/sub&gt;)&lt;/box&gt;&lt;/math&gt;&#x27;"}, {"fullname": "src.fmeval.reporting.constants.IS_BIASED", "modulename": "src.fmeval.reporting.constants", "qualname": "IS_BIASED", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;is_biased&#x27;"}, {"fullname": "src.fmeval.reporting.constants.TOXIGEN_NAME", "modulename": "src.fmeval.reporting.constants", "qualname": "TOXIGEN_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;Toxigen-roberta&#x27;"}, {"fullname": "src.fmeval.reporting.constants.DETOXIFY_NAME", "modulename": "src.fmeval.reporting.constants", "qualname": "DETOXIFY_NAME", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;UnitaryAI Detoxify-unbiased&#x27;"}, {"fullname": "src.fmeval.reporting.constants.TOXIGEN_URI", "modulename": "src.fmeval.reporting.constants", "qualname": "TOXIGEN_URI", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://github.com/microsoft/TOXIGEN&#x27;"}, {"fullname": "src.fmeval.reporting.constants.DETOXIFY_URI", "modulename": "src.fmeval.reporting.constants", "qualname": "DETOXIFY_URI", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;https://github.com/unitaryai/detoxify&#x27;"}, {"fullname": "src.fmeval.reporting.constants.TABLE_DESCRIPTION", "modulename": "src.fmeval.reporting.constants", "qualname": "TABLE_DESCRIPTION", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;Below are a few examples of the highest and lowest-scoring examples across all categories. Some text may be truncated due to length constraints. To view the full prompts, please go to the S3 job output location that you specified when configuring the job. &#x27;"}, {"fullname": "src.fmeval.reporting.constants.WER_TABLE_DESCRIPTION", "modulename": "src.fmeval.reporting.constants", "qualname": "WER_TABLE_DESCRIPTION", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;Below are a few examples of the highest and lowest-scoring examples across all categories. The lower the word error rate, the better the model performs. Some text may be truncated due to length constraints. To view the full prompts, please go to the S3 job output location that you specified when configuring the job.&#x27;"}, {"fullname": "src.fmeval.reporting.constants.STEREOTYPING_TABLE_DESCRIPTION", "modulename": "src.fmeval.reporting.constants", "qualname": "STEREOTYPING_TABLE_DESCRIPTION", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;**Probability ratio:** \\n\\nFor each sentence pair, we additionally report by how much the model stereotypes. This is given by the ratio &lt;math&gt;&lt;box&gt;p(S&lt;sub&gt;more&lt;/sub&gt;)/p(S&lt;sub&gt;less&lt;/sub&gt;)&lt;/box&gt;&lt;/math&gt;. This ratio will be between 0 and \u221e. A ratio of 200, for example, means that the model assigns 200x the probability to the more stereotypical sentence than the less stereotypical sentence.&#x27;"}, {"fullname": "src.fmeval.reporting.constants.FACTUAL_KNOWLEDGE_TABLE_DESCRIPTION", "modulename": "src.fmeval.reporting.constants", "qualname": "FACTUAL_KNOWLEDGE_TABLE_DESCRIPTION", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;Below are a few examples of correct and incorrect model responses. Some text may be truncated due to length constraints. To view the full prompts, please go to the S3 job output location that you specified when configuring the job. &#x27;"}, {"fullname": "src.fmeval.reporting.constants.SCORE_DESCRIPTIONS", "modulename": "src.fmeval.reporting.constants", "qualname": "SCORE_DESCRIPTIONS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;factual_knowledge&#x27;: &#x27;A factual knowledge score is a value between 0 and 1, indicating the percentage of correctly retrieved real-world facts across different knowledge categories.&#x27;, &#x27;prompt_stereotyping&#x27;: &#x27;The is_biased prompt stereotyping score measures whether the model assigns higher probability to the more stereotypical sentence than the less stereotypical one, i.e., &lt;math&gt;&lt;box&gt;p(S&lt;sub&gt;more&lt;/sub&gt;) &gt; p(S&lt;sub&gt;less&lt;/sub&gt;)&lt;/box&gt;&lt;/math&gt;. It is a value between 0 and 1, where 1 indicates that the model always prefers the more stereotypical sentence while 0 means that it never prefers the more stereotypical sentence. An unbiased model prefers more and less stereotypical sentences at equal rates, corresponding to a score of 0.5&#x27;, &#x27;log_probability_difference&#x27;: &#x27;For each sentence pair, we report the log probability difference, a value ranging -&amp;#8734; to &amp;#8734;, indicating how much the model stereotypes. &#x27;, &#x27;f1_score&#x27;: &#x27;Numerical score between 0 (worst) and 1 (best). F1-score is the harmonic mean of precision and recall. It is computed as follows:  precision = true positives / (true positives + false positives) and recall = true positives / (true positives + false negatives). Then F1 = 2 (precision * recall)/(precision + recall) .&#x27;, &#x27;exact_match_score&#x27;: &#x27;An exact match score is a binary score where 1 indicates the model output and answer match exactly and 0 indicates otherwise.&#x27;, &#x27;quasi_exact_match_score&#x27;: &#x27;Similar as above, but both model output and answer are normalised first by removing any articles and punctuation. E.g., 1 also for predicted answers \u201cAntarctica.\u201d or \u201cthe Antarctica\u201d .&#x27;, &#x27;rouge&#x27;: &#x27;A ROUGE-N score computes the N-gram (sequences of n words) word overlaps between the reference and model summary, with the value ranging between 0 (no match) to 1 (perfect match).&#x27;, &#x27;meteor&#x27;: &#x27;Meteor is similar to ROUGE-N, but it also accounts for rephrasing by using traditional NLP techniques such as stemming (e.g. matching \u201csinging\u201d to \u201csing\u201d,\u201csings\u201d etc.) and synonym lists.&#x27;, &#x27;bertscore&#x27;: &#x27;BERTScore uses a second ML model (from the BERT family) to compute sentence embeddings and compare their similarity.&#x27;, &#x27;classification_accuracy_score&#x27;: &#x27;The classification accuracy is `predicted_label == true_label`, reported as the mean accuracy over all datapoints.&#x27;, &#x27;precision_score&#x27;: &#x27;The precision score is computed as `true positives / (true positives + false positives)`. &#x27;, &#x27;recall_score&#x27;: &#x27;The recall score is computed as `true positives / (true positives + false negatives)`&#x27;, &#x27;balanced_accuracy_score&#x27;: &#x27;The balanced accuracy score is the same as accuracy in the binary case, otherwise averaged recall per class.&#x27;, &#x27;word_error_rate&#x27;: &#x27;Word error rate (WER) is a value between 0 and 1, and measures the difference between the model output on the unperturbed input and the output(s) on one or more perturbed versions of the same input. For more details on how word error rate is computed, see the [HuggingFace Article on Word Error Rate](https://huggingface.co/spaces/evaluate-metric/wer).&#x27;, &#x27;delta_rouge&#x27;: &#x27;The performance change of the ROUGE-N score is measured.&#x27;, &#x27;delta_meteor&#x27;: &#x27;The performance change of the METEOR score is measured.&#x27;, &#x27;delta_bertscore&#x27;: &#x27;The performance change of the BERTscore is measured.&#x27;, &#x27;delta_exact_match_score&#x27;: &#x27;The performance change of the Exact Match score is measured.&#x27;, &#x27;delta_quasi_exact_match_score&#x27;: &#x27;The performance change of the Quasi Exact Match score is measured.&#x27;, &#x27;delta_f1_score&#x27;: &#x27;The performance change of the F1 over Words score is measured.&#x27;, &#x27;delta_classification_accuracy_score&#x27;: &#x27;The score is the binary indicator on whether or not the model answer is correct.&#x27;, &#x27;toxicity&#x27;: &#x27;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **toxicity**&#x27;, &#x27;severe_toxicity&#x27;: &#x27;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **severe_toxicity**&#x27;, &#x27;obscene&#x27;: &#x27;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **obscene**&#x27;, &#x27;identity_attack&#x27;: &#x27;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class **identity_attack**&#x27;, &#x27;insult&#x27;: &#x27;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **insult**&#x27;, &#x27;threat&#x27;: &#x27;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **threat**&#x27;, &#x27;sexual_explicit&#x27;: &#x27;A binary score from 0 (no toxicity detected) to 1 (toxicity detected) for the class: **sexual_explicit**&#x27;}"}, {"fullname": "src.fmeval.reporting.constants.DatasetDetails", "modulename": "src.fmeval.reporting.constants", "qualname": "DatasetDetails", "kind": "class", "doc": "<p>DatasetDetails(name, url, description, size)</p>\n", "bases": "typing.NamedTuple"}, {"fullname": "src.fmeval.reporting.constants.DatasetDetails.__init__", "modulename": "src.fmeval.reporting.constants", "qualname": "DatasetDetails.__init__", "kind": "function", "doc": "<p>Create new instance of DatasetDetails(name, url, description, size)</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">url</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">description</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">size</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span>)</span>"}, {"fullname": "src.fmeval.reporting.constants.DatasetDetails.name", "modulename": "src.fmeval.reporting.constants", "qualname": "DatasetDetails.name", "kind": "variable", "doc": "<p>Alias for field number 0</p>\n", "annotation": ": str"}, {"fullname": "src.fmeval.reporting.constants.DatasetDetails.url", "modulename": "src.fmeval.reporting.constants", "qualname": "DatasetDetails.url", "kind": "variable", "doc": "<p>Alias for field number 1</p>\n", "annotation": ": str"}, {"fullname": "src.fmeval.reporting.constants.DatasetDetails.description", "modulename": "src.fmeval.reporting.constants", "qualname": "DatasetDetails.description", "kind": "variable", "doc": "<p>Alias for field number 2</p>\n", "annotation": ": str"}, {"fullname": "src.fmeval.reporting.constants.DatasetDetails.size", "modulename": "src.fmeval.reporting.constants", "qualname": "DatasetDetails.size", "kind": "variable", "doc": "<p>Alias for field number 3</p>\n", "annotation": ": int"}, {"fullname": "src.fmeval.reporting.constants.DATASET_DETAILS", "modulename": "src.fmeval.reporting.constants", "qualname": "DATASET_DETAILS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;trex&#x27;: DatasetDetails(name=&#x27;T-REx&#x27;, url=&#x27;https://hadyelsahar.github.io/t-rex/&#x27;, description=&#x27;A dataset which consists of knowledge triplets extracted from Wikipedia. The triplets take the form (subject, predicate, object), for instance, (Berlin, capital of, Germany) or (Tata Motors, subsidiary of, Tata Group). &#x27;, size=32260), &#x27;boolq&#x27;: DatasetDetails(name=&#x27;BoolQ&#x27;, url=&#x27;https://github.com/google-research-datasets/boolean-questions&#x27;, description=&#x27;A dataset consisting of question-passage-answer triplets. The question can be answered with yes/no, and the answer is contained in the passage. The questions are provided anonymously and unsolicited by users of the Google search engine, and afterwards paired with a paragraph from a Wikipedia article containing the answer.&#x27;, size=12697), &#x27;trivia_qa&#x27;: DatasetDetails(name=&#x27;TriviaQA&#x27;, url=&#x27;http://nlp.cs.washington.edu/triviaqa/&#x27;, description=&#x27;A dataset consisting of 95K question-answer pairs with with on average six supporting evidence documents per question, leading to ~650K question-passage-answer triplets. The questions are authored by trivia enthusiasts and the evidence documents are independently gathered. &#x27;, size=156328), &#x27;natural_questions&#x27;: DatasetDetails(name=&#x27;Natural Questions&#x27;, url=&#x27;https://github.com/google-research-datasets/natural-questions&#x27;, description=&#x27;A dataset consisting of ~320K question-passage-answer triplets. The questions are factual naturally-occurring questions. The passages are extracts from wikipedia articles (referred to as \u201clong answers\u201d in the original dataset). As before, providing the passage is optional depending on whether the open-book or closed-book case should be evaluated.&#x27;, size=4289), &#x27;crows-pairs&#x27;: DatasetDetails(name=&#x27;CrowS-Pairs&#x27;, url=&#x27;https://github.com/nyu-mll/crows-pairs&#x27;, description=&#x27;This dataset provides crowdsourced sentence pairs for the different categories along which stereotyping is to be measured.&#x27;, size=1508), &#x27;xsum&#x27;: DatasetDetails(name=&#x27;XSUM&#x27;, url=&#x27;https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset&#x27;, description=&#x27;A dataset consisting of newspaper articles from the BBC and their reference summaries. The reference summaries consist of a single sentence: the boldfaced sentence at the begininning of each BBC article, provided by article\u2019s authors.&#x27;, size=204045), &#x27;womens_clothing_ecommerce_reviews&#x27;: DatasetDetails(name=&quot;Women&#x27;s E-commerce Clothing Reviews&quot;, url=&#x27;https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews&#x27;, description=&#x27;This dataset consists of clothing reviews, both as a text and numerical scores.&#x27;, size=23486), &#x27;bold&#x27;: DatasetDetails(name=&#x27;BOLD&#x27;, url=&#x27;https://github.com/amazon-science/bold&#x27;, description=&#x27;A large-scale dataset that consists of English prompts aimed at testing bias and toxicity generation across five domains: profession, gender, race, religion, and political ideology.&#x27;, size=23679), &#x27;wikitext2&#x27;: DatasetDetails(name=&#x27;WikiText2&#x27;, url=&#x27;https://huggingface.co/datasets/wikitext&#x27;, description=&#x27;A dataset which consists of Good and Featured articles from Wikipedia. To create prompts, we broke each article down into sentences and extracted first 6 tokens from each sentence as the prompt.&#x27;, size=86007), &#x27;real_toxicity_prompts&#x27;: DatasetDetails(name=&#x27;Real Toxicity Prompts&#x27;, url=&#x27;https://github.com/allenai/real-toxicity-prompts&#x27;, description=&#x27;A dataset of truncated sentence snippets from the web. &#x27;, size=98243), &#x27;real_toxicity_prompts_challenging&#x27;: DatasetDetails(name=&#x27;Real Toxicity Prompts Challenging&#x27;, url=&#x27;https://github.com/allenai/real-toxicity-prompts&#x27;, description=&#x27;A dataset of truncated sentence snippets from the web. Prompts marked as \u201cchallenging\u201d have been found by the authors to consistently lead to generation of toxic continuation by tested models (i.e., GPT-1, GPT-2, GPT-3, CTRL, CTRL-WIKI).&#x27;, size=1199), &#x27;gigaword&#x27;: DatasetDetails(name=&#x27;Gigaword&#x27;, url=&#x27;https://huggingface.co/datasets/gigaword&#x27;, description=&#x27;A dataset with around 4 million news articles with their summaries. We use the \u201cvalidation set\u201d, which includes 190k entries.&#x27;, size=189651), &#x27;gov_report&#x27;: DatasetDetails(name=&#x27;Government Report&#x27;, url=&#x27;https://gov-report-data.github.io/&#x27;, description=&#x27;A dataset including a long-form summarization benchmark. It contains significantly longer documents (9.4k words) and summaries (553 words) than most existing datasets.&#x27;, size=7238)}"}, {"fullname": "src.fmeval.reporting.constants.TREX_DESCRIPTION_EXAMPLES", "modulename": "src.fmeval.reporting.constants", "qualname": "TREX_DESCRIPTION_EXAMPLES", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;We convert these predicates to prompts, e.g., Berlin is the capital of ___ (expected answer: Germany) and Tata Motors is a subsidiary of ___ (expected answer: Tata Group).&#x27;"}, {"fullname": "src.fmeval.reporting.constants.CROWS_PAIRS_DISCLAIMER", "modulename": "src.fmeval.reporting.constants", "qualname": "CROWS_PAIRS_DISCLAIMER", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;**Disclaimer**: 1) The crowdsourced CrowS dataset is noisy. While it gives a good indication of overall model performance, individual pairs may be invalid. 2) CrowS measures U.S.-typical stereotypes. Specifically, the bias categories are taken from the US Equal Employment Opportunities Commission\u2019s list of protected categories and the sentence pairs are produced by Amazon Mechanical Turk workers in the United States.&#x27;"}, {"fullname": "src.fmeval.reporting.eval_output_cells", "modulename": "src.fmeval.reporting.eval_output_cells", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.reporting.eval_output_cells.TABLE_COLUMNS", "modulename": "src.fmeval.reporting.eval_output_cells", "qualname": "TABLE_COLUMNS", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;sent_less_input&#x27;, &#x27;sent_more_input&#x27;, &#x27;model_input&#x27;, &#x27;sent_less_log_prob&#x27;, &#x27;sent_more_log_prob&#x27;, &#x27;target_output&#x27;, &#x27;model_output&#x27;, &#x27;category&#x27;, &#x27;insult&#x27;, &#x27;precision_score&#x27;, &#x27;delta_quasi_exact_match_score&#x27;, &#x27;bertscore&#x27;, &#x27;quasi_exact_match_score&#x27;, &#x27;delta_meteor&#x27;, &#x27;classification_accuracy_score&#x27;, &#x27;factual_knowledge&#x27;, &#x27;prompt_stereotyping&#x27;, &#x27;rouge&#x27;, &#x27;severe_toxicity&#x27;, &#x27;f1_score&#x27;, &#x27;delta_classification_accuracy_score&#x27;, &#x27;log_probability_difference&#x27;, &#x27;threat&#x27;, &#x27;identity_attack&#x27;, &#x27;balanced_accuracy_score&#x27;, &#x27;obscene&#x27;, &#x27;sexual_explicit&#x27;, &#x27;word_error_rate&#x27;, &#x27;delta_rouge&#x27;, &#x27;toxicity&#x27;, &#x27;exact_match_score&#x27;, &#x27;delta_exact_match_score&#x27;, &#x27;recall_score&#x27;, &#x27;meteor&#x27;, &#x27;delta_bertscore&#x27;, &#x27;delta_f1_score&#x27;, &#x27;&lt;math&gt;&lt;box&gt;p(S&lt;sub&gt;more&lt;/sub&gt;)/p(S&lt;sub&gt;less&lt;/sub&gt;)&lt;/box&gt;&lt;/math&gt;&#x27;, &#x27;is_biased&#x27;]"}, {"fullname": "src.fmeval.reporting.eval_output_cells.CategoryBarPlotCell", "modulename": "src.fmeval.reporting.eval_output_cells", "qualname": "CategoryBarPlotCell", "kind": "class", "doc": "<p>This class represents a bar plot that displays category-level and overall evaluation scores.</p>\n", "bases": "fmeval.reporting.cells.BarPlotCell"}, {"fullname": "src.fmeval.reporting.eval_output_cells.CategoryBarPlotCell.__init__", "modulename": "src.fmeval.reporting.eval_output_cells", "qualname": "CategoryBarPlotCell.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>categories</strong>:  The names of the categories.</li>\n<li><strong>scores</strong>:  The values of the category scores.</li>\n<li><strong>score_name</strong>:  The name of the score that was computed in the evaluation.</li>\n<li><strong>dataset_score</strong>:  The overall score for the dataset.</li>\n<li><strong>height</strong>:  Height of the plot as a string</li>\n<li><strong>width</strong>:  Width the plot as a string</li>\n<li><strong>center</strong>:  Boolean indicating if the plot should be center aligned in the page</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">categories</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">scores</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">score_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_score</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">height</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">width</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">center</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">origin</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span>)</span>"}, {"fullname": "src.fmeval.reporting.eval_output_cells.RayDatasetTableCell", "modulename": "src.fmeval.reporting.eval_output_cells", "qualname": "RayDatasetTableCell", "kind": "class", "doc": "<p>This class represents a table that displays data from a Ray Dataset object.</p>\n", "bases": "fmeval.reporting.cells.TableCell"}, {"fullname": "src.fmeval.reporting.eval_output_cells.RayDatasetTableCell.__init__", "modulename": "src.fmeval.reporting.eval_output_cells", "qualname": "RayDatasetTableCell.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>dataset</strong>:  The Ray Dataset that we create a TableCell out of</li>\n<li><strong>col_to_sort</strong>:  The name of the column in the dataset to sort by</li>\n<li><strong>k</strong>:  The number of samples from the dataset to display in the table</li>\n<li><strong>descending</strong>:  Whether to sort in descending order.</li>\n<li><strong>abs_val</strong>:  Whether to sort by absolute value when sorting is enabled.</li>\n<li><strong>caption</strong>:  The caption text before the table.</li>\n<li><strong>cell_align</strong>:  The text alignment within cells.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>,</span><span class=\"param\">\t<span class=\"n\">col_to_sort</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">k</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">descending</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">abs_val</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">caption</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">cell_align</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;left&#39;</span></span>)</span>"}, {"fullname": "src.fmeval.reporting.eval_output_cells.RayDatasetTableCell.truncate_samples", "modulename": "src.fmeval.reporting.eval_output_cells", "qualname": "RayDatasetTableCell.truncate_samples", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>samples</strong>:  List of items representing one row in the table.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Table row with strings longer than MAX_CHAR truncated.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">samples</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.reporting.eval_output_cells.CategoryScoreCell", "modulename": "src.fmeval.reporting.eval_output_cells", "qualname": "CategoryScoreCell", "kind": "class", "doc": "<p>This class displays a bar plot for the different category scores from an evaluation, and outlines the lowest\n    scoring category.</p>\n", "bases": "fmeval.reporting.cells.MarkdownCell"}, {"fullname": "src.fmeval.reporting.eval_output_cells.CategoryScoreCell.__init__", "modulename": "src.fmeval.reporting.eval_output_cells", "qualname": "CategoryScoreCell.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>categories</strong>:  The names of the categories.</li>\n<li><strong>scores</strong>:  The values of the category scores.</li>\n<li><strong>score_name</strong>:  The name of the score that was computed in the evaluation.</li>\n<li><strong>dataset_score</strong>:  The overall score for the dataset.</li>\n<li><strong>n</strong>:  Max number of categories to display.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">categories</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">scores</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">score_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_score</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">n</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">10</span></span>)</span>"}, {"fullname": "src.fmeval.reporting.eval_output_cells.ScoreTableCell", "modulename": "src.fmeval.reporting.eval_output_cells", "qualname": "ScoreTableCell", "kind": "class", "doc": "<p>This class generates two tables displaying the highest and lowest-scoring examples from a particular score.</p>\n", "bases": "fmeval.reporting.cells.MarkdownCell"}, {"fullname": "src.fmeval.reporting.eval_output_cells.ScoreTableCell.__init__", "modulename": "src.fmeval.reporting.eval_output_cells", "qualname": "ScoreTableCell.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>dataset</strong>:  The Ray Dataset used in the evaluation task.</li>\n<li><strong>score_column_name</strong>:  The name of the score column in the dataset.</li>\n<li><strong>binary</strong>:  Boolean indicating if the score is binary.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>,</span><span class=\"param\">\t<span class=\"n\">score_column_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">binary</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "src.fmeval.reporting.eval_output_cells.ScoreCell", "modulename": "src.fmeval.reporting.eval_output_cells", "qualname": "ScoreCell", "kind": "class", "doc": "<p>This class generates visualizations for an evaluation score, including the overall dataset score, a bar plot\n    displaying category-level scores if provided, and tables displaying highest and lowest scoring examples.</p>\n", "bases": "fmeval.reporting.cells.MarkdownCell"}, {"fullname": "src.fmeval.reporting.eval_output_cells.ScoreCell.__init__", "modulename": "src.fmeval.reporting.eval_output_cells", "qualname": "ScoreCell.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>dataset</strong>:  The Ray Dataset used in the evaluation task.</li>\n<li><strong>score_name</strong>:  The name of the score that was computed in the evaluation.</li>\n<li><strong>score_column_name</strong>:  The name of the score column in the dataset.</li>\n<li><strong>dataset_score</strong>:  The aggregated score computed across the whole dataset.</li>\n<li><strong>categories</strong>:  The names of the categories.</li>\n<li><strong>category_scores</strong>:  The values of the category scores.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">score_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">score_column_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_score</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">categories</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">category_scores</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span></span>)</span>"}, {"fullname": "src.fmeval.reporting.eval_output_cells.EvalOutputCell", "modulename": "src.fmeval.reporting.eval_output_cells", "qualname": "EvalOutputCell", "kind": "class", "doc": "<p>Base class representing a markdown cell.</p>\n", "bases": "fmeval.reporting.cells.MarkdownCell"}, {"fullname": "src.fmeval.reporting.eval_output_cells.EvalOutputCell.__init__", "modulename": "src.fmeval.reporting.eval_output_cells", "qualname": "EvalOutputCell.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>eval_output</strong>:  A EvalOutput object from an evaluation.</li>\n<li><strong>dataset</strong>:  The Ray dataset containing the evaluation scores.</li>\n<li><strong>score_column_names</strong>:  A dict mapping the score names and score column names for the evaluation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">eval_output</span><span class=\"p\">:</span> <span class=\"n\">fmeval</span><span class=\"o\">.</span><span class=\"n\">eval_algorithms</span><span class=\"o\">.</span><span class=\"n\">EvalOutput</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">score_column_names</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "src.fmeval.reporting.eval_output_cells.EvalOutputCell.get_dataset_sampling_description", "modulename": "src.fmeval.reporting.eval_output_cells", "qualname": "EvalOutputCell.get_dataset_sampling_description", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>dataset_name</strong>:  The name of the Ray dataset.</li>\n<li><strong>dataset</strong>:  The Ray dataset containing the evaluation scores.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>String describing the number of samples used in the evaluation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">dataset_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.reporting.eval_output_cells.EvalOutputCell.get_dataset_description", "modulename": "src.fmeval.reporting.eval_output_cells", "qualname": "EvalOutputCell.get_dataset_description", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>dataset_name</strong>:  The name of the Ray dataset.</li>\n<li><strong>dataset_type</strong>:  Whether the dataset is a built-in or custom dataset.</li>\n<li><strong>dataset</strong>:  The Ray dataset containing the evaluation scores.</li>\n<li><strong>eval_name</strong>:  The name of the selected evaluation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>The description of the dataset, including the number of samples used in the evaluation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">eval_name</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.reporting.util", "modulename": "src.fmeval.reporting.util", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.reporting.util.format_string", "modulename": "src.fmeval.reporting.util", "qualname": "format_string", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>text</strong>:  text, name of the score or eval.</li>\n<li><strong>remove_underscore</strong>:  Boolean indicating if underscores should be replaced with spaces.</li>\n<li><strong>as_title</strong>:  Boolean indicating if the text is a title, if set to True will capitalize each word.</li>\n<li><strong>as_score</strong>:  Boolean indicating if \"score\" should be appended to the text.</li>\n<li><strong>as_plot_title</strong>:  Boolean indicating if this is a plot title.</li>\n<li><strong>as_eval_name</strong>:  Boolean indicating if this is the name of an evaluation.</li>\n<li><strong>as_column_name</strong>:  Boolean indicating if this is the name of a table column.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>formatted score name.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">remove_underscore</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">as_title</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">as_score</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">as_plot_title</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">as_eval_name</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">as_column_name</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.reporting.util.format_dataset_name", "modulename": "src.fmeval.reporting.util", "qualname": "format_dataset_name", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>dataset_name</strong>:  The name of the dataset.</li>\n<li><strong>hyperlink</strong>:  Boolean indicating if hyperlink should be added to dataset name.</li>\n<li><strong>html</strong>:  Boolean indicating if hyperlink should be added in HTML format.</li>\n<li><strong>color</strong>:  The color of the text.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Properly capitalized dataset name.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">hyperlink</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">html</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">color</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;#006DAA&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.reporting.util.add_hyperlink", "modulename": "src.fmeval.reporting.util", "qualname": "add_hyperlink", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>text</strong>:  The text to add the hyperlink to.</li>\n<li><strong>link</strong>:  The URL to link to the text.</li>\n<li><strong>html</strong>:  Boolean indicating if hyperlink should be added in HTML format.</li>\n<li><strong>color</strong>:  The color of the text.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">link</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">html</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">color</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;#006DAA&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.util", "modulename": "src.fmeval.util", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.fmeval.util.require", "modulename": "src.fmeval.util", "qualname": "require", "kind": "function", "doc": "<p>Raise EvalAlgorithmClientError if expression is not True</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">expression</span>, </span><span class=\"param\"><span class=\"n\">msg</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.util.assert_condition", "modulename": "src.fmeval.util", "qualname": "assert_condition", "kind": "function", "doc": "<p>Raise EvalAlgorithmInternalError if expression is not True</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">expression</span>, </span><span class=\"param\"><span class=\"n\">msg</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.util.project_root", "modulename": "src.fmeval.util", "qualname": "project_root", "kind": "function", "doc": "<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>project root</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">current_file</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.util.camel_to_snake", "modulename": "src.fmeval.util", "qualname": "camel_to_snake", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">name</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.util.get_eval_results_path", "modulename": "src.fmeval.util", "qualname": "get_eval_results_path", "kind": "function", "doc": "<p>Util method to return results path for eval_algos. This method looks for EVAL_RESULTS_PATH environment variable,\nif present returns that else default path\n:returns: Local directory path of eval algo results</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.util.singleton", "modulename": "src.fmeval.util", "qualname": "singleton", "kind": "function", "doc": "<p>Decorator to make a class Singleton</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.fmeval.util.get_num_actors", "modulename": "src.fmeval.util", "qualname": "get_num_actors", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();