import logging
from dataclasses import dataclass
from typing import Dict, Any, Optional, List, Union, Tuple
from ray import ObjectRef

from fmeval.eval_algorithms import EvalScore, EvalOutput, EvalAlgorithm
from fmeval.eval_algorithms.eval_algorithm import EvalAlgorithmInterface, EvalAlgorithmConfig
from fmeval.eval_algorithms import util
from fmeval.util import assert_condition, require, create_shared_resource, get_eval_results_path
from fmeval.constants import BERTSCORE_DEFAULT_MODEL, DatasetColumns
from fmeval.transforms.transform_pipeline import TransformPipeline
from fmeval.data_loaders.data_config import DataConfig
from fmeval.helper_models import BertscoreModelTypes, BertscoreModel
from fmeval.model_runners.model_runner import ModelRunner
from fmeval.transforms.summarization_accuracy_metrics import (
    MeteorScore,
    RougeScore,
    BertScore,
    METEOR_SCORE,
    ROUGE_SCORE,
    BERT_SCORE,
    ROUGE_2,
    ROUGE_TYPES,
)


METRIC_NAMES = [METEOR_SCORE, ROUGE_SCORE, BERT_SCORE]

logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class SummarizationAccuracyConfig(EvalAlgorithmConfig):
    """Configures the summarization accuracy evaluation algorithm.

    :param rouge_type: ROUGE metric type.
    :param use_stemmer_for_rouge: Whether to use stemmer when computing ROUGE metric.
    :param model_type_for_bertscore: BERT model type to use for computing BERT score.
    """

    rouge_type: str = ROUGE_2
    use_stemmer_for_rouge: bool = True
    model_type_for_bertscore: str = BERTSCORE_DEFAULT_MODEL

    def __post_init__(self):
        require(
            self.rouge_type in ROUGE_TYPES,
            f"Invalid rouge_type: {self.rouge_type} requested in SummarizationAccuracyConfig. "
            f"Please choose from acceptable values: {ROUGE_TYPES}.",
        )
        require(
            BertscoreModelTypes.model_is_allowed(self.model_type_for_bertscore),
            f"Invalid model_type_for_bertscore: {self.model_type_for_bertscore} requested in "
            f"SummarizationAccuracyConfig. Please choose from acceptable values: "
            f"{BertscoreModelTypes.model_list()}.",
        )


class SummarizationAccuracy(EvalAlgorithmInterface):
    """Summarization Accuracy evaluation algorithm.

    The aim of this evaluation algorithm is to evaluate how well a model can summarize text.
    The algorithm uses a reference summary to compare the output generated by the model and a series
    of quality metrics based on overlapping between words (ROUGE and METEOR) and similarity scores (BERTScore).
    """

    eval_name = EvalAlgorithm.SUMMARIZATION_ACCURACY.value

    def __init__(self, config: SummarizationAccuracyConfig = SummarizationAccuracyConfig(), use_ray: bool = True):
        """SummarizationAccuracy initializer.

        :param config: Summarization Accuracy evaluation algorithm config.
        :param use_ray: Whether to create a Ray actor for the BertscoreModel used by this evaluation
            algorithm instance. Currently, `evaluate` will only work if `use_ray` is set to True,
            as the execution of the transform pipeline relies on the BertscoreModel existing
            in shared memory. This flag can be set to False if you only plan on invoking the
            `evaluate_sample` method, which is a computationally cheap operation that does not
            require utilizing Ray for parallel execution.
        """
        self.use_ray = use_ray
        meteor_score, rouge_score, bert_score, self.bertscore_model = SummarizationAccuracy.build_pipeline(
            target_output_keys=[DatasetColumns.TARGET_OUTPUT.value.name],
            model_output_keys=[DatasetColumns.MODEL_OUTPUT.value.name],
            meteor_keys=[METEOR_SCORE],
            rouge_keys=[ROUGE_SCORE],
            bertscore_keys=[BERT_SCORE],
            rouge_type=config.rouge_type,
            use_stemmer_for_rouge=config.use_stemmer_for_rouge,
            model_type_for_bertscore=config.model_type_for_bertscore,
            use_ray=use_ray,
        )
        self.pipeline = TransformPipeline([meteor_score, rouge_score, bert_score])

    @staticmethod
    def build_pipeline(
        target_output_keys: List[str],
        model_output_keys: List[str],
        meteor_keys: List[str],
        rouge_keys: List[str],
        bertscore_keys: List[str],
        rouge_type: str,
        use_stemmer_for_rouge: bool,
        bertscore_model: Optional[Union[BertscoreModel, ObjectRef]] = None,
        model_type_for_bertscore: Optional[str] = None,
        use_ray: bool = True,
    ) -> Tuple[MeteorScore, RougeScore, BertScore, Union[BertscoreModel, ObjectRef]]:
        meteor_transform = MeteorScore(
            target_output_keys=target_output_keys,
            model_output_keys=model_output_keys,
            output_keys=meteor_keys,
            allow_duplicate_input_keys=True,
        )
        rouge_transform = RougeScore(
            target_output_keys=target_output_keys,
            model_output_keys=model_output_keys,
            output_keys=rouge_keys,
            allow_duplicate_input_keys=True,
            rouge_type=rouge_type,
            use_stemmer=use_stemmer_for_rouge,
        )
        if bertscore_model is None:  # pragma: no branch
            require(
                model_type_for_bertscore is not None,
                "model_type_for_bertscore must not be None when bertscore_model is not provided.",
            )
            bertscore_model = BertscoreModel(model_type_for_bertscore)
            if use_ray:  # pragma: no branch
                bertscore_model = create_shared_resource(bertscore_model)
        bert_transform = BertScore(
            target_output_keys=target_output_keys,
            model_output_keys=model_output_keys,
            output_keys=bertscore_keys,
            allow_duplicate_input_keys=True,
            bertscore_model=bertscore_model,
        )
        return meteor_transform, rouge_transform, bert_transform, bertscore_model

    @staticmethod
    def create_sample(target_output: str, model_output: str) -> Dict[str, Any]:
        """Create a sample in the record format used by Transforms.

        This function's primary use is to be called by evaluate_sample.

        :param target_output: The target_output parameter passed to evaluate_sample.
        :param model_output: The model_output parameter passed to evaluate_sample.
        """
        return {
            DatasetColumns.TARGET_OUTPUT.value.name: target_output,
            DatasetColumns.MODEL_OUTPUT.value.name: model_output,
        }

    def evaluate_sample(self, target_output: str, model_output: str) -> List[EvalScore]:
        """Compute summarization accuracy metrics for a single sample.

        :param target_output: The expected/desired model output.
        :param model_output: The actual model output.
        :returns: A list of EvalScore objects, one for each of the summarization accuracy metrics.
        """
        sample = SummarizationAccuracy.create_sample(target_output=target_output, model_output=model_output)
        output_record = self.pipeline.execute_record(sample)
        assert_condition(
            all(metric_name in output_record for metric_name in METRIC_NAMES),
            "Summarization Accuracy evaluate_sample has computed an output that is missing at least one metric. "
            f"The output record is {output_record}.",
        )
        return [EvalScore(name=metric_name, value=output_record[metric_name]) for metric_name in METRIC_NAMES]

    def evaluate(
        self,
        model: Optional[ModelRunner] = None,
        dataset_config: Optional[DataConfig] = None,
        prompt_template: Optional[str] = None,
        num_records: int = 100,
        save: bool = False,
    ) -> List[EvalOutput]:
        """Compute summarization accuracy metrics on one or more datasets.

        :param model: An instance of ModelRunner representing the model under evaluation.
        :param dataset_config: Configures the single dataset used for evaluation.
            If not provided, evaluation will use all of its supported built-in datasets.
        :param prompt_template: A template used to generate prompts that are fed to the model.
            If not provided, defaults will be used.
        :param num_records: The number of records to be sampled randomly from the input dataset
            used to perform the evaluation.
        :param save: If set to true, prompt responses and scores will be saved to a file.
            The path that this file is stored at is configured by `eval_results_path`.

        :return: A list of EvalOutput objects.
        """
        require(
            self.use_ray,
            "The use_ray instance attribute of SummarizationAccuracy must be True in order "
            "for the evaluate method to run successfully.",
        )
        return util.run_evaluation(
            eval_name=self.eval_name,
            pipeline=self.pipeline,
            metric_names=METRIC_NAMES,
            required_columns=[DatasetColumns.TARGET_OUTPUT.value.name, DatasetColumns.MODEL_INPUT.value.name],
            eval_results_path=get_eval_results_path(),
            model=model,
            dataset_config=dataset_config,
            prompt_template=prompt_template,
            num_records=num_records,
            save=save,
        )
