import logging
from dataclasses import dataclass
from typing import Dict, Any, Optional, List

from fmeval.perf_util import timed_block
from fmeval.transforms.common import GeneratePrompt, GetModelResponse
from fmeval.util import require
from fmeval.constants import BERTSCORE_DEFAULT_MODEL, DatasetColumns, MEAN
from fmeval.data_loaders.data_config import DataConfig
from fmeval.data_loaders.util import get_dataset
from fmeval.eval_algorithms import EvalAlgorithm, EvalScore, EvalOutput, get_default_prompt_template
from fmeval.eval_algorithms.eval_algorithm import EvalAlgorithmConfig, EvalAlgorithmInterface
from fmeval.eval_algorithms.util import (
    validate_dataset,
    aggregate_evaluation_scores,
    generate_output_dataset_path,
    save_dataset,
    get_dataset_configs,
)
from fmeval.exceptions import EvalAlgorithmClientError
from fmeval.helper_models import BertscoreModelTypes, BertscoreModel
from fmeval.model_runners.model_runner import ModelRunner
from fmeval.transforms.summarization_accuracy_metrics import (
    MeteorScore,
    RougeScore,
    BertScore,
    METEOR_SCORE,
    ROUGE_SCORE,
    BERT_SCORE,
    ROUGE_2,
    ROUGE_TYPES,
)

from fmeval.transforms.transform_pipeline import TransformPipeline
from fmeval.util import create_shared_resource, assert_condition

METRIC_NAMES = [METEOR_SCORE, ROUGE_SCORE, BERT_SCORE]

logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class SummarizationAccuracyConfig(EvalAlgorithmConfig):
    """Configures the summarization accuracy evaluation algorithm.

    :param rouge_type: ROUGE metric type.
    :param use_stemmer_for_rouge: Whether to use stemmer when computing ROUGE metric.
    :param model_type_for_bertscore: BERT model type to use for computing BERT score.
    """

    rouge_type: str = ROUGE_2
    use_stemmer_for_rouge: bool = True
    model_type_for_bertscore: str = BERTSCORE_DEFAULT_MODEL

    def __post_init__(self):
        if self.rouge_type not in ROUGE_TYPES:
            raise EvalAlgorithmClientError(
                f"Invalid rouge_type: {self.rouge_type} requested in SummarizationAccuracyConfig. "
                f"Please choose from acceptable values: {ROUGE_TYPES}."
            )

        if not BertscoreModelTypes.model_is_allowed(self.model_type_for_bertscore):
            raise EvalAlgorithmClientError(
                f"Invalid model_type_for_bertscore: {self.model_type_for_bertscore} requested in "
                f"SummarizationAccuracyConfig. Please choose from acceptable values: "
                f"{BertscoreModelTypes.model_list()}."
            )


class SummarizationAccuracy(EvalAlgorithmInterface):
    """Summarization Accuracy evaluation algorithm.

    The aim of this evaluation algorithm is to evaluate how well a model can summarize text.
    The algorithm uses a reference summary to compare the output generated by the model and a series
    of quality metrics based on overlapping between words (ROUGE and METEOR) and similarity scores (BERTScore).
    """

    eval_name = EvalAlgorithm.SUMMARIZATION_ACCURACY.value

    def __init__(
        self, eval_algorithm_config: SummarizationAccuracyConfig = SummarizationAccuracyConfig(), use_ray: bool = True
    ):
        """SummarizationAccuracy initializer.

        :param eval_algorithm_config: Summarization Accuracy evaluation algorithm config.
        :param use_ray: Whether to use the Ray distributed computing framework to run the evaluation
            algorithm logic. While using Ray will typically speed up the evaluation, setting this flag
            to False can be useful for debugging purposes or in situations where computational resources
            are limited.
        """
        super().__init__(eval_algorithm_config)
        meteor_transform = MeteorScore(
            output_key=METEOR_SCORE,
            target_output_key=DatasetColumns.TARGET_OUTPUT.value.name,
            model_output_key=DatasetColumns.MODEL_OUTPUT.value.name,
        )
        rouge_transform = RougeScore(
            output_key=ROUGE_SCORE,
            target_output_key=DatasetColumns.TARGET_OUTPUT.value.name,
            model_output_key=DatasetColumns.MODEL_OUTPUT.value.name,
            rouge_type=eval_algorithm_config.rouge_type,
            use_stemmer=eval_algorithm_config.use_stemmer_for_rouge,
        )
        bertscore_model = BertscoreModel(eval_algorithm_config.model_type_for_bertscore)
        if use_ray:
            bertscore_model = create_shared_resource(bertscore_model)
        bert_transform = BertScore(
            output_key=BERT_SCORE,
            target_output_key=DatasetColumns.TARGET_OUTPUT.value.name,
            model_output_key=DatasetColumns.MODEL_OUTPUT.value.name,
            bertscore_model=bertscore_model,
        )
        self.pipeline = TransformPipeline([meteor_transform, rouge_transform, bert_transform])

    @staticmethod
    def create_sample(target_output: str, model_output: str) -> Dict[str, Any]:
        """Create a sample in the record format used by Transforms.

        This function's primary use is to be called by evaluate_sample.

        :param target_output: The target_output parameter passed to evaluate_sample.
        :param model_output: The model_output parameter passed to evaluate_sample.
        """
        return {
            DatasetColumns.TARGET_OUTPUT.value.name: target_output,
            DatasetColumns.MODEL_OUTPUT.value.name: model_output,
        }

    def evaluate_sample(self, target_output: str, model_output: str) -> List[EvalScore]:  # type: ignore[override]
        """Compute summarization accuracy metrics for a single sample.

        :param target_output: The expected/desired model output.
        :param model_output: The actual model output.
        :returns: A list of EvalScore objects, one for each of the summarization accuracy metrics.
        """
        sample = SummarizationAccuracy.create_sample(target_output=target_output, model_output=model_output)
        output_record = self.pipeline.execute_record(sample)
        assert_condition(
            all(metric_name in output_record for metric_name in METRIC_NAMES),
            "Summarization Accuracy evaluate_sample has computed an output that is missing at least one metric. "
            f"The output record is {output_record}.",
        )
        return [EvalScore(name=metric_name, value=output_record[metric_name]) for metric_name in METRIC_NAMES]

    def evaluate(
        self,
        model: Optional[ModelRunner] = None,
        dataset_config: Optional[DataConfig] = None,
        prompt_template: Optional[str] = None,
        save: bool = False,
        num_records=100,
    ) -> List[EvalOutput]:
        """
        Summarization Accuracy evaluate

        :param model: An instance of ModelRunner which is the model under evaluation
        :param dataset_config: Configures the single dataset used for evaluation. If not provided,
            evaluation will use all of it's supported built-in datasets
        :param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults
            will be used.
        :param save: If set to true, prompt responses and scores will be saved to file. The output is written to
                     EvalAlgorithmInterface.EVAL_RESULTS_PATH
        :param num_records: The number of records to be sampled randomly from the input dataset to perform the
                            evaluation

        :return: List of EvalOutput objects.
        """
        dataset_configs = get_dataset_configs(dataset_config, self.eval_name)
        eval_outputs = []
        for dataset_config in dataset_configs:
            dataset = get_dataset(dataset_config, num_records)
            validate_dataset(dataset, [DatasetColumns.TARGET_OUTPUT.value.name, DatasetColumns.MODEL_INPUT.value.name])
            dataset_prompt_template = None
            pipeline = self.pipeline

            if DatasetColumns.MODEL_OUTPUT.value.name not in dataset.columns():
                require(model, "No ModelRunner provided. ModelRunner is required for inference on model_inputs")
                dataset_prompt_template = (
                    get_default_prompt_template(dataset_config.dataset_name) if not prompt_template else prompt_template
                )
                gen_prompt = GeneratePrompt(
                    input_keys=[DatasetColumns.MODEL_INPUT.value.name],
                    output_keys=[DatasetColumns.PROMPT.value.name],
                    prompt_template=dataset_prompt_template,
                )
                get_model_response = GetModelResponse(
                    input_keys=gen_prompt.output_keys,
                    output_keys=[DatasetColumns.MODEL_OUTPUT.value.name],
                    model_runner=model,
                )
                pipeline = TransformPipeline([gen_prompt, get_model_response, pipeline])

            with timed_block(f"Computing score and aggregation on dataset {dataset_config.dataset_name}", logger):
                dataset = pipeline.execute(dataset)
                dataset_scores, category_scores = aggregate_evaluation_scores(dataset, METRIC_NAMES, agg_method=MEAN)
                eval_outputs.append(
                    EvalOutput(
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                        prompt_template=dataset_prompt_template,
                        dataset_scores=dataset_scores,
                        category_scores=category_scores,
                        output_path=generate_output_dataset_path(
                            path_to_parent_dir=self._eval_results_path,
                            eval_name=self.eval_name,
                            dataset_name=dataset_config.dataset_name,
                        ),
                    )
                )
            if save:
                save_dataset(
                    dataset=dataset,
                    score_names=METRIC_NAMES,
                    path=generate_output_dataset_path(
                        path_to_parent_dir=self._eval_results_path,
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                    ),
                )

        return eval_outputs
