import logging

import ray
import nltk
import evaluate as hf_evaluate

from dataclasses import dataclass
from typing import Optional, List, Callable, Dict, Any
from ray.data import Dataset
from nltk import word_tokenize
from nltk.translate import meteor_score

import fmeval.util as util
from fmeval.constants import (
    TARGET_OUTPUT_COLUMN_NAME,
    MODEL_INPUT_COLUMN_NAME,
    MODEL_OUTPUT_COLUMN_NAME,
    MEAN,
)
from fmeval.data_loaders.util import DataConfig, get_dataset
from fmeval.eval_algorithms import (
    EvalAlgorithm,
    EvalScore,
    EvalOutput,
    EVAL_DATASETS,
    DATASET_CONFIGS,
    get_default_prompt_template,
)
from fmeval.eval_algorithms.eval_algorithm import (
    EvalAlgorithmConfig,
    EvalAlgorithmInterface,
)
from fmeval.eval_algorithms.helper_models.helper_model import BertscoreHelperModel
from fmeval.eval_algorithms.util import (
    generate_prompt_column_for_dataset,
    generate_model_predict_response_for_dataset,
    validate_dataset,
    aggregate_evaluation_scores,
    generate_output_dataset_path,
    save_dataset,
)
from fmeval.exceptions import EvalAlgorithmClientError
from fmeval.model_runners.model_runner import ModelRunner
from fmeval.perf_util import timed_block

PROMPT_COLUMN_NAME = "prompt"
METEOR_SCORE = "meteor"
ROUGE_SCORE = "rouge"
BERT_SCORE = "bertscore"

# rouge constants
ROUGE_1 = "rouge1"
ROUGE_2 = "rouge2"
ROUGE_L = "rougeL"

ROUGE_TYPES = [ROUGE_1, ROUGE_2, ROUGE_L]

# bertscore constants
MICROSOFT_DEBERTA_MODEL = "microsoft/deberta-xlarge-mnli"
ROBERTA_MODEL = "roberta-large-mnli"
DEFAULT_MODEL_TYPE = MICROSOFT_DEBERTA_MODEL
MODEL_TYPES_SUPPORTED = [MICROSOFT_DEBERTA_MODEL, ROBERTA_MODEL]

logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class SummarizationAccuracyConfig(EvalAlgorithmConfig):
    """
    Configuration for the summarization accuracy eval algorithm

    :param rouge_type: Type of rouge metric in eval results
    :param use_stemmer_for_rouge: bool value to set using stemmer for rouge metric
    :param model_type_for_bertscore: model to use for bert score
    """

    rouge_type: str = ROUGE_2
    use_stemmer_for_rouge: bool = True
    model_type_for_bertscore: str = DEFAULT_MODEL_TYPE

    def __post_init__(self):
        if not self.rouge_type in ROUGE_TYPES:
            raise EvalAlgorithmClientError(
                f"Invalid rouge_type: {self.rouge_type} requested in SummarizationAccuracyConfig, "
                f"please choose from acceptable values: {ROUGE_TYPES}"
            )

        if not self.model_type_for_bertscore in MODEL_TYPES_SUPPORTED:
            raise EvalAlgorithmClientError(
                f"Invalid model_type_for_bertscore: {self.model_type_for_bertscore} requested in "
                f"SummarizationAccuracyConfig, please choose from acceptable values: {MODEL_TYPES_SUPPORTED}"
            )


class SummarizationAccuracy(EvalAlgorithmInterface):
    """
    Summarization Accuracy Eval algorithm

    The aim of this eval algo is to evaluate how well a model can summarize text.
    The algo uses a reference summary to compare the output generated by the model and a series
    of quality metrics based on overlapping between words (ROUGE and METEOR) and similarity scores (BERTScore).
    """

    def __init__(self, eval_algorithm_config: SummarizationAccuracyConfig = SummarizationAccuracyConfig()):
        """Default constructor

        :param eval_algorithm_config: Summarization Accuracy eval algorithm config.
        """
        super().__init__(eval_algorithm_config)
        self.eval_name = EvalAlgorithm.SUMMARIZATION_ACCURACY.value
        self._eval_algorithm_config = eval_algorithm_config
        self._load_eval_helpers()
        self._score_eval_func_mapping = {
            METEOR_SCORE: get_meteor_score,
            ROUGE_SCORE: get_rouge_score,
            BERT_SCORE: get_bert_score,
        }

    def _load_eval_helpers(self):
        """
        Method to download required helpers for eval_algo in constructor call
        """
        # load helper modules for meteor
        nltk.download("wordnet")
        nltk.download("punkt")
        nltk.download("omw-1.4")

        # Initialize the shared BertscoreHelperModel actor that will be shared
        # by every get_bert_score task.
        self._bertscore_helper_model = BertscoreHelperModel.remote(
            model_type=self._eval_algorithm_config.model_type_for_bertscore
        )

    def evaluate_sample(self, target_output: str, model_output: str) -> List[EvalScore]:  # type: ignore[override]
        """
        Summarization Accuracy evaluate sample.

        :param target_output: The expected responses from the model
        :param model_output: The output of a model that we want to evaluate.
        :return: list of EvalScore objects
        """
        if target_output is None:
            raise EvalAlgorithmClientError(
                "Missing required input: target_output, for Summarization Accuracy evaluate_sample"
            )
        if model_output is None:
            raise EvalAlgorithmClientError(
                "Missing required input: model_output, for Summarization Accuracy evaluate_sample"
            )

        return [
            EvalScore(
                name=eval_score,
                value=eval_fn(
                    target_output=target_output,
                    model_output=model_output,
                    config=self._eval_algorithm_config,
                    helper_model=self._bertscore_helper_model,  # only used by get_bert_score
                ),
            )
            for eval_score, eval_fn in self._score_eval_func_mapping.items()
        ]

    def evaluate(
        self,
        model: Optional[ModelRunner] = None,
        dataset_config: Optional[DataConfig] = None,
        prompt_template: Optional[str] = None,
        save: bool = False,
        num_records=100,
    ) -> List[EvalOutput]:
        """
        Summarization Accuracy evaluate

        :param model: An instance of ModelRunner which is the model under evaluation
        :param dataset_config: Configures the single dataset used for evaluation. If not provided,
            evaluation will use all of it's supported built-in datasets
        :param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults
            will be used.
        :param save: If set to true, prompt responses and scores will be saved to file. The output is written to
                     EvalAlgorithmInterface.EVAL_RESULTS_PATH
        :param num_records: The number of records to be sampled randomly from the input dataset to perform the
                            evaluation

        :return: List of EvalOutput objects.
        """
        if dataset_config:
            dataset_configs = [dataset_config]
        else:
            dataset_configs = [DATASET_CONFIGS[dataset_name] for dataset_name in EVAL_DATASETS[self.eval_name]]

        eval_outputs = []
        for dataset_config in dataset_configs:
            dataset = get_dataset(dataset_config, num_records)
            validate_dataset(dataset, [TARGET_OUTPUT_COLUMN_NAME, MODEL_INPUT_COLUMN_NAME])
            dataset_prompt_template = None
            if MODEL_OUTPUT_COLUMN_NAME not in dataset.columns():
                util.require(model, "No ModelRunner provided. ModelRunner is required for inference on model_inputs")
                dataset_prompt_template = (
                    get_default_prompt_template(dataset_config.dataset_name) if not prompt_template else prompt_template
                )
                dataset = generate_prompt_column_for_dataset(
                    dataset_prompt_template, dataset, MODEL_INPUT_COLUMN_NAME, PROMPT_COLUMN_NAME
                )
                assert model  # to satisfy mypy
                dataset = generate_model_predict_response_for_dataset(
                    model, dataset, PROMPT_COLUMN_NAME, MODEL_OUTPUT_COLUMN_NAME
                )
            with timed_block(f"Computing score and aggregation on dataset {dataset_config.dataset_name}", logger):
                dataset = add_score_to_dataset(
                    dataset=dataset,
                    score_name_to_func=self._score_eval_func_mapping,
                    config=self._eval_algorithm_config,
                    helper_model=self._bertscore_helper_model,
                )
                dataset_scores, category_scores = aggregate_evaluation_scores(
                    dataset, [METEOR_SCORE, ROUGE_SCORE, BERT_SCORE], agg_method=MEAN
                )
                eval_outputs.append(
                    EvalOutput(
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                        prompt_template=dataset_prompt_template,
                        dataset_scores=dataset_scores,
                        category_scores=category_scores,
                        output_path=generate_output_dataset_path(
                            path_to_parent_dir=self._eval_results_path,
                            eval_name=self.eval_name,
                            dataset_name=dataset_config.dataset_name,
                        ),
                    )
                )
            if save:
                save_dataset(
                    dataset=dataset,
                    score_names=[METEOR_SCORE, ROUGE_SCORE, BERT_SCORE],
                    path=generate_output_dataset_path(
                        path_to_parent_dir=self._eval_results_path,
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                    ),
                )

        return eval_outputs


def get_meteor_score(target_output: str, model_output: str, config: SummarizationAccuracyConfig, **kwargs) -> float:
    """
    METEOR is a metric for text similarity between the machine-produced summary and human-produced reference summaries.
    Unigrams can be matched based on their surface forms, stemmed forms,
    and meanings; furthermore, METEOR can be easily extended to include more
    advanced matching strategies. Once all generalized unigram matches
    between the two strings have been found, METEOR computes a score for
    this matching using a combination of unigram-precision, unigram-recall, and
    a measure of fragmentation that is designed to directly capture how
    well-ordered the matched words in the machine translation are in relation
    to the reference.

    :param target_output: The expected responses from the model
    :param model_output: The output of a model that we want to evaluate.
    :param config: Eval algo config
    :returns: meteor score
    """
    return meteor_score.single_meteor_score(
        reference=word_tokenize(target_output), hypothesis=word_tokenize(model_output)
    )


def get_rouge_score(target_output: str, model_output: str, config: SummarizationAccuracyConfig, **kwargs) -> float:
    """
    The ROUGE-N, where N=[1,2,L], score is a standard metric for summarization quality.
    It computes the word overlap between the reference and model summary. Given that this metric is based on simple
    word overlap statistics, it works best for extractive summaries.
    Note that if we rephrase the summary without changing its meaning the ROUGE-N score will drop.

    Reference: https://huggingface.co/spaces/evaluate-metric/rouge

    :param target_output: The expected responses from the model
    :param model_output: The output of a model that we want to evaluate.
    :param config: Eval algo config
    :returns: rouge score
    """
    rouge = hf_evaluate.load("rouge")
    return rouge.compute(
        predictions=[model_output],
        references=[target_output],
        use_stemmer=config.use_stemmer_for_rouge,
        rouge_types=[config.rouge_type],
    )[config.rouge_type]


def get_bert_score(target_output: str, model_output: str, config: SummarizationAccuracyConfig, **kwargs) -> float:
    """
    BERTscore is a similarity-based metric that compares the embedding of the prediction and target sentences
    under a learned model, typically, from the BERT family.
    This score may lead to increased flexibility compared to ROUGE and METEOR in terms of rephrasing since
    semantically similar sentences are (typically) embedded similarly.

    https://huggingface.co/spaces/evaluate-metric/bertscore

    :param target_output: The expected responses from the model
    :param model_output: The output of a model that we want to evaluate.
    :param config: Eval algo config
    :param helper_model: The BertscoreHelperModel belonging to an instance of SummarizationAccuracy.
    :returns: bert score
    """
    assert "helper_model" in kwargs
    helper_model = kwargs["helper_model"]
    return ray.get(helper_model.get_helper_scores.remote(target_output, model_output))


def add_score_to_dataset(
    dataset: Dataset,
    score_name_to_func: Dict[str, Callable],
    config: SummarizationAccuracyConfig,
    helper_model: BertscoreHelperModel,
):
    """
    Util method to add a score column to a ray dataset.

    :param dataset: ray Dataset to be used for eval score generation
    :param score_name_to_func: maps column names for scores to be added to the functions used to compute those scores
    :param config: Eval algo config
    :param helper_model: The BertscoreHelperModel belonging to an
        instance of SummarizationAccuracy. Used only by get_bert_score.
    :returns: ray Dataset with score column
    """

    def _generate_eval_scores(row: Dict[str, Any]) -> Dict[str, Any]:  # pragma: no cover
        """
        Map function generating the scores for every input record in input dataset
        """
        for score_name in score_name_to_func:
            eval_func = score_name_to_func[score_name]
            row[score_name] = eval_func(
                row[TARGET_OUTPUT_COLUMN_NAME], row[MODEL_OUTPUT_COLUMN_NAME], config, helper_model=helper_model
            )
        return row

    return dataset.map(_generate_eval_scores).materialize()
