from abc import ABC, abstractmethod
from typing import List
from fmeval.eval_algorithms import EvalScore, EvalOutput


class EvalAlgorithmConfig:
    """Configuration class to be used or extended to provide evaluation algorithm-specific parameters."""


class EvalAlgorithmInterface(ABC):
    """Interface for evaluation algorithms.

    This interface defines two required methods that all evaluation algorithms must implement.
    The signatures of these methods is intentionally as generic as possible, to allow for
    maximum freedom when implementing a new evaluation algorithm.
    """

    @abstractmethod
    def evaluate_sample(self, *args, **kwargs) -> List[EvalScore]:
        """Compute metrics for a single sample, where a sample is defined by the particular algorithm.

        The arguments to this method should be any data that pertains to the sample to be evaluated,
        and any additional data used to compute the relevant metrics/scores.

        Example:
            The evaluate_sample method of the FactualKnowledge evaluation algorithm takes
            two arguments: `target_output` and `model_output`, which are used to compute the factual
            knowledge score.

        :returns: A list of EvalScore objects, where each EvalScore represents a single
            score/metric that is computed by the evaluation algorithm.
            See the built-in evaluation algorithms (ex: FactualKnowledge, SummarizationAccuracy)
            for concrete examples.
        """

    @abstractmethod
    def evaluate(self, *args, **kwargs) -> List[EvalOutput]:
        """Compute metrics on all samples in one or more datasets.

        The format that the dataset(s) in question take is up to the implementer
        of the evaluation algorithm. All built-in evaluation algorithms in fmeval
        currently utilize Ray Datasets. See the built-in evaluation algorithms
        (ex: FactualKnowledge, SummarizationAccuracy) for concrete examples of how
        to implement the `evaluate` method.

        :returns: A list of EvalOutput objects, where an EvalOutput encapsulates
        the EvalScores (and optionally, CategoryScores) generated by the evaluation,
        as well as additional metadata regarding the evaluation.
        """
