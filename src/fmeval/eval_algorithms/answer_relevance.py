import logging
from typing import Optional, List, Dict, Any, Union

import numpy as np

from fmeval.constants import (
    DatasetColumns,
    MEAN,
)
from fmeval.data_loaders.util import get_dataset
from fmeval.data_loaders.data_config import DataConfig
from fmeval.eval_algorithms.save_strategy import SaveStrategy
from fmeval.eval_algorithms.util import get_dataset_configs
from fmeval.eval_algorithms.common import evaluate_dataset
from fmeval.eval_algorithms.eval_algorithm import EvalAlgorithmInterface, EvalAlgorithmConfig
from fmeval.eval_algorithms import (
    EvalAlgorithm,
    EvalOutput,
    EvalScore,
)
from fmeval.eval_algorithms.util import validate_dataset
from fmeval.model_runners.model_runner import ModelRunner
from fmeval.transforms.common import GetModelOutputs, GeneratePrompt
from fmeval.transforms.transform import Transform
from fmeval.transforms.transform_pipeline import TransformPipeline
from fmeval.transforms.util import validate_call
from fmeval.util import get_eval_results_path

logger = logging.getLogger(__name__)

ANSWER_RELEVANCE = EvalAlgorithm.ANSWER_RELEVANCE.value

QUESTION_GENERATION_PROMPT = "question_generation_prompt"
ANSWER = "answer"
RAW_GEN_QUESTION = "raw_gen_question"

QUESTION_GEN_PROMPT = """\
Human: Generate question for the given answer.
Answer:\nThe PSLV-C56 mission is scheduled to be launched on Sunday, 30 July 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space Centre, Sriharikota, Andhra Pradesh, India
Question: When is the scheduled launch date and time for the PSLV-C56 mission, and where will it be launched from?

Answer:$answer
Question: Assistant:"""  # noqa: E501


class AnswerRelevanceScore(Transform):
    """This transform augments its input record with the computed answer relevance score.

    See the docstring for `Answer Relevance` for more details regarding the score itself.
    """

    def __init__(
        self,
        embeddings_model: ModelRunner,
        model_output_key: str = DatasetColumns.MODEL_OUTPUT.value.name,
        output_key: str = ANSWER_RELEVANCE,
    ):
        """AnswerRelevanceScore initializer.

        :param embeddings_model: An instance of ModelRunner representing the embedding model to be used.
        :param model_output_key: The key corresponding to the model output.
        :param output_key: The key corresponding to the answer relevance score that will be added to the input record.
        """
        super().__init__(embeddings_model, model_output_key, output_key)
        self.register_input_output_keys(
            input_keys=[RAW_GEN_QUESTION, model_output_key],
            output_keys=[output_key],
        )
        self.output_key = output_key
        self.model_output_key = model_output_key
        self.embeddings_model = embeddings_model

    @validate_call
    def __call__(self, record: Dict[str, Any]) -> Dict[str, Any]:
        """Augment the input record with the computed answer relevance score.

        :param record: The input record.
        :returns: The input record, with the answer relevance score added in.
        """
        gen_questions = record[RAW_GEN_QUESTION]
        question = record[self.model_output_key]
        record[self.output_key] = self._get_score(self.embeddings_model, question, gen_questions)
        return record

    @staticmethod
    def _get_score(embeddings_model: ModelRunner, question: str, generated_question: str) -> float:
        """Given generated question and if it can be inferred from the given contexts, computer Answer Relevance score.

        :param embeddings_model: An instance of ModelRunner representing the embedding model to be used.
        :param question: The given prompt (model input).
        :param generated_question: An artificial question generated by judge model based on the answer.
        :returns: 0 to 1. See the docstring for `Answer Relevance` for more details
            on what these numerical values represent.
        """
        question_vector = np.asarray(embeddings_model.predict(question))
        gen_question_vector = np.asarray(embeddings_model.predict(generated_question))
        norm = np.linalg.norm(gen_question_vector) * np.linalg.norm(question_vector)
        return np.dot(gen_question_vector, question_vector.T) / norm


class AnswerRelevance(EvalAlgorithmInterface):
    """
    The evaluation metric, Answer Relevancy, focuses on assessing how pertinent the generated answer is to the
    given prompt. It is defined as the mean cosine similarity of the original question to a number of artifical
    questions, which where generated (reverse engineered) based on the answer.
    A lower score is assigned to answers that are incomplete or contain redundant information and higher scores
    indicate better relevancy.
    """

    eval_name = EvalAlgorithm.ANSWER_RELEVANCE.value

    def __init__(self):
        """AnswerRelevance initializer."""
        super().__init__(EvalAlgorithmConfig())

    @staticmethod
    def _build_pipeline(
        judge_model: ModelRunner,
        embeddings_model: ModelRunner,
        question_generation_prompt_template: str,
    ) -> TransformPipeline:
        gen_question_generation_prompt = GeneratePrompt(
            input_keys=[],
            output_keys=[QUESTION_GENERATION_PROMPT],
            prompt_template=question_generation_prompt_template,
            placeholder_to_record_key={
                ANSWER: DatasetColumns.MODEL_OUTPUT.value.name,
            },
        )
        gen_raw_questions = GetModelOutputs(
            input_to_output_keys={QUESTION_GENERATION_PROMPT: [RAW_GEN_QUESTION]},
            model_runner=judge_model,
        )
        compute_score = AnswerRelevanceScore(embeddings_model=embeddings_model)
        transforms = [gen_question_generation_prompt, gen_raw_questions, compute_score]
        pipeline = TransformPipeline(transforms)
        return pipeline

    def evaluate_sample(
        self,
        model_input: str,
        model_output: str,
        target_context: str,
        judge_model: ModelRunner,
        embeddings_model: ModelRunner,
        question_generation_prompt_template: str = QUESTION_GEN_PROMPT,
    ) -> List[EvalScore]:  # type: ignore[override]
        """Compute the Answer Relevance score on a single sample.

        :param model_input: The expected responses from the model.
        :param model_output: The output of the model being evaluated.
        :param target_context: The relevant context retrieved from RAG system.
        :param judge_model: An instance of ModelRunner representing the judge model to be used.
        :param embeddings_model: An instance of ModelRunner representing the embedding model to be used.
        :param question_generation_prompt_template: The template used to construct the prompt to inference
            judge model to get generate question based on answer.
        :return: A single-element list containing an EvalScore corresponding to the answer relevance score.
        """
        sample = {
            DatasetColumns.MODEL_INPUT.value.name: model_input,
            DatasetColumns.MODEL_OUTPUT.value.name: model_output,
            DatasetColumns.TARGET_CONTEXT.value.name: target_context,
        }
        pipeline = self._build_pipeline(
            judge_model=judge_model,
            question_generation_prompt_template=question_generation_prompt_template,
            embeddings_model=embeddings_model,
        )
        result = pipeline.execute_record(sample)
        return [EvalScore(name=ANSWER_RELEVANCE, value=result[ANSWER_RELEVANCE])]

    def evaluate(
        self,
        judge_model: ModelRunner,
        embeddings_model: ModelRunner,
        dataset_config: Optional[Union[DataConfig, List[DataConfig]]] = None,
        num_records: int = 300,
        save: bool = False,
        save_strategy: Optional[SaveStrategy] = None,
        question_generation_prompt_template: str = QUESTION_GEN_PROMPT,
    ) -> List[EvalOutput]:
        """Compute the answer relevance score on one or more datasets.

        :param judge_model: An instance of ModelRunner representing the judge model to be used.
        :param embeddings_model: An instance of ModelRunner representing the embedding model to be used.
        :param dataset_config: Configures a single dataset or list of datasets used for the
            evaluation. If not provided, this method will run evaluations using all of its
            supported built-in datasets.
        :param num_records: The number of records to be sampled randomly from the input dataset(s)
            used to perform the evaluation(s). Note that the default value is 300, rather than
            100, as it is for the rest of the built-in algorithms.
        :param save: If set to true, prompt responses and scores will be saved to a file.
        :param save_strategy: Specifies the strategy to use the save the localized outputs of the evaluations. If not
            specified, it will save it to the path that can be configured by the EVAL_RESULTS_PATH environment variable.
            If that environment variable is also not configured, it will be saved to the default path `/tmp/eval_results/`.
        :param question_generation_prompt_template: The template used to construct the prompt to inference judge model
            to get generate question based on answer.

        :return: A list of EvalOutput objects.
        """
        dataset_configs = get_dataset_configs(dataset_config, self.eval_name)
        eval_outputs = []
        for dataset_config in dataset_configs:
            dataset = get_dataset(dataset_config, num_records)
            validate_dataset(
                dataset,
                [
                    DatasetColumns.MODEL_INPUT.value.name,
                    DatasetColumns.MODEL_OUTPUT.value.name,
                    DatasetColumns.TARGET_CONTEXT.value.name,
                ],
            )
            eval_output = evaluate_dataset(
                dataset=dataset,
                pipeline=self._build_pipeline(judge_model, embeddings_model, question_generation_prompt_template),
                dataset_name=dataset_config.dataset_name,
                eval_name=self.eval_name,
                metric_names=[ANSWER_RELEVANCE],
                eval_results_path=get_eval_results_path(),
                agg_method=MEAN,
                save=save,
                save_strategy=save_strategy,
            )
            eval_outputs.append(eval_output)
        return eval_outputs
