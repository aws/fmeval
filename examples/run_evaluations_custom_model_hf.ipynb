{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T04:46:20.494534Z",
     "start_time": "2023-10-11T04:46:15.734465Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/kvasist/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from transformers import GenerationConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from amazon_fmeval.model_runners.model_runner import ModelRunner\n",
    "from amazon_fmeval.eval_algo_mapping import get_eval_algorithm\n",
    "from amazon_fmeval.eval_algorithms.factual_knowledge import FactualKnowledgeConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T04:46:24.909098Z",
     "start_time": "2023-10-11T04:46:20.506421Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7842dd7870fe497c978df465857b4f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458c8885611e4d57be8be4062bf80d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bda56399c1414e91823776150e73c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60b9835ae7f4eca88985b85e579cf60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0250efcf814531b57fad77933b02f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118dd70264864e84911ab3f75091eb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kvasist/opt/miniconda3/envs/amazon-fmeval/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n\\nThe answer is yes.\\n\\nThe city is the capital', -5.30952787399292)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class HFModelConfig:\n",
    "\tmodel_name: str\n",
    "\tgeneration_config: GenerationConfig\n",
    "\tnormalize_probabilities: bool = False\n",
    "\tseed: int = 0\n",
    "\tremove_prompt_from_generated_text: bool = True\n",
    "\n",
    "\n",
    "class HuggingFaceCausalLLMModelRunner(ModelRunner):\n",
    "\n",
    "\tdef __init__(self, model_config: HFModelConfig):\n",
    "\t\tself.config = model_config\n",
    "\t\tself.model = AutoModelForCausalLM.from_pretrained(self.config.model_name)\n",
    "\t\tself.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
    "\n",
    "\tdef predict(self, prompt: str) -> Tuple[Optional[str], Optional[float]]:\n",
    "\t\tinput_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\t\tgenerations = self.model.generate(\n",
    "\t\t\t**input_ids,\n",
    "\t\t\tmax_new_tokens=self.config.generation_config.max_new_tokens,\n",
    "\t\t\tpad_token_id=self.tokenizer.eos_token_id,\n",
    "\t\t\tgeneration_config=self.config.generation_config,\n",
    "\t\t)\n",
    "\t\tgeneration_contains_input = (\n",
    "\t\t\tinput_ids[\"input_ids\"][0] == generations[0][: input_ids[\"input_ids\"].shape[1]]\n",
    "\t\t).all()\n",
    "\t\tif self.config.remove_prompt_from_generated_text and not generation_contains_input:\n",
    "\t\t\twarnings.warn(\n",
    "\t\t\t\"Your model does not return the prompt as part of its generations. \"\n",
    "\t\t\t\"`remove_prompt_from_generated_text` does nothing.\"\n",
    "\t\t\t)\n",
    "\t\tif self.config.remove_prompt_from_generated_text and generation_contains_input:\n",
    "\t\t\toutput = self.tokenizer.batch_decode(generations[:, input_ids[\"input_ids\"].shape[1]:])[0]\n",
    "\t\telse:\n",
    "\t\t\toutput = self.tokenizer.batch_decode(generations, skip_special_tokens=True)[0]\n",
    "\n",
    "\t\twith torch.inference_mode():\n",
    "\t\t\tinput_ids = self.tokenizer(self.tokenizer.bos_token + prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\t\t\tmodel_output = self.model(input_ids, labels=input_ids)\n",
    "\t\t\tprobability = -model_output[0].item()\n",
    "\n",
    "\t\treturn output, probability\n",
    "\n",
    "\n",
    "# Test with gpt2\n",
    "generation_config = GenerationConfig()\n",
    "model_config = {\"model_name\": \"gpt2\", \"generation_config\": generation_config}\n",
    "hf_config = HFModelConfig(**model_config)\n",
    "model = HuggingFaceCausalLLMModelRunner(model_config=hf_config)\n",
    "print(model.predict(\"London is the capital of?\"))\n",
    "\n",
    "# Test with facebook/bart-large-cnn\n",
    "# generation_config = GenerationConfig(\n",
    "#         max_new_tokens=40,\n",
    "#         do_sample=True,\n",
    "#         top_k=50,\n",
    "#         top_p=0.9,\n",
    "#     )\n",
    "# hf_config = HFModelConfig(model_name=\"facebook/bart-large-cnn\", generation_config=generation_config)\n",
    "# model = HuggingFaceCausalLLMModelRunner(model_config=hf_config)\n",
    "# print(model.predict(\n",
    "#     \"Summarize the following article in 2 sentences: The art metropolis of Berlin inspires locals and visitors with \"\n",
    "#     \"its famous museum landscape and numerous UNESCO World Heritage sites. It is also an international exhibition \"\n",
    "#     \"venue. You will find a selection of current and upcoming exhibitions here.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T04:46:24.909600Z",
     "start_time": "2023-10-11T04:46:24.905207Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate factual_knowledge\n",
    "eval_algorithm_config = FactualKnowledgeConfig(\"<OR>\")\n",
    "eval_algo = get_eval_algorithm(\"factual_knowledge\")(eval_algorithm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T04:46:25.339914Z",
     "start_time": "2023-10-11T04:46:24.910825Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The answer is yes.\n",
      "\n",
      "The city is the capital\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[EvalScore(name='factual_knowledge', value=0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate your custom sample\n",
    "model_output = model.predict(\"London is the capital of?\")[0]\n",
    "print(model_output)\n",
    "eval_algo.evaluate_sample(target_output=\"UK<OR>England<OR>United Kingdom\", model_output=model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-11T05:05:48.111561Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 04:13:08,095\tINFO worker.py:1621 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 04:13:09,283\tINFO read_api.py:374 -- To satisfy the requested parallelism of 20, each read task output will be split into 20 smaller blocks.\n",
      "2023-10-11 04:13:09,913\tINFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadCustomJSON->SplitBlocks(20)] -> ActorPoolMapOperator[MapBatches(process_batch)->Map(ModelRunnerWrapper)]\n",
      "2023-10-11 04:13:09,914\tINFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-10-11 04:13:09,914\tINFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(_MapWorker pid=11286)\u001B[0m sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "\u001B[2m\u001B[36m(_MapWorker pid=11286)\u001B[0m sagemaker.config INFO - Not applying SDK defaults from location: /Users/kvasist/Library/Application Support/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(raylet)\u001B[0m Spilled 2440 MiB, 7 objects, write throughput 1856 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(_MapWorker pid=11297)\u001B[0m sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\u001B[32m [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001B[0m\n",
      "\u001B[2m\u001B[36m(_MapWorker pid=11297)\u001B[0m sagemaker.config INFO - Not applying SDK defaults from location: /Users/kvasist/Library/Application Support/sagemaker/config.yaml\u001B[32m [repeated 8x across cluster]\u001B[0m\n",
      "\u001B[2m\u001B[36m(_MapWorker pid=11299)\u001B[0m sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\u001B[32m [repeated 4x across cluster]\u001B[0m\n",
      "\u001B[2m\u001B[36m(_MapWorker pid=11299)\u001B[0m sagemaker.config INFO - Not applying SDK defaults from location: /Users/kvasist/Library/Application Support/sagemaker/config.yaml\u001B[32m [repeated 4x across cluster]\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 04:13:25,657\tINFO actor_pool_map_operator.py:117 -- MapBatches(process_batch)->Map(ModelRunnerWrapper): Waiting for 9 pool actors to start...\n",
      "\u001B[2m\u001B[36m(raylet)\u001B[0m Spilled 4392 MiB, 11 objects, write throughput 1788 MiB/s.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(MapWorker(MapBatches(process_batch)->Map(ModelRunnerWrapper)) pid=11286)\u001B[0m /Users/kvasist/opt/miniconda3/envs/amazon-fmeval/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "\u001B[2m\u001B[36m(MapWorker(MapBatches(process_batch)->Map(ModelRunnerWrapper)) pid=11286)\u001B[0m   warnings.warn(\n",
      "\u001B[2m\u001B[36m(MapWorker(MapBatches(process_batch)->Map(ModelRunnerWrapper)) pid=11294)\u001B[0m /Users/kvasist/opt/miniconda3/envs/amazon-fmeval/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\u001B[32m [repeated 9x across cluster]\u001B[0m\n",
      "\u001B[2m\u001B[36m(MapWorker(MapBatches(process_batch)->Map(ModelRunnerWrapper)) pid=11294)\u001B[0m   warnings.warn(\u001B[32m [repeated 9x across cluster]\u001B[0m\n",
      "\u001B[2m\u001B[36m(MapWorker(MapBatches(process_batch)->Map(ModelRunnerWrapper)) pid=11287)\u001B[0m /Users/kvasist/opt/miniconda3/envs/amazon-fmeval/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\u001B[32m [repeated 9x across cluster]\u001B[0m\n",
      "\u001B[2m\u001B[36m(MapWorker(MapBatches(process_batch)->Map(ModelRunnerWrapper)) pid=11287)\u001B[0m   warnings.warn(\u001B[32m [repeated 9x across cluster]\u001B[0m\n",
      "2023-10-11 04:14:00,148\tINFO dataset.py:2180 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2023-10-11 04:14:00,150\tINFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(process_batch)] -> AllToAllOperator[Aggregate]\n",
      "2023-10-11 04:14:00,154\tINFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-10-11 04:14:00,154\tINFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Aggregate 1:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 2:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 3:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(MapBatches(process_batch) pid=11276)\u001B[0m sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\u001B[32m [repeated 6x across cluster]\u001B[0m\n",
      "\u001B[2m\u001B[36m(MapBatches(process_batch) pid=11276)\u001B[0m sagemaker.config INFO - Not applying SDK defaults from location: /Users/kvasist/Library/Application Support/sagemaker/config.yaml\u001B[32m [repeated 6x across cluster]\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 04:14:02,329\tINFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(process_batch)] -> LimitOperator[limit=1]\n",
      "2023-10-11 04:14:02,330\tINFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-10-11 04:14:02,330\tINFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 04:14:02,376\tINFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(process_batch)] -> AllToAllOperator[Aggregate] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n",
      "2023-10-11 04:14:02,376\tINFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-10-11 04:14:02,376\tINFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Aggregate 1:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 2:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 3:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 0:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 04:14:02,498\tINFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(process_batch)] -> AllToAllOperator[Aggregate]\n",
      "2023-10-11 04:14:02,498\tINFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-10-11 04:14:02,499\tINFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Aggregate 1:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 2:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 3:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 0:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 04:14:02,606\tINFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(process_batch)->Map(<lambda>)]\n",
      "2023-10-11 04:14:02,606\tINFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-10-11 04:14:02,607\tINFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 04:14:02,657\tINFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(process_batch)->Map(<lambda>)]\n",
      "2023-10-11 04:14:02,657\tINFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-10-11 04:14:02,657\tINFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom dataset\n",
    "from amazon_fmeval.data_loaders.data_config import DataConfig\n",
    "dataset_config = DataConfig(\n",
    "        dataset_name=\"TREX\",\n",
    "        dataset_uri=\"trex_sample.jsonl\",\n",
    "        dataset_mime_type=\"application/jsonlines\",\n",
    "        model_input_location=\"question\",\n",
    "        target_output_location=\"answers\",\n",
    "        category_location=\"knowledge_category\",\n",
    "    )\n",
    "\n",
    "# Evaluate model with amazon-fmeval built-in dataset\n",
    "eval_outputs = eval_algo.evaluate(model=model, dataset_config=dataset_config, prompt_template=\"$feature\", save=True)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T04:53:43.837901Z",
     "start_time": "2023-10-11T04:53:43.830508Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EvalOutput(eval_name='factual_knowledge', dataset_name='TREX', dataset_scores=[EvalScore(name='factual_knowledge', value=0.05472636815920398)], prompt_template='$feature', category_scores=[CategoryScore(name='Capitals', scores=[EvalScore(name='factual_knowledge', value=0.09)]), CategoryScore(name='Subsidiary', scores=[EvalScore(name='factual_knowledge', value=0.019801980198019802)])], output_path='/tmp/eval_results/')]\n"
     ]
    }
   ],
   "source": [
    "# Show Evaluation outputs\n",
    "print(eval_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T04:53:43.854060Z",
     "start_time": "2023-10-11T04:53:43.836902Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model_input\": \"Yamoussoukro is the capital of\", \"model_output\": \" the Krasnodar region of Russia. It\", \"model_log_probability\": -5.57382869720459, \"target_output\": \"Ivory Coast<OR>C\\u00f4te d'Ivoire\", \"category\": \"Capitals\", \"prompt\": \"Yamoussoukro is the capital of\", \"scores\": [{\"name\": \"factual_knowledge\", \"value\": 0}]}\n",
      "\n",
      "{\"model_input\": \"Digne-les-Bains is the capital of\", \"model_output\": \" France, and the capital of the French Republic\", \"model_log_probability\": -4.66910982131958, \"target_output\": \"Alpes-de-Haute-Provence\", \"category\": \"Capitals\", \"prompt\": \"Digne-les-Bains is the capital of\", \"scores\": [{\"name\": \"factual_knowledge\", \"value\": 0}]}\n",
      "\n",
      "{\"model_input\": \"Khandyga is the capital of\", \"model_output\": \" the state of Uttar Pradesh.\\n\\nThe state's population\", \"model_log_probability\": -5.045193672180176, \"target_output\": \"Tomponsky District\", \"category\": \"Capitals\", \"prompt\": \"Khandyga is the capital of\", \"scores\": [{\"name\": \"factual_knowledge\", \"value\": 0}]}\n",
      "\n",
      "{\"model_input\": \"Rouen is the capital of\", \"model_output\": \" the French Republic, and is located in the heart of the French\", \"model_log_probability\": -4.278097629547119, \"target_output\": \"Normandy<OR>Normandie<OR>Norman\", \"category\": \"Capitals\", \"prompt\": \"Rouen is the capital of\", \"scores\": [{\"name\": \"factual_knowledge\", \"value\": 0}]}\n",
      "\n",
      "{\"model_input\": \"Elista is the capital of\", \"model_output\": \" the Republic of the Republic of the Republic of the Republic of the\", \"model_log_probability\": -4.983731269836426, \"target_output\": \"Kalmykia<OR>Kalmyk\", \"category\": \"Capitals\", \"prompt\": \"Elista is the capital of\", \"scores\": [{\"name\": \"factual_knowledge\", \"value\": 0}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show first five rows of saved output\n",
    "with open('/tmp/eval_results/factual_knowledge_TREX.jsonl') as f:\n",
    "\tlines = [next(f) for _ in range(5)]\n",
    "for line in lines:\n",
    "\tprint(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
