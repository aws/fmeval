{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "import os\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary: This notebooks demonstrates how to compare multiple models by plotting their evaluation results in a radar plot.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare models\n",
    "\n",
    "We choose two models available on SageMaker JumpStart: \"huggingface-llm-falcon-7b-bf16\" and \"huggingface-llm-falcon-7b-instruct-bf16\". The two models have the same architecture, but the latter one has been additionally trained with instruction finetuning. We'll evaluate both on the QA task and see whether the additional training makes a difference. We start by defining some helper functions to deploy the models to JumpStart endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to test the endpoint: \n",
    "# 1) we test that the endpoint exists and \n",
    "# 2) that we are extracting the response correcly (i.e., the `output_format` is as expected). \n",
    "# We return the output format for use in the ModelRunner later. \n",
    "def test_endpoint(predictor):\n",
    "    prompt = \"London is the capital of\"\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\": 0.9,\n",
    "            \"temperature\": 0.8,\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"decoder_input_details\" : True,\n",
    "            \"details\" : True\n",
    "        },\n",
    "    }\n",
    "    response = predictor.predict(payload)\n",
    "    print(f'Query successful. \\n\\nExample: Prompt: {prompt} ... Model response: {response[0][\"generated_text\"]}')\n",
    "    output_format ='[0].generated_text' \n",
    "    return output_format \n",
    "\n",
    "# function to get existing endpoint for a model or deploy a new one if none exists \n",
    "def get_endpoint(model_id, model_version, endpoint_name=\"\"):\n",
    "    print(\"Using existing endpoint.\")\n",
    "    predictor = sagemaker.predictor.Predictor(\n",
    "        endpoint_name=endpoint_name,\n",
    "        serializer=sagemaker.serializers.JSONSerializer(),\n",
    "        deserializer = sagemaker.deserializers.JSONDeserializer()\n",
    "    )\n",
    "    try:\n",
    "        output_format = test_endpoint(predictor)\n",
    "    except: \n",
    "        print(\"No working endpoint found. Deploying a new one.\")\n",
    "        my_model = JumpStartModel(model_id=model_id, model_version=model_version)\n",
    "        predictor = my_model.deploy()\n",
    "        endpoint_name = predictor.endpoint_name\n",
    "        output_format = test_endpoint(predictor)\n",
    "    return endpoint_name, predictor, output_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id_base, model_version_base, endpoint_name_base = \"huggingface-llm-falcon-7b-bf16\" , \"*\", \"hf-llm-falcon-7b-bf16-2024-03-21-12-51-01-854\"\n",
    "endpoint_name_base, predictor_base, output_format_base = get_endpoint(model_id_base, model_version_base, endpoint_name_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id_instruct, model_version_instruct, endpoint_name_instruct = \"huggingface-llm-falcon-7b-instruct-bf16\" , \"*\", \"hf-llm-falcon-7b-instruct-bf16-2024-03-21-10-15-06-733\"\n",
    "endpoint_name_instruct, predictor_instruct, output_format_instruct = get_endpoint(model_id_instruct, model_version_instruct, endpoint_name=endpoint_name_instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run the evaluation\n",
    "\n",
    "Next, we run the QA Accuracy evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fmeval.eval_algorithms.qa_accuracy import QAAccuracy, QAAccuracyConfig\n",
    "from fmeval.model_runners.sm_jumpstart_model_runner import JumpStartModelRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_runner_base = JumpStartModelRunner(\n",
    "    endpoint_name=endpoint_name_base,\n",
    "    model_id=model_id_base,\n",
    "    model_version=model_version_base,\n",
    "    output=output_format_base, # you can test whether this is correct using the \n",
    "    content_template='{\"inputs\": $prompt, \"parameters\": {\"do_sample\": true, \"top_p\": 0.9, \"temperature\": 0.8, \"max_new_tokens\": 1024, \"decoder_input_details\": true,\"details\": true}}',\n",
    ")\n",
    "\n",
    "model_runner_instruct = JumpStartModelRunner(\n",
    "    endpoint_name=endpoint_name_instruct,\n",
    "    model_id=model_id_base,\n",
    "    model_version=model_version_instruct,\n",
    "    output=output_format_instruct, # you can test whether this is correct using the \n",
    "    content_template='{\"inputs\": $prompt, \"parameters\": {\"do_sample\": true, \"top_p\": 0.9, \"temperature\": 0.8, \"max_new_tokens\": 1024, \"decoder_input_details\": true,\"details\": true}}',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to configure and run evaluation\n",
    "def run_eval(model, model_name):\n",
    "    # configure eval (use default)\n",
    "    default_config = QAAccuracyConfig()\n",
    "    qa_eval = QAAccuracy(default_config)\n",
    "    \n",
    "    # configure filepath\n",
    "    results_path = f\"example_results/{model_name}.json\"\n",
    "    \n",
    "    # load results from file if the eval has already been run\n",
    "    if os.path.exists(results_path):\n",
    "        with open(results_path, 'r') as f:\n",
    "            results = json.load(f)\n",
    "            print(f'Results loaded from {results_path}')\n",
    "            \n",
    "    # otherwise run the eval and save the results to a file        \n",
    "    else:\n",
    "        results = qa_eval.evaluate(model = model, save=True, num_records=5)\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(results, f, default=lambda c: c.__dict__)\n",
    "            print(f'Results saved to {results_path}')\n",
    "    return results                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have precomputed some evaluations so this notebook can be executed more quickly. If the precomputed files don't exist (e.g., because you are using other models), the evaluation is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_qa_base = run_eval(model_runner_base, model_id_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_qa_instruct = run_eval(model_runner_instruct, model_id_instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the results and visualize them as radar plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages needed for plotting\n",
    "! pip install -U kaleido\n",
    "! pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for loading the results\n",
    "def load_results(models):\n",
    "    accuracy_results = []\n",
    "    for model in models:\n",
    "        file = f'example_results/{model}.json'\n",
    "        with open(file, 'r') as f:\n",
    "            res = json.load(f)\n",
    "            for accuracy_eval in res['accuracy']:\n",
    "                for accuracy_scores in accuracy_eval[\"dataset_scores\"]:\n",
    "                    accuracy_results.append(\n",
    "                        {'model': model, 'evaluation': 'accuracy', 'dataset': accuracy_eval[\"dataset_name\"],\n",
    "                         'metric': accuracy_scores[\"name\"], 'value': accuracy_scores[\"value\"]})\n",
    "        \n",
    "    accuracy_results_df = pd.DataFrame(accuracy_results)\n",
    "    return accuracy_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for plotting the results\n",
    "def visualize_radar(results_df, dataset):\n",
    "    # aggregate 3 datasets into 1 by taking mean across datasets\n",
    "    if dataset == 'all':\n",
    "       mean_across_datasets = results_df.drop('evaluation', axis=1).groupby(['model', 'metric']).describe()['value']['mean']\n",
    "       results_df = pd.DataFrame(mean_across_datasets).reset_index().rename({'mean':'value'}, axis=1)\n",
    "    # plot a single dataset\n",
    "    else:\n",
    "        results_df = results_df[results_df['dataset'] == dataset]\n",
    "    \n",
    "    fig = px.line_polar(results_df, r='value', theta='metric', color='model', line_close=True) \n",
    "    xlim = 1\n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, xlim],\n",
    "            )),\n",
    "        margin=dict(l=150, r=0, t=100, b=80)\n",
    "    )\n",
    "\n",
    "    \n",
    "    title =  'Average Performance over 3 QA Datasets' if dataset == 'all' else dataset\n",
    "    fig.update_layout(\n",
    "            title=dict(text=title, font=dict(size=20), yref='container')\n",
    "        )\n",
    "    \n",
    "    directory = \"example_results\"\n",
    "    fig.show()\n",
    "    fig.write_image(f\"{directory}/radarplot.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_id_base, model_id_instruct]\n",
    "results_df = load_results(models)\n",
    "visualize_radar(results_df, dataset='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instruction-finetuned model (in red) outperforms the non-finetuned model on most metrics. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmeval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
