{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8dae933-0b5f-4c6f-b9fc-145e467071b9",
   "metadata": {},
   "source": [
    "## Evaluating Factual Knowledge with Bring-Your-Own Model Outputs\n",
    "\n",
    "In this example we take a JumpStart Endpoint and run inference on an entire dataset, before running an evaluation. This example is for use-cases where the model output field is already pre-populated and we want to run an evaluation algo on the model output and the target output.\n",
    "\n",
    "Environment:\n",
    "- conda_python3 kernel\n",
    "- Studio Notebook instance type: ml.g4dn.xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59649c6-e41e-4caa-906d-a5acabb391c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip3 install sagemaker\n",
    "\n",
    "#!pip3 install -U pyarrow\n",
    "#!pip3 install -U accelerate\n",
    "#!pip3 install \"ipywidgets>=8\"\n",
    "#!pip3 install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f920ab-e925-4b96-9cbc-50b36b6af3fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Check for beta wheel and built-in dataset\n",
    "if not glob.glob(\"fmeval-0.1.0-py3-none-any.whl\"):\n",
    "    print(\"ERROR - please make sure file exists: fmeval-*-py3-none-any.whl\")\n",
    "\n",
    "if not glob.glob(\"tiny_dataset.jsonl\"):\n",
    "    print(\"ERROR - please make sure file exists: tiny_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e9d2a1-a2b3-471e-ad07-e20ae25aa63f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Install the fmeval-*-py3-none-any.whl distribution.\n",
    "#\n",
    "\n",
    "#!rm -Rf ~/.cache/pip/*\n",
    "\n",
    "#!pip3 install fmeval-0.1.0-py3-none-any.whl --upgrade --upgrade-strategy only-if-needed --force-reinstall\n",
    "#!pip3 install boto3==1.28.65"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a8297-ce56-4793-8aca-095382e76175",
   "metadata": {},
   "source": [
    "### JumpStart Model Setup & Endpoint Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc561a07-56ab-40ed-ac88-faa76c73eef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "# need for FMEval Model Runner Config\n",
    "model_id, model_version, = (\n",
    "    \"huggingface-llm-falcon-7b-instruct-bf16\",\n",
    "    \"*\",\n",
    ")\n",
    "\n",
    "endpoint_name = \"Enter endpoint name here if already existing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff1f7db-720b-4747-909d-3c8a8dacd41c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    # if endpoint already existing\n",
    "    endpoint_name\n",
    "\n",
    "except NameError:\n",
    "    \n",
    "    my_model = JumpStartModel(model_id=model_id)\n",
    "    predictor = my_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81acb9f-4bc6-4b54-9c42-c04fcc7b31c6",
   "metadata": {},
   "source": [
    "#### Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b5f20-5afa-4b63-b6dd-a9388e5b9060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Tell me about Amazon SageMaker.\"\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.8,\n",
    "        \"max_new_tokens\": 1024,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae4fa8-9444-42d1-8dfe-9713f4b26564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "content_type = \"application/json\"\n",
    "runtime = boto3.client(\"sagemaker-runtime\")\n",
    "    \n",
    "try:\n",
    "    endpoint_name = predictor.endpoint_name\n",
    "    response = predictor.predict(payload)\n",
    "    print(response[0][\"generated_text\"])\n",
    "\n",
    "except NameError:\n",
    "    # if you have an existing endpoint\n",
    "    print(f\"Utilizing invoke_endpoint API call for existing endpoint: {endpoint_name}\")\n",
    "    response = runtime.invoke_endpoint(EndpointName = endpoint_name, Body = json.dumps(payload), ContentType = content_type)\n",
    "    result = json.loads(response['Body'].read().decode())\n",
    "    print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9228060-c528-4fc2-adf4-df8e7fe51b83",
   "metadata": {},
   "source": [
    "### Model Inference on Dataset\n",
    "\n",
    "In this case we need to run our model across the dataset prior to configuring an evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60129159-d964-473a-a156-0cfe633d3e99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shape_payload(question: str, \n",
    "                  payload_shape: dict = {\n",
    "                    \"inputs\": prompt,\n",
    "                    \"parameters\": {\n",
    "                        \"do_sample\": True,\n",
    "                        \"top_p\": 0.9,\n",
    "                        \"temperature\": 0.8,\n",
    "                        \"max_new_tokens\": 1024\n",
    "                    },}) -> dict:\n",
    "    \"\"\"\n",
    "    Function to shape payload for model inference\n",
    "    Args:\n",
    "        question (str): Question for the LLM\n",
    "        payload_shape (dict): Adjust for the format your LLM expects\n",
    "    \n",
    "    Returns:\n",
    "        payload_shape: Updated payload shape with the question passed in as an input for prompt\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(question) == 0:\n",
    "        raise ValueError(\"Empty question, please provide a full length question.\")\n",
    "    \n",
    "    payload_shape = {\n",
    "                    \"inputs\": question,\n",
    "                    \"parameters\": {\n",
    "                        \"do_sample\": True,\n",
    "                        \"top_p\": 0.9,\n",
    "                        \"temperature\": 0.8,\n",
    "                        \"max_new_tokens\": 1024\n",
    "                    },}\n",
    "    return payload_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97a140-f92a-4942-a343-c56eda722d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "input_file = \"tiny_dataset.jsonl\"\n",
    "output_file = \"updated_tiny_dataset.jsonl\"\n",
    "\n",
    "# open tiny dataset or your own and create a column for model inference results storage\n",
    "with jsonlines.open(input_file) as lines, jsonlines.open(output_file, \"w\") as predictions:\n",
    "    for line in lines:\n",
    "        if \"question\" in line:\n",
    "            question = line[\"question\"]\n",
    "            formatted_input = shape_payload(question)\n",
    "            response = runtime.invoke_endpoint(EndpointName = endpoint_name, Body = json.dumps(formatted_input), ContentType = content_type)\n",
    "            result = json.loads(response['Body'].read().decode())\n",
    "            answer = result[0]['generated_text']\n",
    "            line[\"model_output\"] = answer\n",
    "            predictions.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6788d618-db06-42a6-ac13-9785f5269788",
   "metadata": {},
   "source": [
    "### FMEval Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d9e90-81af-403e-83f0-28f6a5ee4a10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fmeval.data_loaders.data_config import DataConfig\n",
    "from fmeval.model_runners.sm_jumpstart_model_runner import JumpStartModelRunner\n",
    "from fmeval.constants import MIME_TYPE_JSONLINES\n",
    "from fmeval.eval_algorithms.factual_knowledge import FactualKnowledge, FactualKnowledgeConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae741ef1-85f2-4e0f-afc9-017316ba401b",
   "metadata": {},
   "source": [
    "#### Data Config Setup\n",
    "\n",
    "You can either bring your own dataset or use our built-in datasets such as tiny_dataset. In this case we use the JSONlines dataset we have created with model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7945ed88-581c-4dfb-9ee4-1669467df3c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = DataConfig(\n",
    "    dataset_name=\"tiny_dataset_model_answers\",\n",
    "    dataset_uri=\"updated_tiny_dataset.jsonl\",\n",
    "    dataset_mime_type=MIME_TYPE_JSONLINES,\n",
    "    model_input_location=\"question\",\n",
    "    target_output_location=\"answer\",\n",
    "    model_output_location=\"model_output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d450c-f355-432a-8ae9-2d5b184d4cc3",
   "metadata": {},
   "source": [
    "#### Evaluation Result Configuration\n",
    "\n",
    "By default results are written to the tmp directory: /tmp/eval_results/factual_knowledge_tiny_dataset.jsonl. Here we adjust this and create our own results directory and set this as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5269a7-074c-4592-9cc8-f83dc8a2995b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "eval_dir = \"results-evaluation-model-output\"\n",
    "curr_dir = os.getcwd()\n",
    "eval_results_path = os.path.join(curr_dir, eval_dir) + \"/\"\n",
    "os.environ[\"EVAL_RESULTS_PATH\"] = eval_results_path\n",
    "if os.path.exists(eval_results_path):\n",
    "    print(f\"Directory '{eval_results_path}' exists.\")\n",
    "else:\n",
    "    os.mkdir(eval_results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86721fce-b1dd-4b71-93d2-ed3c405bd28b",
   "metadata": {},
   "source": [
    "### Run Evaluation\n",
    "\n",
    "In this case we run the evaluation without the model runner as we already have inference for our dataset, we simply pass our dataset config and prompt template for use-cases such as this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfacfe4-6cf6-4c46-ac87-4e4ab8c19c88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_algo = FactualKnowledge(FactualKnowledgeConfig(target_output_delimiter=\"<OR>\"))\n",
    "eval_output = eval_algo.evaluate(dataset_config=config, prompt_template=\"$feature\", save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b679c-2fa5-42e9-b090-cd11e3ad62b8",
   "metadata": {},
   "source": [
    "#### Parse Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f4dc8-1a73-4063-8d31-850dfd5e6f31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c5d54-17c5-4805-8840-dd90a264d59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(eval_output, default=vars, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e243cf-42e7-46f5-aae3-c9898e3b262c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "with open(\"results-evaluation-model-output/factual_knowledge_tiny_dataset_model_answers.jsonl\", \"r\") as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "df = pd.DataFrame(data)\n",
    "df['eval_algo'] = df['scores'].apply(lambda x: x[0]['name'])\n",
    "df['eval_score'] = df['scores'].apply(lambda x: x[0]['value'])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3 (Base Python 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-base-python-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
