{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef914c14-56c2-4926-870b-2275a0ecf547",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluating Summarization Accuracy on Claude Model in Bedrock\n",
    "\n",
    "In this notebook we utilize Claude via Bedrock with the FMEval Library to test Summarization Accuracy with three metrics: meteor, rouge, and bert score.\n",
    "\n",
    "Environment:\n",
    "\n",
    "- Base Python 3.0 kernel\n",
    "- Studio Notebook instance type: ml.c5.xlarge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ddf80-88ef-4fde-a465-56c77f68bc65",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e21d2225-ea0b-4f95-8910-2ee3ab1852b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Install the fmeval-*-py3-none-any.whl distribution.\n",
    "#\n",
    "\n",
    "#!rm -Rf ~/.cache/pip/*\n",
    "\n",
    "#!pip3 install fmeval-0.1.0-py3-none-any.whl --upgrade --upgrade-strategy only-if-needed --force-reinstall\n",
    "#!pip3 install boto3==1.28.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e7cec9-07f6-40bf-90a6-940f1063a3be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "\n",
    "# This is dependent on the hardware that you run the evaluation on. If the machine has enough memory you can increase this value or remove this environment variable. As this is a smaller dataset we set this value to 1 to circumvent any OOM issues.\n",
    "os.environ[\"PARALLELIZATION_FACTOR\"] = \"1\"\n",
    "\n",
    "# Bedrock clients for model inference\n",
    "bedrock = boto3.client(service_name='bedrock')\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26da00d-9ef1-4d02-a77b-cf46fefc0b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Check for beta wheel and built-in dataset\n",
    "if not glob.glob(\"fmeval-0.1.0-py3-none-any.whl\"):\n",
    "    print(\"ERROR - please make sure file exists: fmeval-0.1.0-py3-none-any.whl\")\n",
    "\n",
    "if not glob.glob(\"xsum_sample.jsonl\"):\n",
    "    print(\"ERROR - please make sure file exists: xsum_sample.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28894ce-6c2a-4d15-b084-3baedd80c63d",
   "metadata": {},
   "source": [
    "### Sample Bedrock Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb58a6-6005-416f-8116-971e82d0c741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model_id = 'anthropic.claude-v2'\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "# Ensure that your prompt is structured in the format that Claude expects as documented here: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-claude.html#model-parameters-claude-request-body\n",
    "prompt_data = \"\"\"Human: Who is Barack Obama?\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "body = json.dumps({\"prompt\": prompt_data, \"max_tokens_to_sample\": 500})\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, modelId=model_id, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "print(response_body.get(\"completion\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45537518-b006-4c3b-9acd-8833cc0b6703",
   "metadata": {},
   "source": [
    "### FMEval Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3746dd96-2ee9-496c-8188-ea3e26be91a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fmeval.data_loaders.data_config import DataConfig\n",
    "from fmeval.model_runners.bedrock_model_runner import BedrockModelRunner\n",
    "from fmeval.constants import MIME_TYPE_JSONLINES\n",
    "from fmeval.eval_algorithms.summarization_accuracy import SummarizationAccuracy, SummarizationAccuracyConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e0bc40-1eed-4365-864e-ff0421f052b8",
   "metadata": {},
   "source": [
    "#### Data Config Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd29ac29-0689-4783-a173-2920156104e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = DataConfig(\n",
    "    dataset_name=\"xsum_dataset\",\n",
    "    dataset_uri=\"xsum_sample.jsonl\",\n",
    "    dataset_mime_type=MIME_TYPE_JSONLINES,\n",
    "    model_input_location=\"document\",\n",
    "    target_output_location=\"summary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7760200d-2e58-4f5c-a959-3d2350d28115",
   "metadata": {},
   "source": [
    "#### Model Runner Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec7b658-3d43-4225-9267-05fb061d1317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_model_runner = BedrockModelRunner(\n",
    "    model_id=model_id,\n",
    "    output='completion',\n",
    "    content_template='{\"prompt\": $prompt, \"max_tokens_to_sample\": 500}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f37ffa2-5857-4d83-b0fa-dbe2b22b2a14",
   "metadata": {},
   "source": [
    "### Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d54a2c0-6fd0-41af-acb4-d8fd01e53af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_algo = SummarizationAccuracy(SummarizationAccuracyConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673100d-8a72-4e75-a110-8fdd032e90fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_output = eval_algo.evaluate(model=bedrock_model_runner, dataset_config=config, \n",
    "                                 prompt_template=\"Human: $feature\\n\\nAssistant:\\n\", save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e049fe-6ffe-4296-b1be-ccf669af2cb0",
   "metadata": {},
   "source": [
    "#### Parse Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac4be48-b557-4d12-9ba7-b265aef4e451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parse report\n",
    "print(json.dumps(eval_output, default=vars, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61282d19-1d1e-4552-a1eb-aa79634fe633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "with open(\"/tmp/eval_results/summarization_accuracy_xsum_dataset.jsonl\", \"r\") as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "df = pd.DataFrame(data)\n",
    "df['meteor_score'] = df['scores'].apply(lambda x: x[0]['value'])\n",
    "df['rouge_score'] = df['scores'].apply(lambda x: x[1]['value'])\n",
    "df['bert_score'] = df['scores'].apply(lambda x: x[2]['value'])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
