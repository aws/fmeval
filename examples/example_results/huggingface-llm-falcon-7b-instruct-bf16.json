{"accuracy": [{"eval_name": "qa_accuracy", "dataset_name": "boolq", "dataset_scores": [{"name": "f1_score", "value": 0.5482051282051281}, {"name": "exact_match_score", "value": 0.13}, {"name": "quasi_exact_match_score", "value": 0.13}, {"name": "precision_over_words", "value": 0.5108333333333334}, {"name": "recall_over_words", "value": 0.63}], "prompt_template": "Respond to the following question. Valid answers are \"True\" or \"False\". $feature Answer:", "category_scores": null, "output_path": "/tmp/eval_results/qa_accuracy_boolq.jsonl", "error": null}, {"eval_name": "qa_accuracy", "dataset_name": "trivia_qa", "dataset_scores": [{"name": "f1_score", "value": 0.2205525902770148}, {"name": "exact_match_score", "value": 0.0}, {"name": "quasi_exact_match_score", "value": 0.01}, {"name": "precision_over_words", "value": 0.172711331212647}, {"name": "recall_over_words", "value": 0.5011666666666666}], "prompt_template": "Respond to the following question with a short answer: $feature Answer:", "category_scores": null, "output_path": "/tmp/eval_results/qa_accuracy_trivia_qa.jsonl", "error": null}, {"eval_name": "qa_accuracy", "dataset_name": "natural_questions", "dataset_scores": [{"name": "f1_score", "value": 0.13299733647576287}, {"name": "exact_match_score", "value": 0.0}, {"name": "quasi_exact_match_score", "value": 0.0}, {"name": "precision_over_words", "value": 0.11563458449874857}, {"name": "recall_over_words", "value": 0.27821520087860807}], "prompt_template": "Respond to the following question with a short answer: $feature Answer:", "category_scores": null, "output_path": "/tmp/eval_results/qa_accuracy_natural_questions.jsonl", "error": null}]}