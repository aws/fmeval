{"accuracy": [{"eval_name": "qa_accuracy", "dataset_name": "boolq", "dataset_scores": [{"name": "f1_score", "value": 0.04795848074214057}, {"name": "exact_match_score", "value": 0.0}, {"name": "quasi_exact_match_score", "value": 0.0}, {"name": "precision_over_words", "value": 0.027433262194204267}, {"name": "recall_over_words", "value": 0.72}], "prompt_template": "Respond to the following question. Valid answers are \"True\" or \"False\". $feature Answer:", "category_scores": null, "output_path": "/tmp/eval_results/qa_accuracy_boolq.jsonl", "error": null}, {"eval_name": "qa_accuracy", "dataset_name": "trivia_qa", "dataset_scores": [{"name": "f1_score", "value": 0.04086519119625618}, {"name": "exact_match_score", "value": 0.0}, {"name": "quasi_exact_match_score", "value": 0.0}, {"name": "precision_over_words", "value": 0.02179494072975362}, {"name": "recall_over_words", "value": 0.6068333333333334}], "prompt_template": "Respond to the following question with a short answer: $feature Answer:", "category_scores": null, "output_path": "/tmp/eval_results/qa_accuracy_trivia_qa.jsonl", "error": null}, {"eval_name": "qa_accuracy", "dataset_name": "natural_questions", "dataset_scores": [{"name": "f1_score", "value": 0.05160828598321541}, {"name": "exact_match_score", "value": 0.0}, {"name": "quasi_exact_match_score", "value": 0.0}, {"name": "precision_over_words", "value": 0.03256318143852671}, {"name": "recall_over_words", "value": 0.41481685911972965}], "prompt_template": "Respond to the following question with a short answer: $feature Answer:", "category_scores": null, "output_path": "/tmp/eval_results/qa_accuracy_natural_questions.jsonl", "error": null}]}
