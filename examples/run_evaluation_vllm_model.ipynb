{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1357ca38-e056-4243-9074-86bfa7b453de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# This example runs evaluations against a vLLM API Server:\n",
    "#\n",
    "#   https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html#api-server\n",
    "#\n",
    "# This example requires the following files:\n",
    "#\n",
    "#   1. tiny_dataset.jsonl\n",
    "#\n",
    "# This example will write its output to:\n",
    "#\n",
    "#   /tmp/eval_results/factual_knowledge_tiny_dataset.jsonl\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e272b4-0926-447d-9a2d-9087248446a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's check for this example's required files in the environment:\n",
    "#\n",
    "#   1. tiny_dataset.jsonl\n",
    "#\n",
    "\n",
    "import glob\n",
    "\n",
    "if not glob.glob(\"tiny_dataset.jsonl\"):\n",
    "    print(\"ERROR - please make sure file exists: tiny_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b252a57b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from amazon_fmeval.data_loaders.data_config import DataConfig\n",
    "from amazon_fmeval.model_runners.vllm_model_runner import VllmModelRunner\n",
    "from amazon_fmeval.eval_algo_mapping import get_eval_algorithm\n",
    "from amazon_fmeval.constants import MIME_TYPE_JSONLINES\n",
    "from amazon_fmeval.eval_algorithms.factual_knowledge import FactualKnowledge, FactualKnowledgeConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162ff4e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# We create an instance of DataConfig, which tells us about the data that should be used for an evaluation.\n",
    "# This step is only necessary for custom datasets.\n",
    "#\n",
    "config = DataConfig(\n",
    "    dataset_name=\"tiny_dataset\",\n",
    "    dataset_uri=\"tiny_dataset.jsonl\",\n",
    "    dataset_mime_type=MIME_TYPE_JSONLINES,\n",
    "    model_input_location=\"question\",\n",
    "    target_output_location=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d7882",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# We also a create a VllmModelRunner which can perform invocation again the vLLM API Server.\n",
    "#\n",
    "\n",
    "vllm_model_runner = VllmModelRunner(\n",
    "    content_template = '{\"content\": \"$prompt\"}',\n",
    "    output = 'text[0]',\n",
    "    remote_uri = 'http://localhost:8000/generate',\n",
    "    num_completions = 1,\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f865dc5-89a4-4b9d-9bb3-bcc71b06a60d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# If you want to choose the output path, uncomment the lines below.\n",
    "# This is set using the EVAL_RESULTS_PATH environment variable.\n",
    "#\n",
    "\n",
    "import os\n",
    "\n",
    "# eval_results_path = \"/tmp/custom_dir_eval_results/\"\n",
    "# os.environ[\"EVAL_RESULTS_PATH\"] = eval_results_path\n",
    "# os.mkdir(eval_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849635d1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Here, we run the FactualKnowledge evaluation algorithm.\n",
    "#\n",
    "\n",
    "eval_algo = FactualKnowledge(FactualKnowledgeConfig(target_output_delimiter=\"<OR>\"))\n",
    "eval_output = eval_algo.evaluate(model=vllm_model_runner, dataset_config=config, prompt_template=\"$feature\", save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd9eb8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Print the evalaution output.\n",
    "#\n",
    "\n",
    "eval_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2fc006",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Pretty-print the evalaution output (notice the score).\n",
    "#\n",
    "\n",
    "import json\n",
    "print(json.dumps(eval_output, default=vars, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d05e03",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# See the raw evaluation results.\n",
    "#\n",
    "\n",
    "!cat /tmp/eval_results/factual_knowledge_tiny_dataset.jsonl"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
